idx,name,description,label,software
0,file,Specifies the path of the server log file for logging. ,debuggability,aerospike
1,context,Specifies the context and level of logging to be logged.,debuggability,aerospike
2,cache-enabled,"Whether to enable caching of Lua states for each registered Lua module, to benefit performance.",performance,aerospike
3,user-path,Directory to be used by the Aerospike process to store user generated UDF files,environment,aerospike
4,allow-nonxdr-writes,Parameter to control the writes done by a non-XDR client. Setting it to false will disallow all the writes from a nonXDR client (any regular client library). ,security,aerospike
5,allow-xdr-writes,Parameter to control whether to accept write transactions originating from an XDR client.,security,aerospike
6,cold-start-evict-ttl,This sets the TTL below which records will be evicted (will not be loaded) during coldstart. ,reliability,aerospike
7,data-in-index,"Optimization in single bin case, will only allow integer or float stored in index space. ",performance,aerospike
8,default-ttl,Default time-to-live (in seconds) for a record from the time of creation or last update. ,reliability,aerospike
9,disable-cold-start-eviction,"If true, disables eviction that may occur at cold start for this namespace only.",reliability,aerospike
10,disable-nsup,"If true , disables NSUP primary index reductions for this namespace only. ",performance,aerospike
11,disable-write-dup-res,Disables write duplicate resolution for the namespace. ,performance,aerospike
12,enable-benchmarks-batch-sub,Enable histograms for batch sub transactions.,debuggability,aerospike
13,enable-benchmarks-fabric,Enable histograms for fabric. ,debuggability,aerospike
14,enable-benchmarks-read,Enable histograms for read transactions. ,debuggability,aerospike
15,enable-benchmarks-storage,Enable histograms for storage access.,debuggability,aerospike
16,enable-benchmarks-svc,Enable histograms for demarshal and transaction queue related operations. ,debuggability,aerospike
17,enable-benchmarks-udf,Enable histograms for udf transactions.,debuggability,aerospike
18,enable-benchmarks-udf-sub,Enable histograms for udf sub transactions. ,debuggability,aerospike
19,enable-benchmarks-write,Enable histograms for write transactions.,debuggability,aerospike
20,enable-hist-info,Enable histograms for info protocol transactions.,debuggability,aerospike
21,enable-hist-proxy,Enable histograms for proxy transactions. ,debuggability,aerospike
22,enable-xdr,"This controls, at the namespace level, whether digest log entries are being written to the digest log.",debuggability,aerospike
23,evict-hist-buckets,Number of histogram buckets used for evictions. ,performance,aerospike
24,evict-tenths-pct,Maximum 1/10th percentage of objects to be deleted during each iteration of eviction.,reliability,aerospike
25,high-water-disk-pct,Data will be evicted if the disk utilization is greater than this specified percentage.,reliability,aerospike
26,high-water-memory-pct,Data will be evicted if the memory utilization is greater than this specified percentage.,reliability,aerospike
27,max-ttl,Maximum TTL allowed in the server. ,reliability,aerospike
28,memory-size,Maximum amount of memory for the namespace. ,reliability,aerospike
29,migrate-order,Number between 1 and 10 which determines the order namespaces are to be processed when migrating,others,aerospike
30,migrate-retransmit-ms,"How long to wait for success, in milliseconds, before retrying a migration related transaction. ",reliability,aerospike
31,migrate-sleep,Number of microseconds to sleep after each record migration. ,reliability,aerospike
32,ns-forward-xdr-writes,This parameter provides fine grained control at namespace level to forward writes that originated from another XDR to the specified destination datacenters (in xdr section). ,reliability,aerospike
33,partition-tree-sprigs,Number of tree sprigs to use.  Providing more trees (sprigs) reduces the number of levels and speeds up the search.,performance,aerospike
34,prefer-uniform-balance,"If true, this namespace will make an effort to distribute partitions evenly to all nodes. ",performance,aerospike
35,read-consistency-level-override,"When set to a non-default value, overrides the client-specified per-transaction read consistency level for this namespace. ",performance,aerospike
36,replication-factor,Number of copies of a record (including the master copy) maintained in the entire cluster.,reliability,aerospike
37,sets-enable-xdr,Specifies whether XDR should ship all sets in a namespace or not.,reliability,aerospike
38,single-bin,Setting it true will disallow multiple bin (columns) for a record.,others,aerospike
39,stop-writes-pct,"Disallow writes (except deletes, replica writes and migration writes) when memory utilization (tracked under memory_used_bytes ) is above this specified percentage.",reliability,aerospike
40,strong-consistency,Set the namespace to Strong Consistency mode to favor consistency over availability. ,reliability,aerospike
41,strong-consistency-allow-expunge,"When set to true , allows non-durable deletes to be used with strong-consistency .",reliability,aerospike
42,tomb-raider-eligible-age,Number of seconds to retain a tombstone,reliability,aerospike
43,tomb-raider-period,"Minimum amount of time, in seconds, between tomb-raider runs. ",reliability,aerospike
44,transaction-pending-limit,Maximum pending transactions that can be queued up to work on the same key.,reliability,aerospike
45,write-commit-level-override,"When set to a non-default value, overrides the client-specified per-transaction write commit level for this namespace.",reliability,aerospike
46,xdr-remote-datacenter,Name of the datacenter to forward this namespace to,others,aerospike
47,max-cells,Sets the maximum desired number of cells in the approximation. ,reliability,aerospike
48,max-level,Maximum depth to go for a single cell,reliability,aerospike
49,mount,Path to the mount directory (typically on NVMe SSD).,environment,aerospike
50,mounts-high-water-pct,Data will be evicted if the mount's utilization is greater than this specified percentage,reliability,aerospike
51,mounts-size-limit,Maximum amount of device space for the mount(s) on this namespace. ,reliability,aerospike
52,set-enable-xdr,Set-specific parameter to enable/disable shipping through XDR,reliability,aerospike
53,cold-start-empty,Setting this to true will cause cold start to ignore existing data on drives and start as if empty. Does not affect warm start.,others,aerospike
54,commit-to-device,Wait for write to flush to disk before acknowledging the client. ,reliability,aerospike
55,compression,"Use of this item requires a feature-key-file , and specifies the algorithm used to compress records on SSD.",performance,aerospike
56,compression-level,The compression level to use with zstd compression. A higher level means more but slower compression. ,performance,aerospike
57,data-in-memory,Keep a copy of all data in memory always.,reliability,aerospike
58,defrag-queue-min,Don't defrag unless the queue has this many eligible wblocks.,performance,aerospike
59,defrag-sleep,Number of microseconds to sleep after each wblock defragged.,reliability,aerospike
60,device,Raw device used to store the namespace.,environment,aerospike
61,direct-files," If direct-files is set true , then the odirect and odsync flags are enabled for file IO. ",reliability,aerospike
62,enable-osync,Tells the device to flush on every write. This may impact performance. ,performance,aerospike
63,encryption,Specifies the algorithm used by encryption at rest. ,security,aerospike
64,encryption-key-file,Enables encryption at rest by providing the location of the encryption key file.,security,aerospike
65,file,Data file path on rotational disk (using a file system).,environment,aerospike
66,filesize,Maximum size for each file storage defined in this namespace.,reliability,aerospike
67,flush-max-ms,Configures the maximum amount of time that a Streaming Write Buffer (SWB) can go without being written to device. ,reliability,aerospike
68,max-write-cache,Number of bytes (should be multiple of write-block-size) the system is allowed to keep pending write blocks before failing writes.,performance,aerospike
69,post-write-queue,Write block buffers to keep as cache (per device). ,performance,aerospike
70,read-page-cache,This allows the OS to leverage page cache and can help with latencies for some workload types. ,performance,aerospike
71,tomb-raider-sleep,Number of microseconds to sleep in between large block reads on disk. ,reliability,aerospike
72,write-block-size,Size in bytes of each I/O block that is written to the disk. ,performance,aerospike
73,channel-bulk-fds,Number of bulk channel sockets to open to each neighbor node. ,performance,aerospike
74,channel-bulk-recv-threads,Number of threads processing intra-cluster messages arriving through the bulk channel.,performance,aerospike
75,channel-meta-recv-threads,Number of threads processing intra-cluster messages arriving through the meta channel. ,performance,aerospike
76,channel-rw-fds,Number of read/write channel sockets to open to each neighbor node. ,performance,aerospike
77,channel-rw-recv-threads,Number of threads processing intra-cluster messages arriving through the rw (read/write) channel.,performance,aerospike
78,keepalive-enabled,Enables the nodes to send keep-alive messages to each other,reliability,aerospike
79,keepalive-intvl,Interval in seconds between successive keep-alive packets.,reliability,aerospike
80,keepalive-time,Time in seconds from the last user data packet sent on the socket before sending the first keep-alive packet.,reliability,aerospike
81,port,Port for inter-node communication within a cluster.,environment,aerospike
82,send-threads,Number of intra-node send threads to be used. ,performance,aerospike
83,address,IP address for cluster-state heartbeat communication for mesh.,environment,aerospike
84,interval,Interval in milliseconds in which heartbeats are sent.,reliability,aerospike
85,mcast-ttl,TTL for multicast packets.,reliability,aerospike
86,mesh-seed-address-port,Mesh address (host-name or IP) and port info for seed server(s). ,environment,aerospike
87,multicast-group,IP address for cluster-state heartbeat communication over multicast,environment,aerospike
88,port,Port for cluster-state communication (mesh or multicast).,environment,aerospike
89,protocol,Heartbeat protocol version to be used by cluster. ,environment,aerospike
90,timeout,Number of missing heartbeats after which the remote node will be declared dead.,reliability,aerospike
91,port,Port used for info management. ,environment,aerospike
92,access-address,An access address is an IP address that is announced to clients and used by clients for connecting to the cluster.,environment,aerospike
93,address,The IP address at which the server listens for client connections. ,environment,aerospike
94,alternate-access-address,Can be used to choose a specific IP address or DNS name that will be published as an alternate list for clients to connect,environment,aerospike
95,port,The port at which the server listens for client connections.,environment,aerospike
96,tls-name,this parameter specifies which TLS parameters to use for the given context TLS connections. ,security,aerospike
97,tls-port,"Port that is TLS enabled at which the server listens for client connections, heartbeat connections or fabric connections (based on the subcontext this is set at).",environment,aerospike
98,ca-file,Path to the CA file needed for mutual authentication.,environment,aerospike
99,ca-path,Path to the directory of the CA file for mutual authentication. ,environment,aerospike
100,cert-blacklist,Path to the file containing rogue certificates serial numbers. ,environment,aerospike
101,cert-file,Path to the TLS certificate file when TLS is enabled.,environment,aerospike
102,key-file,Path to the key file when TLS is enabled. ,security,aerospike
103,key-file-password,Password for the key-file . ,security,aerospike
104,enable-security,Enable access control. ,security,aerospike
105,privilege-refresh-period,Frequency in seconds with which the node verifies credentials and permissions for active client connections.,security,aerospike
106,polling-period,How frequently (in seconds) to query the LDAP server for user group membership information. ,security,aerospike
107,query-base-dn,Distinguished name of the LDAP directory entry at which to begin the search when querying for a user's group membership information.,environment,aerospike
108,query-user-dn,Distinguished name of the user designated for user group membership queries.,environment,aerospike
109,server,Name of the LDAP server to use. ,environment,aerospike
110,session-ttl,"Lifetime (in seconds) of an access token. A TCP connection attempt with an expired token will fail, and the clien must log in again to get a fresh token. ",reliability,aerospike
111,tls-ca-file,Path to the CA certificate file used for validating TLS connections to the LDAP server.,security,aerospike
112,token-hash-method,Hash algorithm to use when generating the HMAC for access tokens. ,security,aerospike
113,report-authentication,Set to true to report successful authentications in aerospike.log.,debuggability,aerospike
114,report-data-op,Set to true to report on data transactions for a namespace (and optionally a set).,debuggability,aerospike
115,report-sys-admin,Set to true to report systems administration operations,security,aerospike
116,report-user-admin,Set to true to report successful user administration operations ,security,aerospike
117,report-violation,Set to true to report security violations ,security,aerospike
118,report-authentication,Set to true to report successful authentications in the syslog file.,debuggability,aerospike
119,report-user-admin,Set to true to report successful user administration operations in the syslog file.,debuggability,aerospike
120,report-violation,Set to true to report security violations in the syslog file.,security,aerospike
121,advertise-ipv6,Requires heartbeat v3. Set to true in order enable IPv6.,environment,aerospike
122,batch-index-threads,Number of batch index response worker threads. ,performance,aerospike
123,batch-max-buffers-per-queue,Number of 128 KiB response buffers allowed in each batch index queue before it is marked as full. ,performance,aerospike
124,batch-max-requests,Max number of keys allowed per node.,reliability,aerospike
125,enable-health-check,Monitors the health of a cluster and attempts to idendity potential outlier nodes. ,reliability,aerospike
126,feature-key-file,Location of the digitally signed feature key file containing the features that are enabled,security,aerospike
127,hist-track-back,Total time span in seconds over which to cache data.,performance,aerospike
128,info-threads,Number of threads to create to process info requests.,performance,aerospike
129,log-local-time,Set this configuration to true to set logs to have local time stamp ,debuggability,aerospike
130,log-millis,Set this to true in order to get millisecond timestamps in the log file.,others,aerospike
131,migrate-fill-delay,Number of seconds to delay before starting 'fill' migrations. ,reliability,aerospike
132,migrate-threads,Number of threads per server allocated for data migration.,performance,aerospike
133,min-cluster-size,The minimum number of nodes required for a cluster to form. ,performance,aerospike
134,nsup-delete-sleep,Number of microseconds to sleep between generating delete transactions.,reliability,aerospike
135,nsup-delete-sleep,The interval (secs) at which expiration/eviction thread (namespace supervisor) wakes up.,reliability,aerospike
136,cluster_name,The name of the cluster. This is mainly used to prevent machines in one logical cluster from joining another.,others,cassandra
137,num_tokens,"This defines the number of tokens randomly assigned to this node on the ring The more tokens, relative to other nodes, the larger the proportion of data that this node will store. ",performance,cassandra
138,allocate_tokens_for_keyspace,Triggers automatic allocation of num_tokens tokens for this node. The allocation algorithm attempts to choose tokens in a way that optimizes replicated load over the nodes in the datacenter for the replication strategy used by the specified keyspace.,performance,cassandra
139,initial_token,"initial_token allows you to specify tokens manually. While you can use it with vnodes (num_tokens > 1, above) in which case you should provide a comma-separated list it's primarily used when adding nodes to legacy clusters that do not have vnodes enabled.",others,cassandra
140,hinted_handoff_enabled,"If a write is made and a replica node for the key is down (and hinted_handoff_enabled == true), Cassandra will write a hint.",reliability,cassandra
141,hinted_handoff_disabled_datacenters,"When hinted_handoff_enabled is true, a black list of data centers that will not perform hinted handoff",reliability,cassandra
142,max_hint_window_in_ms,"this defines the maximum amount of time a dead host will have hints generated. After it has been dead this long, new hints for it will not be created until it has been seen alive and gone down again.",reliability,cassandra
143,hinted_handoff_throttle_in_kb,"Maximum throttle in KBs per second, per delivery thread. This will be reduced proportionally to the number of nodes in the cluster. ",reliability,cassandra
144,max_hints_delivery_threads,"Number of threads with which to deliver hints; Consider increasing this number when you have multi-dc deployments, since cross-dc handoff tends to be slower",performance,cassandra
145,hints_directory,"Directory where Cassandra should store hints. If not set, the default directory is $CASSANDRA_HOME/data/hints.",environment,cassandra
146,hints_flush_period_in_ms,How often hints should be flushed from the internal buffers to disk. Will not trigger fsync.,reliability,cassandra
147,max_hints_file_size_in_mb,"Maximum size for a single hints file, in megabytes.",reliability,cassandra
148,hints_compression,"Compression to apply to the hint files. If omitted, hints files will be written uncompressed. LZ4, Snappy, and Deflate compressors are supported.",performance,cassandra
149,batchlog_replay_throttle_in_kb,"Maximum throttle in KBs per second, total. This will be reduced proportionally to the number of nodes in the cluster.",reliability,cassandra
150,authenticator,"Authentication backend, implementing IAuthenticator; used to identify users Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator, PasswordAuthenticator}.",security,cassandra
151,authorizer,"Authorization backend, implementing IAuthorizer; used to limit access/provide permissions Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer, CassandraAuthorizer}.",security,cassandra
152,role_manager,"Part of the Authentication & Authorization backend, implementing IRoleManager; used to maintain grants and memberships between roles. ",security,cassandra
153,roles_validity_in_ms,"Validity period for roles cache (fetching granted roles can be an expensive operation depending on the role manager, CassandraRoleManager is one example) Granted roles are cached for authenticated sessions in AuthenticatedUser and after the period specified here, become eligible for (async) reload.",security,cassandra
154,roles_update_interval_in_ms,"Refresh interval for roles cache (if enabled). After this interval, cache entries become eligible for refresh. Upon next access, an async reload is scheduled and the old value returned until it completes. ",reliability,cassandra
155,permissions_validity_in_ms,"Validity period for permissions cache (fetching permissions can be an expensive operation depending on the authorizer, CassandraAuthorizer is one example).",performance,cassandra
156,permissions_update_interval_in_ms,"Refresh interval for permissions cache (if enabled). After this interval, cache entries become eligible for refresh.",reliability,cassandra
157,credentials_validity_in_ms,"Please note, credentials are cached in their encrypted form, so while activating this cache may reduce the number of queries made to the underlying table, it may not bring a significant reduction in the latency of individual authentication attempts.",security,cassandra
158,credentials_update_interval_in_ms,"Refresh interval for credentials cache (if enabled). After this interval, cache entries become eligible for refresh. ",reliability,cassandra
159,partitioner,The partitioner is responsible for distributing groups of rows (by partition key) across nodes in the cluster. You should leave this alone for new clusters. ,others,cassandra
160,data_file_directories,Directories where Cassandra should store data on disk.,environment,cassandra
161,commitlog_directory,"commit log. when running on magnetic HDD, this should be a separate spindle than the data directories.",debuggability,cassandra
162,cdc_enabled,Enable / disable CDC functionality on a per-node basis.,debuggability,cassandra
163,cdc_raw_directory,CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the segment contains mutations for a CDC-enabled table.,debuggability,cassandra
164,disk_failure_policy,Policy for data disk failures,reliability,cassandra
165,commit_failure_policy,Policy for commit disk failures,reliability,cassandra
166,prepared_statements_cache_size_mb,Maximum size of the native protocol prepared statement cache,reliability,cassandra
167,key_cache_size_in_mb,"The key cache is fairly tiny for the amount of time it saves, so it's worthwhile to use it at large numbers. The row cache saves even more time, but must contain the entire row, so it is extremely space-intensive. ",performance,cassandra
168,key_cache_save_period,"Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache. Row cache saving is much more expensive and has limited use.",performance,cassandra
169,key_cache_keys_to_save,"Number of keys from the key cache to save Disabled by default, meaning all keys are going to be saved",performance,cassandra
170,row_cache_class_name,Row cache implementation class name.,performance,cassandra
171,row_cache_size_in_mb,Maximum size of the row cache in memory. ,performance,cassandra
172,row_cache_save_period,"Duration in seconds after which Cassandra should save the row cache.  Saved caches greatly improve cold-start speeds, and is relatively cheap in terms of I/O for the key cache. Row cache saving is much more expensive and has limited use.",performance,cassandra
173,row_cache_keys_to_save,"Number of keys from the row cache to save. Specify 0 (which is the default), meaning all keys are going to be saved",performance,cassandra
174,counter_cache_size_in_mb,Maximum size of the counter cache in memory.,performance,cassandra
175,counter_cache_save_period,Duration in seconds after which Cassandra should save the counter cache (keys only). Caches are saved to saved_caches_directory as specified in this configuration file.,performance,cassandra
176,counter_cache_keys_to_save,"Number of keys from the counter cache to save Disabled by default, meaning all keys are going to be saved",performance,cassandra
177,saved_caches_directory,"saved caches If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.",environment,cassandra
178,commitlog_sync,This window should be kept short because the writer threads will be unable to do extra work while waiting.,debuggability,cassandra
179,commitlog_segment_size_in_mb,"The size of the individual commitlog file segments. A commitlog segment may be archived, deleted, or recycled once all the data in it (potentially from each columnfamily in the system) has been flushed to sstables.",debuggability,cassandra
180,commitlog_compression,"Compression to apply to the commit log. If omitted, the commit log will be written uncompressed.",performance,cassandra
181,concurrent_reads,concurrent_reads should be set to (16 * number_of_drives) in order to allow the operations to enqueue low enough in the stack that the OS and drives can reorder them.,performance,cassandra
182,concurrent_writes,"since writes are almost never IO bound, the ideal number of concurrent_writes is dependent on the number of cores in your system; (8 * number_of_cores) is a good rule of thumb.",performance,cassandra
183,concurrent_materialized_view_writes,"For materialized view writes, as there is a read involved, so this should be limited by the less of concurrent reads or concurrent writes.",performance,cassandra
184,file_cache_size_in_mb,Maximum memory to use for sstable chunk cache and buffer pooling.,performance,cassandra
185,memtable_heap_space_in_mb,"Total permitted memory to use for memtables. Cassandra will stop accepting writes when the limit is exceeded until a flush completes, and will trigger a flush based on memtable_cleanup_threshold ",reliability,cassandra
186,memtable_cleanup_threshold,"Ratio of occupied non-flushing memtable size to total permitted size that will trigger a flush of the largest memtable. Larger mct will mean larger flushes and hence less compaction, but also less concurrent flush activity which can make it difficult to keep your disks fed under heavy write load.",reliability,cassandra
187,memtable_allocation_type,Specify the way Cassandra allocates and manages memtable memory. ,performance,cassandra
188,commitlog_total_space_in_mb,Total space to use for commit logs on disk.,debuggability,cassandra
189,memtable_flush_writers,This sets the number of memtable flush writer threads per disk as well as the total number of memtables that can be flushed concurrently. These are generally a combination of compute and IO bound.,performance,cassandra
190,cdc_total_space_in_mb,Total space to use for change-data-capture logs on disk.,debuggability,cassandra
191,cdc_free_space_check_interval_ms,"When we hit our cdc_raw limit and the CDCCompactor is either running behind or experiencing backpressure, we check at the following interval to see if any new space for cdc-tracked tables has been made available.",reliability,cassandra
192,index_summary_capacity_in_mb,A fixed memory pool size in MB for for SSTable index summaries. ,performance,cassandra
193,index_summary_resize_interval_in_minutes,How frequently index summaries should be resampled. This is done periodically to redistribute memory from the fixed-size pool to sstables proportional their recent read rates.,performance,cassandra
194,trickle_fsync,"Whether to, when doing sequential writing, fsync() at intervals in order to force the operating system to flush the dirty buffers. Enable this to avoid sudden dirty buffer flushing from impacting read latencies. ",reliability,cassandra
195,storage_port,"TCP port, for commands and data For security reasons, you should not expose this port to the internet. Firewall it if needed.",security,cassandra
196,ssl_storage_port,"SSL port, for encrypted communication. ",security,cassandra
197,listen_address,Address or interface to bind to and tell other Cassandra nodes to connect to. You _must_ change this if you want multiple nodes to be able to communicate!,environment,cassandra
198,listen_interface,"Set listen_address OR listen_interface, not both. Interfaces must correspond to a single address, IP aliasing is not supported.",environment,cassandra
199,listen_interface_prefer_ipv6,If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address you can specify which should be chosen using listen_interface_prefer_ipv6.,environment,cassandra
200,broadcast_address,Address to broadcast to other Cassandra nodes Leaving this blank will set it to the same value as listen_address,environment,cassandra
201,listen_on_broadcast_address,"When using multiple physical network interfaces, set this to true to listen on broadcast_address in addition to the listen_address, allowing nodes to communicate in both interfaces. ",environment,cassandra
202,internode_authenticator,"Internode authentication backend, implementing IInternodeAuthenticator; used to allow/disallow connections from peer nodes.",security,cassandra
203,start_native_transport,Whether to start the native transport server.,others,cassandra
204,native_transport_port,"port for the CQL native transport to listen for clients on For security reasons, you should not expose this port to the internet. ",security,cassandra
205,native_transport_port_ssl,Setting native_transport_port_ssl to a different value from native_transport_port will use encryption for native_transport_port_ssl while keeping native_transport_port unencrypted.,security,cassandra
206,native_transport_max_threads,The maximum threads for handling requests (note that idle threads are stopped after 30 seconds so there is not corresponding minimum setting).,performance,cassandra
207,native_transport_max_frame_size_in_mb,The maximum size of allowed frame. Frame (requests) larger than this will be rejected as invalid.,reliability,cassandra
208,native_transport_max_concurrent_connections,The maximum number of concurrent client connections. ,reliability,cassandra
209,native_transport_max_concurrent_connections_per_ip,The maximum number of concurrent client connections per source ip.,reliability,cassandra
210,rpc_address,The address or interface to bind the native transport server to.,environment,cassandra
211,rpc_interface,"Set rpc_address OR rpc_interface, not both. Interfaces must correspond to a single address, IP aliasing is not supported.",environment,cassandra
212,rpc_interface_prefer_ipv6,If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address you can specify which should be chosen using rpc_interface_prefer_ipv6.,others,cassandra
213,broadcast_rpc_address,RPC address to broadcast to drivers and other Cassandra nodes. ,environment,cassandra
214,rpc_keepalive,enable or disable keepalive on rpc/native connections,others,cassandra
215,incremental_backups,Set to true to have Cassandra create a hard link to each sstable flushed or streamed locally in a backups/ subdirectory of the keyspace data. Removing these links is the operator's responsibility.,reliability,cassandra
216,snapshot_before_compaction,"Whether or not to take a snapshot before each compaction. Be careful using this option, since Cassandra won't clean up the snapshots for you. Mostly useful if you're paranoid when there is a data format change.",reliability,cassandra
217,auto_snapshot,Whether or not a snapshot is taken of the data before keyspace truncation or dropping of column families. ,reliability,cassandra
218,column_index_size_in_kb,Granularity of the collation index of rows within a partition. ,others,cassandra
219,column_index_cache_size_in_kb,Per sstable indexed key cache entries (the collation index in memory mentioned above) exceeding this size will not be held on heap.,performance,cassandra
220,compaction_throughput_mb_per_sec,Throttles compaction to the given total throughput across the entire system.,performance,cassandra
221,sstable_preemptive_open_interval_in_mb,"This helps to smoothly transfer reads between the sstables, reducing page cache churn and keeping hot rows hot",performance,cassandra
222,read_request_timeout_in_ms,How long the coordinator should wait for read operations to complete,reliability,cassandra
223,range_request_timeout_in_ms,How long the coordinator should wait for seq or index scans to complete,reliability,cassandra
224,write_request_timeout_in_ms,How long the coordinator should wait for writes to complete,reliability,cassandra
225,counter_write_request_timeout_in_ms,How long the coordinator should wait for counter writes to complete,reliability,cassandra
226,cas_contention_timeout_in_ms,How long a coordinator should continue to retry a CAS operation that contends with other proposals for the same row,reliability,cassandra
227,truncate_request_timeout_in_ms,How long the coordinator should wait for truncates to complete ,reliability,cassandra
228,request_timeout_in_ms,"The default timeout for other, miscellaneous operations",reliability,cassandra
229,slow_query_log_timeout_in_ms,How long before a node logs slow queries. ,reliability,cassandra
230,cross_node_timeout,Enable operation timeout information exchange between nodes to accurately measure request timeouts.,reliability,cassandra
231,endpoint_snitch,Set this to a class that implements IEndpointSnitch.,others,cassandra
232,dynamic_snitch_update_interval_in_ms,controls how often to perform the more expensive part of host score calculation,performance,cassandra
233,dynamic_snitch_reset_interval_in_ms,"controls how often to reset all host scores, allowing a bad host to possibly recover",reliability,cassandra
234,internode_compression,internode_compression controls whether traffic between nodes is compressed,performance,cassandra
235,inter_dc_tcp_nodelay,"Enable or disable tcp_nodelay for inter-dc communication. Disabling it will result in larger (but fewer) network packets being sent, reducing overhead from the TCP protocol itself, at the cost of increasing latency if you block for cross-datacenter responses",performance,cassandra
236,tracetype_query_ttl,TTL for different trace types used during logging of the repair process.,reliability,cassandra
237,enable_user_defined_functions,"If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at INFO level UDFs (user defined functions) are disabled by default.",debuggability,cassandra
238,enable_scripted_user_defined_functions,Enables scripted UDFs (JavaScript UDFs).,others,cassandra
239,transparent_data_encryption_options,Enables encrypting data at-rest (on disk). ,security,cassandra
240,batch_size_warn_threshold_in_kb,Log WARN on any multiple-partition batch size exceeding this value.,debuggability,cassandra
241,batch_size_fail_threshold_in_kb,Fail any multiple-partition batch exceeding this value.,reliability,cassandra
242,compaction_large_partition_warning_threshold_mb,Log a warning when compacting partitions larger than this value,debuggability,cassandra
243,gc_log_threshold_in_ms,GC Pauses greater than 200 ms will be logged at INFO level This threshold can be adjusted to minimize logging if necessary,debuggability,cassandra
244,gc_warn_threshold_in_ms,GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level ,debuggability,cassandra
245,max_value_size_in_mb,Maximum size of any value in SSTables. Safety measure to detect SSTable corruption early. ,security,cassandra
246,back_pressure_enabled,"Back-pressure settings # If enabled, the coordinator will apply the back-pressure strategy specified below to each mutation sent to replicas, with the aim of reducing pressure on overloaded replicas.",performance,cassandra
247,otc_coalescing_window_us,How many microseconds to wait for coalescing. ,reliability,cassandra
248,otc_coalescing_enough_coalesced_messages,Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.,others,cassandra
249,otc_backlog_expiration_interval_ms,How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.,reliability,cassandra
250,ideal_consistency_level,Track a metric per keyspace indicating whether replication achieved the ideal consistency level for writes without timing out. This is different from the consistency level requested by each write which may be lower in order to facilitate availability.,reliability,cassandra
251,hadoop.common.configuration.version,version of this configuration file,environment,hadoop-common
252,hadoop.tmp.dir,A base for other temporary directories.,environment,hadoop-common
253,io.native.lib.available,Controls whether to use native libraries for bz2 and zlib compression codecs or not. The property does not control any other native libraries.,environment,hadoop-common
254,hadoop.http.filter.initializers,"A comma separated list of class names. Each class in the list must extend org.apache.hadoop.http.FilterInitializer. The corresponding Filter will be initialized. Then, the Filter will be applied to all user facing jsp and servlet web pages. The ordering of the list defines the ordering of the filters.",others,hadoop-common
255,hadoop.security.authorization,Is service-level authorization enabled?,security,hadoop-common
256,hadoop.security.instrumentation.requires.admin,"Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS).",security,hadoop-common
257,hadoop.security.authentication,"Possible values are simple (no authentication), and kerberos",security,hadoop-common
258,hadoop.security.group.mapping,"Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used. This implementation shells out to the Linux/Unix environment with the bash -c groups command to resolve a list of groups for a user.",security,hadoop-common
259,hadoop.security.dns.interface,"The name of the Network Interface from which the service should determine its host name for Kerberos login. e.g. eth2. In a multi-homed environment, the setting can be used to affect the _HOST substitution in the service Kerberos principal. If this configuration value is not set, the service will use its default hostname as returned by InetAddress.getLocalHost().getCanonicalHostName(). Most clusters will not require this setting.",security,hadoop-common
260,hadoop.security.dns.nameserver,The host name or IP address of the name server (DNS) which a service Node should use to determine its own host name for Kerberos Login. Requires hadoop.security.dns.interface. Most clusters will not require this setting.,environment,hadoop-common
261,hadoop.security.dns.log-slow-lookups.enabled,Time name lookups (via SecurityUtil) and log them if they exceed the configured threshold.,security,hadoop-common
262,hadoop.security.dns.log-slow-lookups.threshold.ms,"If slow lookup logging is enabled, this threshold is used to decide if a lookup is considered slow enough to be logged.",security,hadoop-common
263,hadoop.security.groups.cache.secs,"This is the config controlling the validity of the entries in the cache containing the user->group mapping. When this duration has expired, then the implementation of the group mapping provider is invoked to get the groups of the user and then cached back.",security,hadoop-common
264,hadoop.security.groups.negative-cache.secs,"Expiration time for entries in the the negative user-to-group mapping caching, in seconds. This is useful when invalid users are retrying frequently. It is suggested to set a small value for this expiration, since a transient error in group lookup could temporarily lock out a legitimate user. Set this to zero or negative value to disable negative user-to-group caching.",security,hadoop-common
265,hadoop.security.groups.cache.warn.after.ms,"If looking up a single user to group takes longer than this amount of milliseconds, we will log a warning message.",reliability,hadoop-common
266,hadoop.security.groups.cache.background.reload,"Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.",security,hadoop-common
267,hadoop.security.groups.cache.background.reload.threads,Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.,performance,hadoop-common
268,hadoop.security.groups.shell.command.timeout,"Used by the ShellBasedUnixGroupsMapping class, this property controls how long to wait for the underlying shell command that is run to fetch groups. Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes longer than the value configured, the command is aborted and the groups resolver would return a result of no groups found. A value of 0s (default) would mean an infinite wait (i.e. wait until the command exits on its own).",reliability,hadoop-common
269,hadoop.security.group.mapping.ldap.connection.timeout.ms,"This property is the connection timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't establish a connection within the specified period, it will abort the connect attempt. Non-positive value means no LDAP connection timeout is specified in which case it waits for the connection to establish until the underlying network times out.",reliability,hadoop-common
270,hadoop.security.group.mapping.ldap.read.timeout.ms,"This property is the read timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't get a LDAP response within the specified period, it will abort the read attempt. Non-positive value means no read timeout is specified in which case it waits for the response infinitely.",reliability,hadoop-common
271,hadoop.security.group.mapping.ldap.url,The URL of the LDAP server to use for resolving user groups when using the LdapGroupsMapping user to group mapping.,security,hadoop-common
272,hadoop.security.group.mapping.ldap.ssl,Whether or not to use SSL when connecting to the LDAP server.,security,hadoop-common
273,hadoop.security.group.mapping.ldap.ssl.keystore,File path to the SSL keystore that contains the SSL certificate required by the LDAP server.,environment,hadoop-common
274,hadoop.security.group.mapping.ldap.ssl.keystore.password.file,"The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",security,hadoop-common
275,hadoop.security.group.mapping.ldap.ssl.keystore.password,The password of the LDAP SSL keystore. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,security,hadoop-common
276,hadoop.security.credential.clear-text-fallback,true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.,security,hadoop-common
277,hadoop.security.credential.provider.path,A comma-separated list of URLs that indicates the type and location of a list of providers that should be consulted.,security,hadoop-common
278,hadoop.security.credstore.java-keystore-provider.password-file,The path to a file containing the custom password for all keystores that may be configured in the provider path.,security,hadoop-common
279,hadoop.security.group.mapping.ldap.ssl.truststore,File path to the SSL truststore that contains the root certificate used to sign the LDAP server's certificate. Specify this if the LDAP server's certificate is not signed by a well known certificate authority.,security,hadoop-common
280,hadoop.security.group.mapping.ldap.ssl.truststore.password.file,The path to a file containing the password of the LDAP SSL truststore. IMPORTANT: This file should be readable only by the Unix user running the daemons.,security,hadoop-common
281,hadoop.security.group.mapping.ldap.bind.user,The distinguished name of the user to bind as when connecting to the LDAP server. This may be left blank if the LDAP server supports anonymous binds.,security,hadoop-common
282,hadoop.security.group.mapping.ldap.bind.password.file,"The path to a file containing the password of the bind user. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",security,hadoop-common
283,hadoop.security.group.mapping.ldap.bind.password,The password of the bind user. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,security,hadoop-common
284,hadoop.security.group.mapping.ldap.base,"The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.",environment,hadoop-common
285,hadoop.security.group.mapping.ldap.userbase,"The search base for the LDAP connection for user search query. This is a distinguished name, and its the root of the LDAP directory for users. If not set, hadoop.security.group.mapping.ldap.base is used.",environment,hadoop-common
286,hadoop.security.group.mapping.ldap.groupbase,"The search base for the LDAP connection for group search . This is a distinguished name, and its the root of the LDAP directory for groups. If not set, hadoop.security.group.mapping.ldap.base is used.",environment,hadoop-common
287,hadoop.security.group.mapping.ldap.search.filter.user,"An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to an LDAP server with a non-AD schema, this should be replaced with (&(objectClass=inetOrgPerson)(uid={0}). {0} is a special string used to denote where the username fits into the filter. If the LDAP server supports posixGroups, Hadoop can enable the feature by setting the value of this property to ""posixAccount"" and the value of the hadoop.security.group.mapping.ldap.search.filter.group property to ""posixGroup"".",security,hadoop-common
288,hadoop.security.group.mapping.ldap.search.filter.group,An additional filter to use when searching for LDAP groups. This should be changed when resolving groups against a non-Active Directory installation. See the description of hadoop.security.group.mapping.ldap.search.filter.user to enable posixGroups support.,security,hadoop-common
289,hadoop.security.group.mapping.ldap.search.attr.memberof,"The attribute of the user object that identifies its group objects. By default, Hadoop makes two LDAP queries per user if this value is empty. If set, Hadoop will attempt to resolve group names from this attribute, instead of making the second LDAP query to get group objects. The value should be 'memberOf' for an MS AD installation.",security,hadoop-common
290,hadoop.security.group.mapping.ldap.search.attr.member,The attribute of the group object that identifies the users that are members of the group. The default will usually be appropriate for any LDAP installation.,security,hadoop-common
291,hadoop.security.group.mapping.ldap.search.attr.group.name,The attribute of the group object that identifies the group name. The default will usually be appropriate for all LDAP systems.,security,hadoop-common
292,hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to. Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.,security,hadoop-common
293,hadoop.security.group.mapping.ldap.posix.attr.uid.name,The attribute of posixAccount to use when groups for membership. Mostly useful for schemas wherein groups have memberUids that use an attribute other than uidNumber.,security,hadoop-common
294,hadoop.security.group.mapping.ldap.posix.attr.gid.name,The attribute of posixAccount indicating the group id.,security,hadoop-common
295,hadoop.security.group.mapping.ldap.directory.search.timeout,The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.,reliability,hadoop-common
296,hadoop.security.group.mapping.providers,Comma separated of names of other providers to provide user to group mapping. Used by CompositeGroupsMapping.,security,hadoop-common
297,hadoop.security.group.mapping.providers.combined,"true or false to indicate whether groups from the providers are combined or not. The default value is true. If true, then all the providers will be tried to get groups and all the groups are combined to return as the final results. Otherwise, providers are tried one by one in the configured list order, and if any groups are retrieved from any provider, then the groups will be returned without trying the left ones.",security,hadoop-common
298,hadoop.security.service.user.name.key,"For those cases where the same RPC protocol is implemented by multiple servers, this configuration is required for specifying the principal name to use for the service when the client wishes to make an RPC call.",security,hadoop-common
299,fs.azure.user.agent.prefix,"WASB passes User-Agent header to the Azure back-end. The default value contains WASB version, Java Runtime version, Azure Client library version, and the value of the configuration option fs.azure.user.agent.prefix.",others,hadoop-common
300,hadoop.security.uid.cache.secs,This is the config controlling the validity of the entries in the cache containing the userId to userName and groupId to groupName used by NativeIO getFstat().,security,hadoop-common
301,hadoop.service.shutdown.timeout,"Timeout to wait for each shutdown operation to complete. If a hook takes longer than this time to complete, it will be interrupted, so the service will shutdown. This allows the service shutdown to recover from a blocked operation. Some shutdown hooks may need more time than this, for example when a large amount of data needs to be uploaded to an object store. In this situation: increase the timeout. The minimum duration of the timeout is 1 second, ""1s"".",reliability,hadoop-common
302,hadoop.rpc.protection,"A comma-separated list of protection values for secured sasl connections. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side.",security,hadoop-common
303,hadoop.security.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection. If not specified, the full set of values specified in hadoop.rpc.protection is used while determining the QOP used for the connection. If a class is specified, then the QOP values returned by the class will be used while determining the QOP used for the connection.",security,hadoop-common
304,hadoop.security.sensitive-config-keys,"A comma-separated or multi-line list of regular expressions to match configuration keys that should be redacted where appropriate, for example, when logging modified properties during a reconfiguration, private credentials should not be logged.",security,hadoop-common
305,hadoop.workaround.non.threadsafe.getpwuid,"Some operating systems or authentication modules are known to have broken implementations of getpwuid_r and getpwgid_r, such that these calls are not thread-safe. Symptoms of this problem include JVM crashes with a stack trace inside these functions. If your system exhibits this issue, enable this configuration parameter to include a lock around the calls as a workaround. An incomplete list of some systems known to have this issue is available at https://wiki.apache.org/hadoop/KnownBrokenPwuidImplementations",reliability,hadoop-common
306,hadoop.kerberos.kinit.command,Used to periodically renew Kerberos credentials when provided to Hadoop. The default setting assumes that kinit is in the PATH of users running the Hadoop client. Change this to the absolute path to kinit if this is not the case.,security,hadoop-common
307,hadoop.kerberos.min.seconds.before.relogin,"The minimum time between relogin attempts for Kerberos, in seconds.",reliability,hadoop-common
308,hadoop.security.auth_to_local,Maps kerberos principals to local user names,security,hadoop-common
309,hadoop.token.files,List of token cache files that have delegation tokens for hadoop service,security,hadoop-common
310,io.file.buffer.size,"The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hadoop-common
311,io.bytes.per.checksum,The number of bytes per checksum. Must not be larger than io.file.buffer.size.,performance,hadoop-common
312,io.skip.checksum.errors,"If true, when a checksum error is encountered while reading a sequence file, entries are skipped, instead of throwing an exception.",reliability,hadoop-common
313,io.compression.codecs,"A comma-separated list of the compression codec classes that can be used for compression/decompression. In addition to any classes specified with this property (which take precedence), codec classes on the classpath are discovered using a Java ServiceLoader.",performance,hadoop-common
314,io.compression.codec.bzip2.library,"The native-code library to be used for compression and decompression by the bzip2 codec. This library could be specified either by by name or the full pathname. In the former case, the library is located by the dynamic linker, usually searching the directories specified in the environment variable LD_LIBRARY_PATH. The value of ""system-native"" indicates that the default system library should be used. To indicate that the algorithm should operate entirely in Java, specify ""java-builtin"".",environment,hadoop-common
315,io.serializations,A list of serialization classes that can be used for obtaining serializers and deserializers.,performance,hadoop-common
316,io.seqfile.local.dir,The local directory where sequence file stores intermediate data files during merge. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,environment,hadoop-common
317,io.map.index.skip,Number of index entries to skip between each entry. Zero by default. Setting this to values larger than zero can facilitate opening large MapFiles using less memory.,performance,hadoop-common
318,io.map.index.interval,"MapFile consist of two files - data file (tuples) and index file (keys). For every io.map.index.interval records written in the data file, an entry (record-key, data-file-position) is written in the index file. This is to allow for doing binary search later within the index file to look up records by their keys and get their closest positions in the data file.",performance,hadoop-common
319,fs.defaultFS,"The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem.",environment,hadoop-common
320,fs.default.name,Deprecated. Use (fs.defaultFS) property instead,environment,hadoop-common
321,fs.trash.interval,"Number of minutes after which the checkpoint gets deleted. If zero, the trash feature is disabled. This option may be configured both on the server and the client. If trash is disabled server side then the client side configuration is checked. If trash is enabled on the server side then the value configured on the server is used and the client configuration value is ignored.",performance,hadoop-common
322,fs.trash.checkpoint.interval,"Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.",reliability,hadoop-common
323,fs.protected.directories,A comma-separated list of directories which cannot be deleted even by the superuser unless they are empty. This setting can be used to guard important system directories against accidental deletion due to administrator error.,security,hadoop-common
324,fs.AbstractFileSystem.file.impl,The AbstractFileSystem for file: uris.,others,hadoop-common
325,fs.AbstractFileSystem.har.impl,The AbstractFileSystem for har: uris.,others,hadoop-common
326,fs.AbstractFileSystem.hdfs.impl,The FileSystem for hdfs: uris.,others,hadoop-common
327,fs.AbstractFileSystem.viewfs.impl,The AbstractFileSystem for view file system for viewfs: uris (ie client side mount table:).,others,hadoop-common
328,fs.viewfs.rename.strategy,"Allowed rename strategy to rename between multiple mountpoints. Allowed values are SAME_MOUNTPOINT,SAME_TARGET_URI_ACROSS_MOUNTPOINT and SAME_FILESYSTEM_ACROSS_MOUNTPOINT.",others,hadoop-common
329,fs.AbstractFileSystem.ftp.impl,The FileSystem for Ftp: uris.,others,hadoop-common
330,fs.AbstractFileSystem.webhdfs.impl,The FileSystem for webhdfs: uris.,others,hadoop-common
331,fs.AbstractFileSystem.swebhdfs.impl,The FileSystem for swebhdfs: uris.,others,hadoop-common
332,fs.ftp.host,FTP filesystem connects to this server,environment,hadoop-common
333,fs.ftp.host.port,FTP filesystem connects to fs.ftp.host on this port,environment,hadoop-common
334,fs.ftp.data.connection.mode,"Set the FTPClient's data connection mode based on configuration. Valid values are ACTIVE_LOCAL_DATA_CONNECTION_MODE, PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.",others,hadoop-common
335,fs.ftp.transfer.mode,"Set FTP's transfer mode based on configuration. Valid values are STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.",others,hadoop-common
336,fs.df.interval,Disk usage statistics refresh interval in msec.,debuggability,hadoop-common
337,fs.du.interval,File space usage statistics refresh interval in msec.,debuggability,hadoop-common
338,fs.s3.awsAccessKeyId,AWS access key ID used by S3 block file system.,security,hadoop-common
339,fs.s3.awsSecretAccessKey,AWS secret key used by S3 block file system.,security,hadoop-common
340,fs.s3.block.size,Block size to use when writing files to S3.,performance,hadoop-common
341,fs.s3.buffer.dir,Determines where on the local filesystem the s3:/s3n: filesystem should store files before sending them to S3 (or after retrieving them from S3).,environment,hadoop-common
342,fs.s3.maxRetries,"The maximum number of retries for reading or writing files to S3, before we signal failure to the application.",reliability,hadoop-common
343,fs.s3.sleepTimeSeconds,The number of seconds to sleep between each S3 retry.,reliability,hadoop-common
344,fs.swift.impl,The implementation class of the OpenStack Swift Filesystem,others,hadoop-common
345,fs.automatic.close,"By default, FileSystem instances are automatically closed at program exit using a JVM shutdown hook. Setting this property to false disables this behavior. This is an advanced option that should only be used by server applications requiring a more carefully orchestrated shutdown sequence.",reliability,hadoop-common
346,fs.s3n.awsAccessKeyId,AWS access key ID used by S3 native file system.,security,hadoop-common
347,fs.s3n.awsSecretAccessKey,AWS secret key used by S3 native file system.,security,hadoop-common
348,fs.s3n.block.size,Block size to use when reading files using the native S3 filesystem (s3n: URIs).,performance,hadoop-common
349,fs.s3n.multipart.uploads.enabled,"Setting this property to true enables multiple uploads to native S3 filesystem. When uploading a file, it is split into blocks if the size is larger than fs.s3n.multipart.uploads.block.size.",performance,hadoop-common
350,fs.s3n.multipart.uploads.block.size,The block size for multipart uploads to native S3 filesystem. Default size is 64MB.,performance,hadoop-common
351,fs.s3n.multipart.copy.block.size,The block size for multipart copy in native S3 filesystem. Default size is 5GB.,performance,hadoop-common
352,fs.s3n.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for S3. Unset by default, and the only other currently allowable value is AES256.",security,hadoop-common
353,fs.s3a.access.key,AWS access key ID used by S3A file system. Omit for IAM role-based or provider-based authentication.,security,hadoop-common
354,fs.s3a.secret.key,AWS secret key used by S3A file system. Omit for IAM role-based or provider-based authentication.,security,hadoop-common
355,fs.s3a.aws.credentials.provider,"Comma-separated class names of credential provider classes which implement com.amazonaws.auth.AWSCredentialsProvider. These are loaded and queried in sequence for a valid set of credentials. Each listed class must implement one of the following means of construction, which are attempted in order: 1. a public constructor accepting java.net.URI and org.apache.hadoop.conf.Configuration, 2. a public static method named getInstance that accepts no arguments and returns an instance of com.amazonaws.auth.AWSCredentialsProvider, or 3. a public default constructor. Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows anonymous access to a publicly accessible S3 bucket without any credentials. Please note that allowing anonymous access to an S3 bucket compromises security and therefore is unsuitable for most use cases. It can be useful for accessing public data sets without requiring AWS credentials. If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider: supports static configuration of AWS access key ID and secret access key. See also fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports configuration of AWS access key ID and secret access key in environment variables named AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider: a shared instance of com.amazonaws.auth.InstanceProfileCredentialsProvider from the AWS SDK, which supports use of instance profile credentials if running in an EC2 VM. Using this shared instance potentially reduces load on the EC2 instance metadata service for multi-threaded applications.",security,hadoop-common
356,fs.s3a.session.token,"Session token, when using org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider as one of the providers.",security,hadoop-common
357,fs.s3a.security.credential.provider.path,"Optional comma separated list of credential providers, a list which is prepended to that set in hadoop.security.credential.provider.path",security,hadoop-common
358,fs.s3a.connection.maximum,Controls the maximum number of simultaneous connections to S3.,performance,hadoop-common
359,fs.s3a.connection.ssl.enabled,Enables or disables SSL connections to S3.,security,hadoop-common
360,fs.s3a.endpoint,"AWS S3 endpoint to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the standard region (s3.amazonaws.com) is assumed.",environment,hadoop-common
361,fs.s3a.path.style.access,Enable S3 path style access ie disabling the default virtual hosting behaviour. Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.,security,hadoop-common
362,fs.s3a.proxy.host,Hostname of the (optional) proxy server for S3 connections.,environment,hadoop-common
363,fs.s3a.proxy.port,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",environment,hadoop-common
364,fs.s3a.proxy.username,Username for authenticating with proxy server.,security,hadoop-common
365,fs.s3a.proxy.password,Password for authenticating with proxy server.,security,hadoop-common
366,fs.s3a.proxy.domain,Domain for authenticating with proxy server.,security,hadoop-common
367,fs.s3a.proxy.workstation,Workstation for authenticating with proxy server.,security,hadoop-common
368,fs.s3a.attempts.maximum,How many times we should retry commands on transient errors.,reliability,hadoop-common
369,fs.s3a.connection.establish.timeout,Socket connection setup timeout in milliseconds.,reliability,hadoop-common
370,fs.s3a.connection.timeout,Socket connection timeout in milliseconds.,reliability,hadoop-common
371,fs.s3a.socket.send.buffer,Socket send buffer hint to amazon connector. Represented in bytes.,performance,hadoop-common
372,fs.s3a.socket.recv.buffer,Socket receive buffer hint to amazon connector. Represented in bytes.,performance,hadoop-common
373,fs.s3a.paging.maximum,How many keys to request from S3 when doing directory listings at a time.,performance,hadoop-common
374,fs.s3a.threads.max,The total number of threads available in the filesystem for data uploads *or any other queued filesystem operation*.,performance,hadoop-common
375,fs.s3a.threads.keepalivetime,Number of seconds a thread can be idle before being terminated.,reliability,hadoop-common
376,fs.s3a.max.total.tasks,The number of operations which can be queued for execution,reliability,hadoop-common
377,fs.s3a.multipart.size,"How big (in bytes) to split upload or copy operations up into. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,hadoop-common
378,fs.s3a.multipart.threshold,"How big (in bytes) to split upload or copy operations up into. This also controls the partition size in renamed files, as rename() involves copying the source file(s). A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,hadoop-common
379,fs.s3a.multiobjectdelete.enable,"When enabled, multiple single-object delete requests are replaced by a single 'delete multiple objects'-request, reducing the number of requests. Beware: legacy S3-compatible object stores might not support this request.",performance,hadoop-common
380,fs.s3a.acl.default,"Set a canned ACL for newly created and copied objects. Value may be Private, PublicRead, PublicReadWrite, AuthenticatedRead, LogDeliveryWrite, BucketOwnerRead, or BucketOwnerFullControl.",security,hadoop-common
381,fs.s3a.multipart.purge,"True if you want to purge existing multipart uploads that may not have been completed/aborted correctly. The corresponding purge age is defined in fs.s3a.multipart.purge.age. If set, when the filesystem is instantiated then all outstanding uploads older than the purge age will be terminated -across the entire bucket. This will impact multipart uploads by other applications and users. so should be used sparingly, with an age value chosen to stop failed uploads, without breaking ongoing operations.",reliability,hadoop-common
382,fs.s3a.multipart.purge.age,Minimum age in seconds of multipart uploads to purge.,reliability,hadoop-common
383,fs.s3a.server-side-encryption-algorithm,"Specify a server-side encryption algorithm for s3a: file system. Unset by default. It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS' and 'SSE-C'.",security,hadoop-common
384,fs.s3a.server-side-encryption.key,"Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty, you'll be using your default's S3 KMS key, otherwise you should set this property to the specific KMS key id.",security,hadoop-common
385,fs.s3a.signing-algorithm,Override the default signing algorithm so legacy implementations can still be used,security,hadoop-common
386,fs.s3a.block.size,"Block size to use when reading files using s3a: file system. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,hadoop-common
387,fs.s3a.buffer.dir,Comma separated list of directories that will be used to buffer file uploads to.,environment,hadoop-common
388,fs.s3a.fast.upload,Use the incremental block-based fast upload mechanism with the buffering mechanism set in fs.s3a.fast.upload.buffer.,performance,hadoop-common
389,fs.s3a.fast.upload.buffer,"The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by: fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.",performance,hadoop-common
390,fs.s3a.fast.upload.active.blocks,"Maximum Number of blocks a single output stream can have active (uploading, or queued to the central FileSystem instance's pool of queued operations. This stops a single stream overloading the shared thread pool.",performance,hadoop-common
391,fs.s3a.readahead.range,"Bytes to read ahead during a seek() before closing and re-opening the S3 HTTP connection. This option will be overridden if any call to setReadahead() is made to an open stream. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,hadoop-common
392,fs.s3a.user.agent.prefix,"Sets a custom value that will be prepended to the User-Agent header sent in HTTP requests to the S3 back-end by S3AFileSystem. The User-Agent header always includes the Hadoop version number followed by a string generated by the AWS SDK. An example is ""User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6"". If this optional property is set, then its value is prepended to create a customized User-Agent. For example, if this configuration property was set to ""MyApp"", then an example of the resulting User-Agent would be ""User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6"".",others,hadoop-common
393,fs.s3a.metadatastore.authoritative,"When true, allow MetadataStore implementations to act as source of truth for getting file status and directory listings. Even if this is set to true, MetadataStore implementations may choose not to return authoritative results. If the configured MetadataStore does not support being authoritative, this setting will have no effect.",security,hadoop-common
394,fs.s3a.metadatastore.impl,"Fully-qualified name of the class that implements the MetadataStore to be used by s3a. The default class, NullMetadataStore, has no effect: s3a will continue to treat the backing S3 service as the one and only source of truth for file and directory metadata.",others,hadoop-common
395,fs.s3a.s3guard.cli.prune.age,Default age (in milliseconds) after which to prune metadata from the metadatastore when the prune command is run. Can be overridden on the command-line.,reliability,hadoop-common
396,fs.s3a.impl,The implementation class of the S3A Filesystem,others,hadoop-common
397,fs.s3a.s3guard.ddb.region,"AWS DynamoDB region to connect to. An up-to-date list is provided in the AWS Documentation: regions and endpoints. Without this property, the S3Guard will operate table in the associated S3 bucket region.",environment,hadoop-common
398,fs.s3a.s3guard.ddb.table,"The DynamoDB table name to operate. Without this property, the respective S3 bucket name will be used.",others,hadoop-common
399,fs.s3a.s3guard.ddb.table.create,"If true, the S3A client will create the table if it does not already exist.",others,hadoop-common
400,fs.s3a.s3guard.ddb.table.capacity.read,"Provisioned throughput requirements for read operations in terms of capacity units for the DynamoDB table. This config value will only be used when creating a new DynamoDB table, though later you can manually provision by increasing or decreasing read capacity as needed for existing tables. See DynamoDB documents for more information.",performance,hadoop-common
401,fs.s3a.s3guard.ddb.table.capacity.write,Provisioned throughput requirements for write operations in terms of capacity units for the DynamoDB table. Refer to related config fs.s3a.s3guard.ddb.table.capacity.read before usage.,performance,hadoop-common
402,fs.s3a.s3guard.ddb.max.retries,"Max retries on batched DynamoDB operations before giving up and throwing an IOException. Each retry is delayed with an exponential backoff timer which starts at 100 milliseconds and approximately doubles each time. The minimum wait before throwing an exception is sum(100, 200, 400, 800, .. 100*2^N-1 ) == 100 * ((2^N)-1) So N = 9 yields at least 51.1 seconds (51,100) milliseconds of blocking before throwing an IOException.",reliability,hadoop-common
403,fs.s3a.s3guard.ddb.background.sleep,Length (in milliseconds) of pause between each batch of deletes when pruning metadata. Prevents prune operations (which can typically be low priority background operations) from overly interfering with other I/O operations.,reliability,hadoop-common
404,fs.AbstractFileSystem.s3a.impl,The implementation class of the S3A AbstractFileSystem.,others,hadoop-common
405,fs.wasb.impl,The implementation class of the Native Azure Filesystem,others,hadoop-common
406,fs.wasbs.impl,The implementation class of the Secure Native Azure Filesystem,others,hadoop-common
407,fs.azure.secure.mode,"Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs to run under. Setting it ""true"" would make fs.azure.NativeAzureFileSystem use SAS keys to communicate with Azure storage.",security,hadoop-common
408,fs.azure.local.sas.key.mode,"Works in conjuction with fs.azure.secure.mode. Setting this config to true results in fs.azure.NativeAzureFileSystem using the local SAS key generation where the SAS keys are generating in the same process as fs.azure.NativeAzureFileSystem. If fs.azure.secure.mode flag is set to false, this flag has no effect.",security,hadoop-common
409,fs.azure.sas.expiry.period,"The default value to be used for expiration period for SAS keys generated. Can use the following suffix (case insensitive): ms(millis), s(sec), m(min), h(hour), d(day) to specify the time (such as 2s, 2m, 1h, etc.).",security,hadoop-common
410,fs.azure.authorization,"Config flag to enable authorization support in WASB. Setting it to ""true"" enables authorization support to WASB. Currently WASB authorization requires a remote service to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url configuration",security,hadoop-common
411,fs.azure.authorization.caching.enable,Config flag to enable caching of authorization results and saskeys in WASB. This flag is relevant only when fs.azure.authorization is enabled.,security,hadoop-common
412,fs.azure.saskey.usecontainersaskeyforallaccess,Use container saskey for access to all blobs within the container. Blob-specific saskeys are not used when this setting is enabled. This setting provides better performance compared to blob-specific saskeys.,security,hadoop-common
413,io.seqfile.compress.blocksize,The minimum block size for compression in block compressed SequenceFiles.,performance,hadoop-common
414,io.mapfile.bloom.size,"The size of BloomFilter-s used in BloomMapFile. Each time this many keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter). Larger values minimize the number of filters, which slightly increases the performance, but may waste too much space if the total number of keys is usually much smaller than this number.",performance,hadoop-common
415,io.mapfile.bloom.error.rate,"The rate of false positives in BloomFilter-s used in BloomMapFile. As this value decreases, the size of BloomFilter-s increases exponentially. This value is the probability of encountering false positives (default is 0.5%).",performance,hadoop-common
416,hadoop.util.hash.type,The default implementation of Hash. Currently this can take one of the two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.,others,hadoop-common
417,ipc.client.idlethreshold,Defines the threshold number of connections after which connections will be inspected for idleness.,reliability,hadoop-common
418,ipc.client.kill.max,Defines the maximum number of clients to disconnect in one go.,reliability,hadoop-common
419,ipc.client.connection.maxidletime,The maximum time in msec after which a client will bring down the connection to the server.,reliability,hadoop-common
420,ipc.client.connect.max.retries,Indicates the number of retries a client will make to establish a server connection.,reliability,hadoop-common
421,ipc.client.connect.retry.interval,Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.,reliability,hadoop-common
422,ipc.client.connect.timeout,Indicates the number of milliseconds a client will wait for the socket to establish a server connection.,reliability,hadoop-common
423,ipc.client.connect.max.retries.on.timeouts,Indicates the number of retries a client will make on socket timeout to establish a server connection.,reliability,hadoop-common
424,ipc.client.tcpnodelay,Use TCP_NODELAY flag to bypass Nagle's algorithm transmission delays.,performance,hadoop-common
425,ipc.client.low-latency,Use low-latency QoS markers for IPC connections.,performance,hadoop-common
426,ipc.client.ping,"Send a ping to the server when timeout on reading the response, if set to true. If no failure is detected, the client retries until at least a byte is read or the time given by ipc.client.rpc-timeout.ms is passed.",reliability,hadoop-common
427,ipc.ping.interval,"Timeout on waiting response from server, in milliseconds. The client will send ping when the interval is passed without receiving bytes, if ipc.client.ping is set to true.",reliability,hadoop-common
428,ipc.client.rpc-timeout.ms,"Timeout on waiting response from server, in milliseconds. If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval.",reliability,hadoop-common
429,ipc.server.listen.queue.size,Indicates the length of the listen queue for servers accepting client connections.,performance,hadoop-common
430,ipc.server.log.slow.rpc,This setting is useful to troubleshoot performance issues for various services. If this value is set to true then we log requests that fall into 99th percentile as well as increment RpcSlowCalls counter.,debuggability,hadoop-common
431,ipc.maximum.data.length,This indicates the maximum IPC message length (bytes) that can be accepted by the server. Messages larger than this value are rejected by the immediately to avoid possible OOMs. This setting should rarely need to be changed.,reliability,hadoop-common
432,ipc.maximum.response.length,This indicates the maximum IPC message length (bytes) that can be accepted by the client. Messages larger than this value are rejected immediately to avoid possible OOMs. This setting should rarely need to be changed. Set to 0 to disable.,reliability,hadoop-common
433,hadoop.security.impersonation.provider.class,"A class which implements ImpersonationProvider interface, used to authorize whether one user can impersonate a specific user. If not specified, the DefaultImpersonationProvider will be used. If a class is specified, then that class will be used to determine the impersonation capability.",security,hadoop-common
434,hadoop.rpc.socket.factory.class.default,"Default SocketFactory to use. This parameter is expected to be formatted as ""package.FactoryClassName"".",environment,hadoop-common
435,hadoop.rpc.socket.factory.class.ClientProtocol,"SocketFactory to use to connect to a DFS. If null or empty, use hadoop.rpc.socket.class.default. This socket factory is also used by DFSClient to create sockets to DataNodes.",environment,hadoop-common
436,hadoop.socks.server,Address (host:port) of the SOCKS server to be used by the SocksSocketFactory.,environment,hadoop-common
437,net.topology.node.switch.mapping.impl,"The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.",others,hadoop-common
438,net.topology.impl,The default implementation of NetworkTopology which is classic three layer one.,others,hadoop-common
439,net.topology.script.file.name,"The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.",environment,hadoop-common
440,net.topology.script.number.args,The max number of args that the script configured with net.topology.script.file.name should be run with. Each arg is an IP address.,others,hadoop-common
441,net.topology.table.file.name,"The file name for a topology file, which is used when the net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.",environment,hadoop-common
442,file.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hadoop-common
443,file.bytes-per-checksum,The number of bytes per checksum. Must not be larger than file.stream-buffer-size,performance,hadoop-common
444,file.client-write-packet-size,Packet size for clients to write,performance,hadoop-common
445,file.blocksize,Block size,performance,hadoop-common
446,file.replication,Replication factor,reliability,hadoop-common
447,s3.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hadoop-common
448,s3.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3.stream-buffer-size,performance,hadoop-common
449,s3.client-write-packet-size,Packet size for clients to write,performance,hadoop-common
450,s3.blocksize,Block size,performance,hadoop-common
451,s3.replication,Replication factor,reliability,hadoop-common
452,s3native.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hadoop-common
453,s3native.bytes-per-checksum,The number of bytes per checksum. Must not be larger than s3native.stream-buffer-size,performance,hadoop-common
454,s3native.client-write-packet-size,Packet size for clients to write,performance,hadoop-common
455,s3native.blocksize,Block size,performance,hadoop-common
456,s3native.replication,Replication factor,reliability,hadoop-common
457,ftp.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hadoop-common
458,ftp.bytes-per-checksum,The number of bytes per checksum. Must not be larger than ftp.stream-buffer-size,performance,hadoop-common
459,ftp.client-write-packet-size,Packet size for clients to write,performance,hadoop-common
460,ftp.blocksize,Block size,performance,hadoop-common
461,ftp.replication,Replication factor,reliability,hadoop-common
462,tfile.io.chunk.size,Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown()).,performance,hadoop-common
463,tfile.fs.output.buffer.size,Buffer size used for FSDataOutputStream in bytes.,performance,hadoop-common
464,tfile.fs.input.buffer.size,Buffer size used for FSDataInputStream in bytes.,performance,hadoop-common
465,hadoop.http.authentication.type,Defines authentication used for Oozie HTTP endpoint. Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#,security,hadoop-common
466,hadoop.http.authentication.token.validity,Indicates how long (in seconds) an authentication token is valid before it has to be renewed.,security,hadoop-common
467,hadoop.http.authentication.signature.secret.file,The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.,security,hadoop-common
468,hadoop.http.authentication.cookie.domain,"The domain to use for the HTTP cookie that stores the authentication token. In order to authentiation to work correctly across all Hadoop nodes web-consoles the domain must be correctly set. IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings. For this setting to work properly all nodes in the cluster must be configured to generate URLs with hostname.domain names on it.",security,hadoop-common
469,hadoop.http.authentication.simple.anonymous.allowed,Indicates if anonymous requests are allowed when using 'simple' authentication.,security,hadoop-common
470,hadoop.http.authentication.kerberos.principal,Indicates the Kerberos principal to be used for HTTP endpoint. The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.,security,hadoop-common
471,hadoop.http.authentication.kerberos.keytab,Location of the keytab file with the credentials for the principal. Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.,security,hadoop-common
472,hadoop.http.cross-origin.enabled,Enable/disable the cross-origin (CORS) filter.,others,hadoop-common
473,hadoop.http.cross-origin.allowed-origins,Comma separated list of origins that are allowed for web services needing cross-origin (CORS) support. Wildcards (*) and patterns allowed,others,hadoop-common
474,hadoop.http.cross-origin.allowed-methods,Comma separated list of methods that are allowed for web services needing cross-origin (CORS) support.,others,hadoop-common
475,hadoop.http.cross-origin.allowed-headers,Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support.,others,hadoop-common
476,hadoop.http.cross-origin.max-age,The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support.,reliability,hadoop-common
477,dfs.ha.fencing.methods,List of fencing methods to use for service fencing. May contain builtin methods (eg shell and sshfence) or user-defined method.,reliability,hadoop-common
478,dfs.ha.fencing.ssh.connect-timeout,"SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.",reliability,hadoop-common
479,dfs.ha.fencing.ssh.private-key-files,The SSH private key files to use with the builtin sshfence fencer.,security,hadoop-common
480,hadoop.http.staticuser.user,"The user name to filter as, on static web filters while rendering content. An example use is the HDFS web UI (user to be used for browsing files).",others,hadoop-common
481,ha.zookeeper.quorum,"A list of ZooKeeper server addresses, separated by commas, that are to be used by the ZKFailoverController in automatic failover.",environment,hadoop-common
482,ha.zookeeper.session-timeout.ms,"The session timeout to use when the ZKFC connects to ZooKeeper. Setting this value to a lower value implies that server crashes will be detected more quickly, but risks triggering failover too aggressively in the case of a transient error or network blip.",reliability,hadoop-common
483,ha.zookeeper.parent-znode,"The ZooKeeper znode under which the ZK failover controller stores its information. Note that the nameservice ID is automatically appended to this znode, so it is not normally necessary to configure this, even in a federated environment.",reliability,hadoop-common
484,ha.zookeeper.acl,"A comma-separated list of ZooKeeper ACLs to apply to the znodes used by automatic failover. These ACLs are specified in the same format as used by the ZooKeeper CLI. If the ACL itself contains secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.",security,hadoop-common
485,ha.zookeeper.auth,"A comma-separated list of ZooKeeper authentications to add when connecting to ZooKeeper. These are specified in the same format as used by the ""addauth"" command in the ZK CLI. It is important that the authentications specified here are sufficient to access znodes with the ACL specified in ha.zookeeper.acl. If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.",security,hadoop-common
486,hadoop.ssl.keystores.factory.class,The keystores factory to use for retrieving certificates.,security,hadoop-common
487,hadoop.ssl.require.client.cert,Whether client certificates are required,security,hadoop-common
488,hadoop.ssl.hostname.verifier,"The hostname verifier to provide for HttpsURLConnections. Valid values are: DEFAULT, STRICT, STRICT_IE6, DEFAULT_AND_LOCALHOST and ALLOW_ALL",security,hadoop-common
489,hadoop.ssl.server.conf,"Resource file from which ssl server keystore information will be extracted. This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",security,hadoop-common
490,hadoop.ssl.client.conf,"Resource file from which ssl client keystore information will be extracted This file is looked up in the classpath, typically it should be in Hadoop conf/ directory.",security,hadoop-common
491,hadoop.ssl.enabled,Deprecated. Use dfs.http.policy and yarn.http.policy instead.,security,hadoop-common
492,hadoop.ssl.enabled.protocols,The supported SSL protocols.,security,hadoop-common
493,hadoop.jetty.logs.serve.aliases,Enable/Disable aliases serving from jetty,others,hadoop-common
494,fs.permissions.umask-mode,"The umask used when creating files and directories. Can be in octal or in symbolic. Examples are: ""022"" (octal for u=rwx,g=r-x,o=r-x in symbolic), or ""u=rwx,g=rwx,o="" (symbolic for 007 in octal).",security,hadoop-common
495,ha.health-monitor.connect-retry-interval.ms,How often to retry connecting to the service.,reliability,hadoop-common
496,ha.health-monitor.check-interval.ms,How often to check the service.,reliability,hadoop-common
497,ha.health-monitor.sleep-after-disconnect.ms,How long to sleep after an unexpected RPC error.,reliability,hadoop-common
498,ha.health-monitor.rpc-timeout.ms,Timeout for the actual monitorHealth() calls.,reliability,hadoop-common
499,ha.failover-controller.new-active.rpc-timeout.ms,Timeout that the FC waits for the new active to become active,reliability,hadoop-common
500,ha.failover-controller.graceful-fence.rpc-timeout.ms,Timeout that the FC waits for the old active to go to standby,reliability,hadoop-common
501,ha.failover-controller.graceful-fence.connection.retries,FC connection retries for graceful fencing,reliability,hadoop-common
502,ha.failover-controller.cli-check.rpc-timeout.ms,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",reliability,hadoop-common
503,ipc.client.fallback-to-simple-auth-allowed,"When a client is configured to attempt a secure connection, but attempts to connect to an insecure server, that server may instruct the client to switch to SASL SIMPLE (unsecure) authentication. This setting controls whether or not the client will accept this instruction from the server. When false (the default), the client will not allow the fallback to SIMPLE authentication, and will abort the connection.",security,hadoop-common
504,fs.client.resolve.remote.symlinks,"Whether to resolve symlinks when accessing a remote Hadoop filesystem. Setting this to false causes an exception to be thrown upon encountering a symlink. This setting does not apply to local filesystems, which automatically resolve local symlinks.",reliability,hadoop-common
505,nfs.exports.allowed.hosts,"By default, the export can be mounted by any client. The value string contains machine name and access privilege, separated by whitespace characters. The machine name format can be a single host, a Java regular expression, or an IPv4 address. The access privilege uses rw or ro to specify read/write or read-only access of the machines to exports. If the access privilege is not provided, the default is read-only. Entries are separated by "";"". For example: ""192.168.0.0/22 rw ; host.*\.example\.com ; host1.test.org ro;"". Only the NFS gateway needs to restart after this property is updated.",security,hadoop-common
506,hadoop.user.group.static.mapping.overrides,"Static mapping of user to groups. This will override the groups if available in the system for the specified user. In other words, groups look-up will not happen for these users, instead groups mapped in this configuration will be used. Mapping should be in this format. user1=group1,group2;user2=;user3=group2; Default, ""dr.who=;"" will consider ""dr.who"" as user without groups.",security,hadoop-common
507,rpc.metrics.quantile.enable,"Setting this property to true and rpc.metrics.percentiles.intervals to a comma-separated list of the granularity in seconds, the 50/75/90/95/99th percentile latency for rpc queue/processing time in milliseconds are added to rpc metrics.",reliability,hadoop-common
508,rpc.metrics.percentiles.intervals,A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for rpc queue/processing time. The metrics are outputted if rpc.metrics.quantile.enable is set to true.,reliability,hadoop-common
509,hadoop.security.crypto.codec.classes.EXAMPLECIPHERSUITE,"The prefix for a given crypto codec, contains a comma-separated list of implementation classes for a given crypto codec (eg EXAMPLECIPHERSUITE). The first implementation will be used if available, others are fallbacks.",security,hadoop-common
510,hadoop.security.crypto.codec.classes.aes.ctr.nopadding,"Comma-separated list of crypto codec implementations for AES/CTR/NoPadding. The first implementation will be used if available, others are fallbacks.",security,hadoop-common
511,hadoop.security.crypto.cipher.suite,Cipher suite for crypto codec.,security,hadoop-common
512,hadoop.security.crypto.jce.provider,The JCE provider name used in CryptoCodec.,security,hadoop-common
513,hadoop.security.crypto.jceks.key.serialfilter,"Enhanced KeyStore Mechanisms in JDK 8u171 introduced jceks.key.serialFilter. If jceks.key.serialFilter is configured, the JCEKS KeyStore uses it during the deserialization of the encrypted Key object stored inside a SecretKeyEntry. If jceks.key.serialFilter is not configured it will cause an error when recovering keystore file in KeyProviderFactory when recovering key from keystore file using JDK 8u171 or newer. The filter pattern uses the same format as jdk.serialFilter. The value of this property will be used as the following: 1. The value of jceks.key.serialFilter system property takes precedence over the value of this property. 2. In the absence of jceks.key.serialFilter system property the value of this property will be set as the value of jceks.key.serialFilter. 3. If the value of this property and jceks.key.serialFilter system property has not been set, org.apache.hadoop.crypto.key.KeyProvider sets a default value for jceks.key.serialFilter.",security,hadoop-common
514,hadoop.security.crypto.buffer.size,The buffer size used by CryptoInputStream and CryptoOutputStream.,performance,hadoop-common
515,hadoop.security.java.secure.random.algorithm,The java secure random algorithm.,security,hadoop-common
516,hadoop.security.secure.random.impl,Implementation of secure random.,security,hadoop-common
517,hadoop.security.random.device.file.path,OS security random device file path.,security,hadoop-common
518,hadoop.security.key.provider.path,"The KeyProvider to use when managing zone keys, and interacting with encryption keys when reading and writing to an encryption zone. For hdfs clients, the provider path will be same as namenode's provider path.",security,hadoop-common
519,fs.har.impl.disable.cache,Don't cache 'har' filesystem instances.,others,hadoop-common
520,hadoop.security.kms.client.authentication.retry-count,Number of time to retry connecting to KMS on authentication failure,reliability,hadoop-common
521,hadoop.security.kms.client.encrypted.key.cache.size,Size of the EncryptedKeyVersion cache Queue for each key,performance,hadoop-common
522,hadoop.security.kms.client.encrypted.key.cache.low-watermark,"If size of the EncryptedKeyVersion cache Queue falls below the low watermark, this cache queue will be scheduled for a refill",performance,hadoop-common
523,hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,Number of threads to use for refilling depleted EncryptedKeyVersion cache Queues,performance,hadoop-common
524,hadoop.security.kms.client.encrypted.key.cache.expiry,"Cache expiry time for a Key, after which the cache Queue for this key will be dropped. Default = 12hrs",reliability,hadoop-common
525,hadoop.security.kms.client.timeout,"Sets value for KMS client connection timeout, and the read timeout to KMS servers.",reliability,hadoop-common
526,hadoop.security.kms.client.failover.sleep.base.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least hadoop.security.client.failover.sleep.base.millis milliseconds. And so on.",reliability,hadoop-common
527,hadoop.security.kms.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis milliseconds.",reliability,hadoop-common
528,ipc.server.max.connections,"The maximum number of concurrent connections a server is allowed to accept. If this limit is exceeded, incoming connections will first fill the listen queue and then may go to an OS-specific listen overflow queue. The client may fail or timeout, but the server can avoid running out of file descriptors using this feature. 0 means no limit.",reliability,hadoop-common
529,hadoop.registry.rm.enabled,"Is the registry enabled in the YARN Resource Manager? If true, the YARN RM will, as needed. create the user and system paths, and purge service records when containers, application attempts and applications complete. If false, the paths must be created by other means, and no automatic cleanup of service records will take place.",others,hadoop-common
530,hadoop.registry.zk.root,The root zookeeper node for the registry,environment,hadoop-common
531,hadoop.registry.zk.session.timeout.ms,Zookeeper session timeout in milliseconds,reliability,hadoop-common
532,hadoop.registry.zk.connection.timeout.ms,Zookeeper connection timeout in milliseconds,reliability,hadoop-common
533,hadoop.registry.zk.retry.times,Zookeeper connection retry count before failing,reliability,hadoop-common
534,hadoop.registry.zk.retry.ceiling.ms,"Zookeeper retry limit in milliseconds, during exponential backoff. This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period",reliability,hadoop-common
535,hadoop.registry.zk.quorum,List of hostname:port pairs defining the zookeeper quorum binding for the registry,environment,hadoop-common
536,hadoop.registry.secure,"Key to set if the registry is secure. Turning it on changes the permissions policy from ""open access"" to restrictions on kerberos with the option of a user adding one or more auth key pairs down their own tree.",security,hadoop-common
537,hadoop.registry.system.acls,"A comma separated list of Zookeeper ACL identifiers with system access to the registry in a secure cluster. These are given full access to all entries. If there is an ""@"" at the end of a SASL entry it instructs the registry client to append the default kerberos domain.",security,hadoop-common
538,hadoop.registry.kerberos.realm,"The kerberos realm: used to set the realm of system principals which do not declare their realm, and any other accounts that need the value. If empty, the default realm of the running process is used. If neither are known and the realm is needed, then the registry service/client will fail.",security,hadoop-common
539,hadoop.registry.jaas.context,Key to define the JAAS context. Used in secure mode,security,hadoop-common
540,hadoop.shell.missing.defaultFs.warning,Enable hdfs shell commands to display warnings if (fs.defaultFS) property is not set.,debuggability,hadoop-common
541,hadoop.shell.safely.delete.limit.num.files,"Used by -safely option of hadoop fs shell -rm command to avoid accidental deletion of large directories. When enabled, the -rm command requires confirmation if the number of files to be deleted is greater than this limit. The default limit is 100 files. The warning is disabled if the limit is 0 or the -safely is not specified in -rm command.",security,hadoop-common
542,fs.client.htrace.sampler.classes,The class names of the HTrace Samplers to use for Hadoop filesystem clients.,debuggability,hadoop-common
543,hadoop.htrace.span.receiver.classes,The class names of the Span Receivers to use for Hadoop.,others,hadoop-common
544,hadoop.http.logs.enabled,"Enable the ""/logs"" endpoint on all Hadoop daemons, which serves local logs, but may be considered a security risk due to it listing the contents of a directory.",debuggability,hadoop-common
545,fs.client.resolve.topology.enabled,"Whether the client machine will use the class specified by property net.topology.node.switch.mapping.impl to compute the network distance between itself and remote machines of the FileSystem. Additional properties might need to be configured depending on the class specified in net.topology.node.switch.mapping.impl. For example, if org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file needs to be specified in net.topology.script.file.name.",others,hadoop-common
546,adl.feature.ownerandgroup.enableupn,"When true : User and Group in FileStatus/AclStatus response is represented as user friendly name as per Azure AD profile. When false (default) : User and Group in FileStatus/AclStatus response is represented by the unique identifier from Azure AD profile (Object ID as GUID). For optimal performance, false is recommended.",performance,hadoop-common
547,fs.adl.oauth2.access.token.provider.type,"Defines Azure Active Directory OAuth2 access token provider type. Supported types are ClientCredential, RefreshToken, MSI, DeviceCode, and Custom. The ClientCredential type requires property fs.adl.oauth2.client.id, fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url. The RefreshToken type requires property fs.adl.oauth2.client.id and fs.adl.oauth2.refresh.token. The MSI type reads optional property fs.adl.oauth2.msi.port, if specified. The DeviceCode type requires property fs.adl.oauth2.devicecode.clientapp.id. The Custom type requires property fs.adl.oauth2.access.token.provider.",security,hadoop-common
548,fs.adl.oauth2.client.id,The OAuth2 client id.,security,hadoop-common
549,fs.adl.oauth2.credential,The OAuth2 access key.,security,hadoop-common
550,fs.adl.oauth2.refresh.url,The OAuth2 token endpoint.,security,hadoop-common
551,fs.adl.oauth2.refresh.token,The OAuth2 refresh token.,security,hadoop-common
552,fs.adl.oauth2.access.token.provider,The class name of the OAuth2 access token provider.,security,hadoop-common
553,fs.adl.oauth2.msi.port,"The localhost port for the MSI token service. This is the port specified when creating the Azure VM. The default, if this setting is not specified, is 50342. Used by MSI token provider.",environment,hadoop-common
554,fs.adl.oauth2.devicecode.clientapp.id,The app id of the AAD native app in whose context the auth request should be made. Used by DeviceCode token provider.,environment,hadoop-common
555,hadoop.caller.context.enabled,"When the feature is enabled, additional fields are written into name-node audit log records for auditing coarse granularity operations.",debuggability,hadoop-common
556,hadoop.caller.context.max.size,"The maximum bytes a caller context string can have. If the passed caller context is longer than this maximum bytes, client will truncate it before sending to server. Note that the server may have a different maximum size, and will truncate the caller context to the maximum size it allows.",performance,hadoop-common
557,hadoop.caller.context.signature.max.size,"The caller's signature (optional) is for offline validation. If the signature exceeds the maximum allowed bytes in server, the caller context will be abandoned, in which case the caller context will not be recorded in audit logs.",security,hadoop-common
558,seq.io.sort.mb,"The total amount of buffer memory to use while sorting files, while using SequenceFile.Sorter, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",performance,hadoop-common
559,seq.io.sort.factor,The number of streams to merge at once while sorting files using SequenceFile.Sorter. This determines the number of open file handles.,performance,hadoop-common
560,hadoop.zk.address,Host:Port of the ZooKeeper server to be used.,environment,hadoop-common
561,hadoop.zk.num-retries,Number of tries to connect to ZooKeeper.,reliability,hadoop-common
562,hadoop.zk.retry-interval-ms,Retry interval in milliseconds when connecting to ZooKeeper.,reliability,hadoop-common
563,hadoop.zk.timeout-ms,"ZooKeeper session timeout in milliseconds. Session expiration is managed by the ZooKeeper cluster itself, not by the client. This value is used by the cluster to determine when the client's session expires. Expirations happens when the cluster does not hear from the client within the specified session timeout period (i.e. no heartbeat).",reliability,hadoop-common
564,hadoop.zk.acl,ACL's to be used for ZooKeeper znodes.,security,hadoop-common
565,hadoop.zk.auth,"Specify the auths to be used for the ACL's specified in hadoop.zk.acl. This takes a comma-separated list of authentication mechanisms, each of the form 'scheme:auth' (the same syntax used for the 'addAuth' command in the ZK CLI).",security,hadoop-common
566,attachment_stream_buffer_size,Higher values may result in better read performance due to fewer read operations and/or more OS page cache hits.,performance,couchdb
567,database_dir,Specifies location of CouchDB database files,environment,couchdb
568,default_security,Default security object for databases if not explicitly set. ,security,couchdb
569,delayed_commits,"Setting this config value to true may improve performance, at cost of some durability.",performance,couchdb
570,file_compression,"Method used to compress everything that is appended to database and view index files, except for attachments",performance,couchdb
571,max_dbs_open,This option places an upper bound on the number of databases that can be open at once.,reliability,couchdb
572,os_process_timeout,"If an external process, such as a query server or external process, runs for this amount of milliseconds without returning any results, it will be terminated.",reliability,couchdb
573,uri_file,This file contains the full URI that can be used to access this instance of CouchDB.,environment,couchdb
574,users_db_suffix,Specifies the suffix (last component of a name) of the system database for storing CouchDB users.,environment,couchdb
575,util_driver_dir,"Specifies location of binary drivers (icu, ejson, etc.).",environment,couchdb
576,uuid,Unique identifier for this CouchDB server instance.,environment,couchdb
577,view_index_dir,Specifies location of CouchDB view index files. ,environment,couchdb
578,max_document_size,Limit maximum document body size. ,reliability,couchdb
579,delete_dbs,"If set to true and a user is deleted, the respective database gets deleted as well.",reliability,couchdb
580,port,Defines the port number to listen,environment,couchdb
581,authentication_handlers,List of authentication handlers used by CouchDB.,security,couchdb
582,allow_jsonp,The true value of this option enables JSONP support,others,couchdb
583,bind_address,Defines the IP address by which the node-local port is available.,environment,couchdb
584,changes_timeout,Specifies default timeout value for Changes Feed in milliseconds,reliability,couchdb
585,config_whitelist,Sets the configuration modification whitelist.,security,couchdb
586,cacert_file,The path to a file containing PEM encoded CA certificates.,security,couchdb
587,cert_file,Path to a file containing the user's certificate,security,couchdb
588,key_file,Path to file containing user's private PEM encoded key,security,couchdb
589,ssl_certificate_max_depth,Maximum peer certificate depth (must be set even if certificate validation is off),security,couchdb
590,verify_ssl_certificates,Set to true to validate peer certificates,security,couchdb
591,tls_versions,Set to a list of permitted SSL/TLS protocol versions,security,couchdb
592,require_valid_user,"When this option is set to true, no requests are allowed from anonymous users. Everyone must be authenticated.",security,couchdb
593,authentication_redirect,Specifies the location for redirection on successful authentication if a text/html response is accepted by the client (via an Accept header).,security,couchdb
594,iterations,"The number of iterations for password hashing by the PBKDF2 algorithm. A higher number provides better hash durability, but comes at a cost in performance for each request that requires authentication.",security,couchdb
595,require_valid_user,"When this option is set to true, no requests are allowed from anonymous users. Everyone must be authenticated.",security,couchdb
596,secret,The secret token is used for Proxy Authentication and for Cookie Authentication.,security,couchdb
597,timeout,Number of seconds since the last request before sessions will be expired.,reliability,couchdb
598,users_db_public,"Allow all users to view user documents. By default, only admins may browse all users documents, while users may browse only their own document.",security,couchdb
599,doc_buffer_size,Specifies the copy buffer's maximum size in bytes,performance,couchdb
600,checkpoint_after,Triggers a checkpoint after the specified amount of bytes were successfully copied to the compacted database,reliability,couchdb
601,check_interval,"The delay, in seconds, between each check for which database and view indexes need to be compacted",reliability,couchdb
602,min_file_size,"If a database or view index file is smaller than this value (in bytes), compaction will not happen.",performance,couchdb
603,keyvalue_buffer_size,Specifies maximum copy buffer size in bytes used during compaction,performance,couchdb
604,max_jobs,"Number of actively running replications. Making this value too high could cause performance issues, while making it too low could mean replications jobs might not have enough time to make progress before getting unscheduled again. ",performance,couchdb
605,max_churn,"Maximum number of replication jobs to start and stop during rescheduling. This parameter, along with interval, defines the rate of job replacement. ",performance,couchdb
606,update_docs,When set to true replicator will update replication document with error and triggered states.,others,couchdb
607,worker_batch_size,With lower batch sizes checkpoints are done more frequently. Lower batch sizes also reduce the total amount of used RAM memory,performance,couchdb
608,worker_processes,More worker processes can give higher network throughput but can also imply more disk and network IO,performance,couchdb
609,connection_timeout,HTTP connection timeout per replication.,reliability,couchdb
610,retries_per_request,"If a request fails, the replicator will retry it up to N times.",reliability,couchdb
611,checkpoint_interval,Defines replication checkpoint interval in milliseconds. ,reliability,couchdb
612,use_checkpoints,"If use_checkpoints is set to true, CouchDB will make checkpoints during replication and at the completion of replication. ",reliability,couchdb
613,commit_freq,Specifies the delay in seconds before view index changes are committed to disk. ,reliability,couchdb
614,os_process_limit,Hard limit on the number of OS processes usable by Query Servers.,reliability,couchdb
615,os_process_soft_limit,Soft limit on the number of OS processes usable by Query Servers.,reliability,couchdb
616,reduce_limit,Controls Reduce overflow error that raises when output of reduce functions is too big,reliability,couchdb
617,auth_cache,This daemon provides authentication caching to avoid repeated opening and closing of the _users database for each request requiring authentication,security,couchdb
618,index_server,This manages the process handling for keeping track of the index state as well as managing the updater and compactor handling,debuggability,couchdb
619,compression_level,"Defines zlib compression level for the attachments from 1 (lowest, fastest) to 9 (highest, slowest).",others,couchdb
620,algorithm,CouchDB provides various algorithms to generate the UUID values that are used for document _id's,others,couchdb
621,max_count,"No more than this number of UUIDs will be sent in a single request. If more UUIDs are requested, a HTTP error response will be thrown.",reliability,couchdb
622,cluster.name,The name of the CrateDB cluster the node should join to.,others,cratedb
623,node.name,The name of the node. If no name is configured a random one will be generated.,others,cratedb
624,node.max_local_storage_nodes,Defines how many nodes are allowed to be started on the same machine using the same configured data path ,performance,cratedb
625,node.data,Whether or not this node will store data.,others,cratedb
626,node.sql.read_only,"If set to true, the node will only allow SQL statements which are resulting in read operations.",security,cratedb
627,http.port,This defines the TCP port range to which the CrateDB HTTP service will be bound to.,environment,cratedb
628,http.publish_port,The port HTTP clients should use to communicate with the node. ,environment,cratedb
629,transport.tcp.port,This defines the TCP port range to which the CrateDB transport service will be bound to.,environment,cratedb
630,transport.publish_port,The port that the node publishes to the cluster for its own discovery.,environment,cratedb
631,psql.port,This defines the TCP port range to which the CrateDB Postgres service will be bound to.,environment,cratedb
632,path.conf,Filesystem path to the directory containing the configuration files crate.yml and log4j2.properties.,environment,cratedb
633,path.data,Filesystem path to the directory where this CrateDB node stores its data (table data and cluster metadata).,environment,cratedb
634,path.logs,Filesystem path to a directory where log files should be stored.,debuggability,cratedb
635,path.repo,A list of filesystem or UNC paths where repositories of type fs may be stored.,environment,cratedb
636,bootstrap.memory_lock,"CrateDB performs poorly when the JVM starts swapping: you should ensure that it never swaps. If set to true, CrateDB will use the mlockall system call on startup to ensure that the memory pages of the CrateDB process are locked into RAM.",performance,cratedb
637,auth.trust.http_default_user,The default user that should be used for authentication when clients connect to CrateDB via HTTP protocol and they do not specify a user via the Authorization request header.,security,cratedb
638,auth.host_based.enabled,Setting to enable or disable Host Based Authentication (HBA). It is disabled by default.,security,cratedb
639,ssl.http.enabled,Set this to true to enable secure communication between the CrateDB node and the client through SSL via the HTTPS protocol.,security,cratedb
640,ssl.psql.enabled,Set this to true to enable secure communication between the CrateDB node and the client through SSL via the PostgreSQL wire protocol.,security,cratedb
641,ssl.ingestion.mqtt.enabled,Set this to true to enable secure communication between the CrateDB node and the client through SSL via the MQTT protocol.,security,cratedb
642,ssl.keystore_filepath,The full path to the node keystore file.,security,cratedb
643,ssl.keystore_password,The password used to decrypt the keystore file defined with ssl.keystore_filepath.,security,cratedb
644,ssl.truststore_filepath,"The full path to the node truststore file. If not defined, then only a keystore will be used.",security,cratedb
645,ssl.truststore_password,The password used to decrypt the truststore file defined with ssl.truststore_filepath.,security,cratedb
646,http.cors.enabled,Enable or disable cross-origin resource sharing.,others,cratedb
647,http.cors.max-age,Max cache age of a preflight request in seconds.,reliability,cratedb
648,blobs.path,Path to a filesystem directory where to store blob data allocated for this node.,environment,cratedb
649,repositories.url.allowed_urls,This setting is a security measure to prevent access to arbitrary resources.,security,cratedb
650,indices.query.bool.max_clause_count,"This setting defines the maximum number of elements an array can have so that the != ANY(), LIKE ANY() and the NOT LIKE ANY() operators can be applied on it.",reliability,cratedb
651,ingestion.mqtt.enabled,Enables the MQTT ingestion source on this node.,others,cratedb
652,ingestion.mqtt.port,TCP port on which the endpoint is exposed.,environment,cratedb
653,ingestion.mqtt.timeout,The default keep-alive timeout for establised connections.,reliability,cratedb
654,stats.enabled,A boolean indicating whether or not to collect statistical information about the cluster.,debuggability,cratedb
655,stats.jobs_log_size,"The maximum number of job records kept to be kept in the sys.jobs_log table on each node. These records are used for performance analytics. A larger job log produces more comprehensive stats, but uses more RAM.",debuggability,cratedb
656,stats.jobs_log_expiration,The job record expiry time in seconds.,debuggability,cratedb
657,stats.jobs_log_filter,An expression to determine if a job should be recorded into sys.jobs_log. The expression must evaluate to a boolean.,debuggability,cratedb
658,stats.jobs_log_persistent_filter,An expression to determine if a job should also be recorded to the regular CrateDB log. ,debuggability,cratedb
659,stats.operations_log_size,"The maximum number of operations records to be kept in the sys.operations_log table on each node. Operations records are used for performance analytics. A larger operations log produces more comprehensive stats, but uses more RAM.",debuggability,cratedb
660,stats.operations_log_expiration,Entries of sys.operations_log are cleared by a periodically job when they are older than the specified expire time.,debuggability,cratedb
661,stats.service.interval,Defines the refresh interval to refresh tables statistics used to produce optimal query execution plans.,performance,cratedb
662,cluster.graceful_stop.timeout,Defines the maximum waiting time in milliseconds for the reallocation process to finish. The force setting will define the behaviour when the shutdown process runs into this timeout.,reliability,cratedb
663,cluster.graceful_stop.force,Defines whether graceful stop should force stopping of the node if it runs into the timeout which is specified with the cluster.graceful_stop.timeout setting.,reliability,cratedb
664,bulk.request_timeout,Defines the timeout of internal shard-based requests involved in the execution of SQL DML Statements over a huge amount of rows.,reliability,cratedb
665,discovery.zen.ping_interval,How often to ping other nodes. Nodes must remain responsive to pings or they will be marked as failed and removed from the cluster.,reliability,cratedb
666,discovery.zen.ping_timeout,The time to wait for ping responses from other nodes when discovering. Set this option to a higher value on a slow or congested network to minimize discovery failures.,reliability,cratedb
667,discovery.zen.ping_retries,How many ping failures (network timeouts) indicate that a node has failed.,reliability,cratedb
668,discovery.zen.publish_timeout,Time a node is waiting for responses from other nodes to a published cluster state.,reliability,cratedb
669,discovery.ec2.groups,A list of security groups; either by ID or name. Only instances with the given group will be used for unicast host discovery.,security,cratedb
670,discovery.ec2.any_group,Defines whether all (false) or just any (true) security group must be present for the instance to be used for discovery.,security,cratedb
671,discovery.ec2.host_type,Defines via which host type to communicate with other instances.,security,cratedb
672,discovery.ec2.availability_zones,A list of availability zones. Only instances within the given availability zone will be used for unicast host discovery.,environment,cratedb
673,cloud.azure.management.resourcegroup.name,The name of the resource group the CrateDB cluster is running on.,others,cratedb
674,cluster.routing.allocation.allow_rebalance,Allow to control when rebalancing will happen based on the total state of all the indices shards in the cluster. ,performance,cratedb
675,cluster.routing.allocation.cluster_concurrent_rebalance,Define how many concurrent rebalancing tasks are allowed cluster wide.,performance,cratedb
676,cluster.routing.allocation.node_initial_primaries_recoveries,"Define the number of initial recoveries of primaries that are allowed per node. Since most times local gateway is used, those should be fast and we can handle more of those per node without creating load.",reliability,cratedb
677,cluster.routing.allocation.node_concurrent_recoveries,How many concurrent recoveries are allowed to happen on a node.,performance,cratedb
678,cluster.routing.allocation.balance.shard,Defines the weight factor for shards allocated on a node (float). Raising this raises the tendency to equalize the number of shards across all nodes in the cluster.,performance,cratedb
679,cluster.routing.allocation.balance.index,Defines a factor to the number of shards per index allocated on a specific node (float). Increasing this value raises the tendency to equalize the number of shards per index across all nodes in the cluster.,performance,cratedb
680,cluster.routing.allocation.balance.threshold,Minimal optimization value of operations that should be performed (non negative float). Increasing this value will cause the cluster to be less aggressive about optimising the shard balance.,performance,cratedb
681,cluster.routing.allocation.disk.threshold_enabled,Prevent shard allocation on nodes depending of the disk usage.,performance,cratedb
682,indices.recovery.max_bytes_per_sec,Specifies the maximum number of bytes that can be transferred during shard recovery per seconds.,reliability,cratedb
683,indices.recovery.retry_delay_state_sync,Defines the time to wait after an issue caused by cluster state syncing before retrying to recover.,reliability,cratedb
684,indices.recovery.retry_delay_network,Defines the time to wait after an issue caused by the network before retrying to recover.,reliability,cratedb
685,indices.recovery.internal_action_timeout,Defines the timeout for internal requests made as part of the recovery.,reliability,cratedb
686,indices.recovery.internal_action_long_timeout,Defines the timeout for internal requests made as part of the recovery that are expected to take a long time. ,reliability,cratedb
687,indices.recovery.recovery_activity_timeout,Recoveries that don't show any activity for more then this interval will fail. ,reliability,cratedb
688,cluster.info.update.interval,Defines how often the cluster collect metadata information (e.g. disk usages etc.) if no concrete event is triggered.,debuggability,cratedb
689,gateway.expected_nodes,The setting gateway.expected_nodes defines the number of nodes that should be waited for until the cluster state is recovered immediately. ,reliability,cratedb
690,gateway.recover_after_time,The gateway.recover_after_time setting defines the time to wait before starting starting the recovery once the number of nodes defined in gateway.recover_after_nodes are started. ,reliability,cratedb
691,gateway.recover_after_nodes,The gateway.recover_after_nodes setting defines the number of nodes that need to be started before the cluster state recovery will start.,reliability,cratedb
692,hadoop.hdfs.configuration.version,version of this configuration file,environment,hdfs
693,dfs.namenode.rpc-address,"RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port.",environment,hdfs
694,dfs.namenode.rpc-bind-host,"The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.rpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
695,dfs.namenode.servicerpc-address,"RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be connecting to this address if it is configured. In the case of HA/Federation where multiple namenodes exist, the name service id is added to the name e.g. dfs.namenode.servicerpc-address.ns1 dfs.namenode.rpc-address.EXAMPLENAMESERVICE The value of this property will take the form of nn-host1:rpc-port. If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.",environment,hdfs
696,dfs.namenode.servicerpc-bind-host,"The actual address the service RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.servicerpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
697,dfs.namenode.lifeline.rpc-address,"NameNode RPC lifeline address. This is an optional separate RPC address that can be used to isolate health checks and liveness to protect against resource exhaustion in the main RPC handler pool. In the case of HA/Federation where multiple NameNodes exist, the name service ID is added to the name e.g. dfs.namenode.lifeline.rpc-address.ns1. The value of this property will take the form of nn-host1:rpc-port. If this property is not defined, then the NameNode will not start a lifeline RPC server. By default, the property is not defined.",environment,hdfs
698,dfs.namenode.lifeline.rpc-bind-host,"The actual address the lifeline RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.lifeline.rpc-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
699,dfs.namenode.secondary.http-address,The secondary namenode http server address and port.,environment,hdfs
700,dfs.namenode.secondary.https-address,The secondary namenode HTTPS server address and port.,environment,hdfs
701,dfs.datanode.address,The datanode server address and port for data transfer.,environment,hdfs
702,dfs.datanode.http.address,The datanode http server address and port.,environment,hdfs
703,dfs.datanode.ipc.address,The datanode ipc server address and port.,environment,hdfs
704,dfs.datanode.http.internal-proxy.port,The datanode's internal web proxy port. By default it selects a random port available in runtime.,environment,hdfs
705,dfs.datanode.handler.count,The number of server threads for the datanode.,performance,hdfs
706,dfs.namenode.http-address,The address and the base port where the dfs namenode web ui will listen on.,environment,hdfs
707,dfs.namenode.http-bind-host,"The actual adress the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.http-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTP server listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
708,dfs.namenode.heartbeat.recheck-interval,"This time decides the interval to check for expired datanodes. With this value and dfs.heartbeat.interval, the interval of deciding the datanode is stale or not is also calculated. The unit of this configuration is millisecond.",reliability,hdfs
709,dfs.http.policy,Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons: The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https,security,hdfs
710,dfs.client.https.need-auth,Whether SSL client certificate authentication is required,security,hdfs
711,dfs.client.cached.conn.retry,"The number of times the HDFS client will pull a socket from the cache. Once this number is exceeded, the client will try to create a new socket.",reliability,hdfs
712,dfs.https.server.keystore.resource,Resource file from which ssl server keystore information will be extracted,environment,hdfs
713,dfs.client.https.keystore.resource,Resource file from which ssl client keystore information will be extracted,environment,hdfs
714,dfs.datanode.https.address,The datanode secure http server address and port.,environment,hdfs
715,dfs.namenode.https-address,The namenode secure http server address and port.,environment,hdfs
716,dfs.namenode.https-bind-host,"The actual adress the HTTPS server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.namenode.https-address. It can also be specified per name node or name service for HA/Federation. This is useful for making the name node HTTPS server listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
717,dfs.datanode.dns.interface,The name of the Network Interface from which a data node should report its IP address. e.g. eth2. This setting may be required for some multi-homed nodes where the DataNodes are assigned multiple hostnames and it is desirable for the DataNodes to use a non-default hostname. Prefer using hadoop.security.dns.interface over dfs.datanode.dns.interface.,environment,hdfs
718,dfs.datanode.dns.nameserver,The host name or IP address of the name server (DNS) which a DataNode should use to determine its own host name. Prefer using hadoop.security.dns.nameserver over dfs.datanode.dns.nameserver.,environment,hdfs
719,dfs.namenode.backup.address,The backup node server address and port. If the port is 0 then the server will start on a free port.,environment,hdfs
720,dfs.namenode.backup.http-address,The backup node http server address and port. If the port is 0 then the server will start on a free port.,environment,hdfs
721,dfs.namenode.replication.considerLoad,Decide if chooseTarget considers the target's load or not,others,hdfs
722,dfs.namenode.replication.considerLoad.factor,"The factor by which a node's load can exceed the average before being rejected for writes, only if considerLoad is true.",reliability,hdfs
723,dfs.default.chunk.view.size,The number of bytes to view for a file on the browser.,others,hdfs
724,dfs.datanode.du.reserved.calculator,"Determines the class of ReservedSpaceCalculator to be used for calculating disk space reservedfor non-HDFS data. The default calculator is ReservedSpaceCalculatorAbsolute which will use dfs.datanode.du.reserved for a static reserved number of bytes. ReservedSpaceCalculatorPercentage will use dfs.datanode.du.reserved.pct to calculate the reserved number of bytes based on the size of the storage. ReservedSpaceCalculatorConservative and ReservedSpaceCalculatorAggressive will use their combination, Conservative will use maximum, Aggressive minimum. For more details see ReservedSpaceCalculator.",performance,hdfs
725,dfs.datanode.du.reserved,"Reserved space in bytes per volume. Always leave this much space free for non dfs use. Specific storage type based reservation is also supported. The property can be followed with corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage. For example, reserved space for RAM_DISK storage can be configured using property 'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured then dfs.datanode.du.reserved will be used.",performance,hdfs
726,dfs.datanode.du.reserved.pct,"Reserved space in percentage. Read dfs.datanode.du.reserved.calculator to see when this takes effect. The actual number of bytes reserved will be calculated by using the total capacity of the data directory in question. Specific storage type based reservation is also supported. The property can be followed with corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage. For example, reserved percentage space for RAM_DISK storage can be configured using property 'dfs.datanode.du.reserved.pct.ram_disk'. If specific storage type reservation is not configured then dfs.datanode.du.reserved.pct will be used.",others,hdfs
727,dfs.namenode.name.dir,"Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.",environment,hdfs
728,dfs.namenode.name.dir.restore,"Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir. When enabled, a recovery of any failed directory is attempted during checkpoint.",reliability,hdfs
729,dfs.namenode.fs-limits.max-component-length,Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check.,others,hdfs
730,dfs.namenode.fs-limits.max-directory-items,Defines the maximum number of items that a directory may contain. Cannot set the property to a value less than 1 or more than 6400000.,others,hdfs
731,dfs.namenode.fs-limits.min-block-size,"Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance.",performance,hdfs
732,dfs.namenode.fs-limits.max-blocks-per-file,"Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.",performance,hdfs
733,dfs.namenode.edits.dir,"Determines where on the local filesystem the DFS name node should store the transaction (edits) file. If this is a comma-delimited list of directories then the transaction file is replicated in all of the directories, for redundancy. Default value is same as dfs.namenode.name.dir",environment,hdfs
734,dfs.namenode.edits.dir.required,"This should be a subset of dfs.namenode.edits.dir, to ensure that the transaction (edits) file in these places is always up-to-date.",environment,hdfs
735,dfs.namenode.shared.edits.dir,A directory on shared storage between the multiple namenodes in an HA cluster. This directory will be written by the active and read by the standby in order to keep the namespaces synchronized. This directory does not need to be listed in dfs.namenode.edits.dir above. It should be left empty in a non-HA cluster.,environment,hdfs
736,dfs.permissions.enabled,"If ""true"", enable permission checking in HDFS. If ""false"", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories.",security,hdfs
737,dfs.permissions.superusergroup,The name of the group of super-users. The value should be a single group name.,others,hdfs
738,dfs.cluster.administrators,"ACL for the admins, this configuration is used to control who can access the default servlets in the namenode, etc. The value should be a comma separated list of users and groups. The user list comes first and is separated by a space followed by the group list, e.g. ""user1,user2 group1,group2"". Both users and groups are optional, so ""user1"", "" group1"", """", ""user1 group1"", ""user1,user2 group1,group2"" are all valid (note the leading space in "" group1""). '*' grants access to all users and groups, e.g. '*', '* ' and ' *' are all valid.",others,hdfs
739,dfs.namenode.acls.enabled,"Set to true to enable support for HDFS ACLs (Access Control Lists). By default, ACLs are disabled. When ACLs are disabled, the NameNode rejects all RPCs related to setting or getting ACLs.",security,hdfs
740,dfs.namenode.lazypersist.file.scrub.interval.sec,The NameNode periodically scans the namespace for LazyPersist files with missing blocks and unlinks them from the namespace. This configuration key controls the interval between successive scans. Set it to a negative value to disable this behavior.,reliability,hdfs
741,dfs.block.access.token.enable,"If ""true"", access tokens are used as capabilities for accessing datanodes. If ""false"", no access tokens are checked on accessing datanodes.",security,hdfs
742,dfs.block.access.key.update.interval,Interval in minutes at which namenode updates its access keys.,security,hdfs
743,dfs.block.access.token.lifetime,The lifetime of access tokens in minutes.,security,hdfs
744,dfs.datanode.data.dir,"Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows.",environment,hdfs
745,dfs.datanode.data.dir.perm,Permissions for the directories on on the local filesystem where the DFS data node store its blocks. The permissions can either be octal or symbolic.,security,hdfs
746,dfs.replication,Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.,reliability,hdfs
747,dfs.replication.max,Maximal block replication.,reliability,hdfs
748,dfs.namenode.replication.min,Minimal block replication.,reliability,hdfs
749,dfs.namenode.maintenance.replication.min,Minimal live block replication in existence of maintenance mode.,reliability,hdfs
750,dfs.namenode.safemode.replication.min,a separate minimum replication factor for calculating safe block count. This is an expert level setting. Setting this lower than the dfs.namenode.replication.min is not recommend and/or dangerous for production setups. When it's not set it takes value from dfs.namenode.replication.min,reliability,hdfs
751,dfs.namenode.max-corrupt-file-blocks-returned,"The maximum number of corrupt file blocks listed by NameNode Web UI, JMX and other client request.",reliability,hdfs
752,dfs.blocksize,"The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB).",others,hdfs
753,dfs.client.block.write.retries,"The number of retries for writing blocks to the data nodes, before we signal failure to the application.",reliability,hdfs
754,dfs.client.block.write.replace-datanode-on-failure.enable,"If there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy",reliability,hdfs
755,dfs.client.block.write.replace-datanode-on-failure.policy,This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. ALWAYS: always add a new datanode when an existing datanode is removed. NEVER: never add a new datanode. DEFAULT: Let r be the replication number. Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.,reliability,hdfs
756,dfs.client.block.write.replace-datanode-on-failure.best-effort,"This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true. Best effort means that the client will try to replace a failed datanode in write pipeline (provided that the policy is satisfied), however, it continues the write operation in case that the datanode replacement also fails. Suppose the datanode replacement fails. false: An exception should be thrown so that the write will fail. true : The write should be resumed with the remaining datandoes. Note that setting this property to true allows writing to a pipeline with a smaller number of datanodes. As a result, it increases the probability of data loss.",reliability,hdfs
757,dfs.client.block.write.replace-datanode-on-failure.min-replication,"The minimum number of replications that are needed to not to fail the write pipeline if new datanodes can not be found to replace failed datanodes (could be due to network failure) in the write pipeline. If the number of the remaining datanodes in the write pipeline is greater than or equal to this property value, continue writing to the remaining nodes. Otherwise throw exception. If this is set to 0, an exception will be thrown, when a replacement can not be found. See also dfs.client.block.write.replace-datanode-on-failure.policy",reliability,hdfs
758,dfs.blockreport.intervalMsec,Determines block reporting interval in milliseconds.,reliability,hdfs
759,dfs.blockreport.initialDelay,Delay for first block report in seconds.,reliability,hdfs
760,dfs.blockreport.split.threshold,If the number of blocks on the DataNode is below this threshold then it will send block reports for all Storage Directories in a single message. If the number of blocks exceeds this threshold then the DataNode will send block reports for each Storage Directory in separate messages. Set to zero to always split.,debuggability,hdfs
761,dfs.namenode.max.full.block.report.leases,The maximum number of leases for full block reports that the NameNode will issue at any given time. This prevents the NameNode from being flooded with full block reports that use up all the RPC handler threads. This number should never be more than the number of RPC handler threads or less than 1.,reliability,hdfs
762,dfs.namenode.full.block.report.lease.length.ms,The number of milliseconds that the NameNode will wait before invalidating a full block report lease. This prevents a crashed DataNode from permanently using up a full block report lease.,reliability,hdfs
763,dfs.datanode.directoryscan.interval,Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.,reliability,hdfs
764,dfs.datanode.directoryscan.threads,How many threads should the threadpool used to compile reports for volumes in parallel have.,performance,hdfs
765,dfs.datanode.directoryscan.throttle.limit.ms.per.sec,"The report compilation threads are limited to only running for a given number of milliseconds per second, as configured by the property. The limit is taken per thread, not in aggregate, e.g. setting a limit of 100ms for 4 compiler threads will result in each thread being limited to 100ms, not 25ms. Note that the throttle does not interrupt the report compiler threads, so the actual running time of the threads per second will typically be somewhat higher than the throttle limit, usually by no more than 20%. Setting this limit to 1000 disables compiler thread throttling. Only values between 1 and 1000 are valid. Setting an invalid value will result in the throttle being disabled and an error message being logged. 1000 is the default setting.",performance,hdfs
766,dfs.heartbeat.interval,Determines datanode heartbeat interval in seconds.,reliability,hdfs
767,dfs.datanode.lifeline.interval.seconds,"Sets the interval in seconds between sending DataNode Lifeline Protocol messages from the DataNode to the NameNode. The value must be greater than the value of dfs.heartbeat.interval. If this property is not defined, then the default behavior is to calculate the interval as 3x the value of dfs.heartbeat.interval. Note that normal heartbeat processing may cause the DataNode to postpone sending lifeline messages if they are not required. Under normal operations with speedy heartbeat processing, it is possible that no lifeline messages will need to be sent at all. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",reliability,hdfs
768,dfs.namenode.handler.count,The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.,performance,hdfs
769,dfs.namenode.service.handler.count,The number of Namenode RPC server threads that listen to requests from DataNodes and from all other non-client nodes. dfs.namenode.service.handler.count will be valid only if dfs.namenode.servicerpc-address is configured.,performance,hdfs
770,dfs.namenode.lifeline.handler.ratio,"A ratio applied to the value of dfs.namenode.handler.count, which then provides the number of RPC server threads the NameNode runs for handling the lifeline RPC server. For example, if dfs.namenode.handler.count is 100, and dfs.namenode.lifeline.handler.factor is 0.10, then the NameNode starts 100 * 0.10 = 10 threads for handling the lifeline RPC server. It is common to tune the value of dfs.namenode.handler.count as a function of the number of DataNodes in a cluster. Using this property allows for the lifeline RPC server handler threads to be tuned automatically without needing to touch a separate property. Lifeline message processing is lightweight, so it is expected to require many fewer threads than the main NameNode RPC server. This property is not used if dfs.namenode.lifeline.handler.count is defined, which sets an absolute thread count. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",performance,hdfs
771,dfs.namenode.lifeline.handler.count,"Sets an absolute number of RPC server threads the NameNode runs for handling the DataNode Lifeline Protocol and HA health check requests from ZKFC. If this property is defined, then it overrides the behavior of dfs.namenode.lifeline.handler.ratio. By default, it is not defined. This property has no effect if dfs.namenode.lifeline.rpc-address is not defined.",performance,hdfs
772,dfs.namenode.safemode.threshold-pct,Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. Values less than or equal to 0 mean not to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safe mode permanent.,security,hdfs
773,dfs.namenode.safemode.min.datanodes,Specifies the number of datanodes that must be considered alive before the name node exits safemode. Values less than or equal to 0 mean not to take the number of live datanodes into account when deciding whether to remain in safe mode during startup. Values greater than the number of datanodes in the cluster will make safe mode permanent.,security,hdfs
774,dfs.namenode.safemode.extension,"Determines extension of safe mode in milliseconds after the threshold level is reached. Support multiple time unit suffix (case insensitive), as described in dfs.heartbeat.interval.",security,hdfs
775,dfs.namenode.resource.check.interval,"The interval in milliseconds at which the NameNode resource checker runs. The checker calculates the number of the NameNode storage volumes whose available spaces are more than dfs.namenode.resource.du.reserved, and enters safemode if the number becomes lower than the minimum value specified by dfs.namenode.resource.checked.volumes.minimum.",reliability,hdfs
776,dfs.namenode.resource.du.reserved,The amount of space to reserve/require for a NameNode storage directory in bytes. The default is 100MB.,others,hdfs
777,dfs.namenode.resource.checked.volumes,A list of local directories for the NameNode resource checker to check in addition to the local edits directories.,environment,hdfs
778,dfs.namenode.resource.checked.volumes.minimum,The minimum number of redundant NameNode storage volumes required.,others,hdfs
779,dfs.datanode.balance.bandwidthPerSec,"Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size (such as 128k, 512m, 1g, etc.). Or provide complete size in bytes (such as 134217728 for 128 MB).",performance,hdfs
780,dfs.hosts,"Names a file that contains a list of hosts that are permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, all hosts are permitted.",security,hdfs
781,dfs.hosts.exclude,"Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.",security,hdfs
782,dfs.namenode.max.objects,"The maximum number of files, directories and blocks dfs supports. A value of zero indicates no limit to the number of objects that dfs supports.",others,hdfs
783,dfs.namenode.datanode.registration.ip-hostname-check,"If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.",security,hdfs
784,dfs.namenode.decommission.interval,"Namenode periodicity in seconds to check if decommission or maintenance is complete. Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.",reliability,hdfs
785,dfs.namenode.decommission.blocks.per.interval,"The approximate number of blocks to process per decommission or maintenance interval, as defined in dfs.namenode.decommission.interval.",performance,hdfs
786,dfs.namenode.decommission.max.concurrent.tracked.nodes,The maximum number of decommission-in-progress or entering-maintenance datanodes nodes that will be tracked at one time by the namenode. Tracking these datanode consumes additional NN memory proportional to the number of blocks on the datnode. Having a conservative limit reduces the potential impact of decommissioning or maintenance of a large number of nodes at once. A value of 0 means no limit will be enforced.,debuggability,hdfs
787,dfs.namenode.replication.interval,The periodicity in seconds with which the namenode computes replication work for datanodes.,reliability,hdfs
788,dfs.namenode.accesstime.precision,The access time for HDFS file is precise upto this value. The default value is 1 hour. Setting a value of 0 disables access times for HDFS.,others,hdfs
789,dfs.datanode.plugins,Comma-separated list of datanode plug-ins to be activated.,others,hdfs
790,dfs.namenode.plugins,Comma-separated list of namenode plug-ins to be activated.,others,hdfs
791,dfs.namenode.block-placement-policy.default.prefer-local-node,"Controls how the default block placement policy places the first replica of a block. When true, it will prefer the node where the client is running. When false, it will prefer a node in the same rack as the client. Setting to false avoids situations where entire copies of large files end up on a single node, thus creating hotspots.",performance,hdfs
792,dfs.stream-buffer-size,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,hdfs
793,dfs.bytes-per-checksum,The number of bytes per checksum. Must not be larger than dfs.stream-buffer-size,performance,hdfs
794,dfs.client-write-packet-size,Packet size for clients to write,others,hdfs
795,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,"The maximum period to keep a DN in the excluded nodes list at a client. After this period, in milliseconds, the previously excluded node(s) will be removed automatically from the cache and will be considered good for block allocations again. Useful to lower or raise in situations where you keep a file open for very long periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance restarts. Defaults to 10 minutes.",reliability,hdfs
796,dfs.namenode.checkpoint.dir,Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.,environment,hdfs
797,dfs.namenode.checkpoint.edits.dir,Determines where on the local filesystem the DFS secondary name node should store the temporary edits to merge. If this is a comma-delimited list of directories then the edits is replicated in all of the directories for redundancy. Default value is same as dfs.namenode.checkpoint.dir,environment,hdfs
798,dfs.namenode.checkpoint.period,The number of seconds between two periodic checkpoints.,reliability,hdfs
799,dfs.namenode.checkpoint.txns,"The Secondary NameNode or CheckpointNode will create a checkpoint of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless of whether 'dfs.namenode.checkpoint.period' has expired.",reliability,hdfs
800,dfs.namenode.checkpoint.check.period,The SecondaryNameNode and CheckpointNode will poll the NameNode every 'dfs.namenode.checkpoint.check.period' seconds to query the number of uncheckpointed transactions.,reliability,hdfs
801,dfs.namenode.checkpoint.max-retries,"The SecondaryNameNode retries failed checkpointing. If the failure occurs while loading fsimage or replaying edits, the number of retries is limited by this variable.",reliability,hdfs
802,dfs.namenode.num.checkpoints.retained,The number of image checkpoint files (fsimage_*) that will be retained by the NameNode and Secondary NameNode in their storage directories. All edit logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained checkpoint will also be retained.,reliability,hdfs
803,dfs.namenode.num.extra.edits.retained,"The number of extra transactions which should be retained beyond what is minimally necessary for a NN restart. It does not translate directly to file's age, or the number of files kept, but to the number of transactions (here ""edits"" means transactions). One edit file may contain several transactions (edits). During checkpoint, NameNode will identify the total number of edits to retain as extra by checking the latest checkpoint transaction value, subtracted by the value of this property. Then, it scans edits files to identify the older ones that don't include the computed range of retained transactions that are to be kept around, and purges them subsequently. The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have been offline for some time and need to have a longer backlog of retained edits in order to start again. Typically each edit is on the order of a few hundred bytes, so the default of 1 million edits should be on the order of hundreds of MBs or low GBs. NOTE: Fewer extra edits may be retained than value specified for this setting if doing so would mean that more segments would be retained than the number configured by dfs.namenode.max.extra.edits.segments.retained.",reliability,hdfs
804,dfs.namenode.max.extra.edits.segments.retained,"The maximum number of extra edit log segments which should be retained beyond what is minimally necessary for a NN restart. When used in conjunction with dfs.namenode.num.extra.edits.retained, this configuration property serves to cap the number of extra edits files to a reasonable value.",reliability,hdfs
805,dfs.namenode.delegation.key.update-interval,The update interval for master key for delegation tokens in the namenode in milliseconds.,security,hdfs
806,dfs.namenode.delegation.token.max-lifetime,The maximum lifetime in milliseconds for which a delegation token is valid.,security,hdfs
807,dfs.namenode.delegation.token.renew-interval,The renewal interval for delegation token in milliseconds.,security,hdfs
808,dfs.datanode.failed.volumes.tolerated,The number of volumes that are allowed to fail before a datanode stops offering service. By default any volume failure will cause a datanode to shutdown.,reliability,hdfs
809,dfs.image.compress,Should the dfs image be compressed?,performance,hdfs
810,dfs.image.compression.codec,"If the dfs image is compressed, how should they be compressed? This has to be a codec defined in io.compression.codecs.",performance,hdfs
811,dfs.image.transfer.timeout,"Socket timeout for the HttpURLConnection instance used in the image transfer. This is measured in milliseconds. This timeout prevents client hangs if the connection is idle for this configured timeout, during image transfer.",reliability,hdfs
812,dfs.image.transfer.bandwidthPerSec,"Maximum bandwidth used for regular image transfers (instead of bootstrapping the standby namenode), in bytes per second. This can help keep normal namenode operations responsive during checkpointing. A default value of 0 indicates that throttling is disabled. The maximum bandwidth used for bootstrapping standby namenode is configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec.",performance,hdfs
813,dfs.image.transfer-bootstrap-standby.bandwidthPerSec,"Maximum bandwidth used for transferring image to bootstrap standby namenode, in bytes per second. A default value of 0 indicates that throttling is disabled. This default value should be used in most cases, to ensure timely HA operations. The maximum bandwidth used for regular image transfers is configured with dfs.image.transfer.bandwidthPerSec.",performance,hdfs
814,dfs.image.transfer.chunksize,Chunksize in bytes to upload the checkpoint. Chunked streaming is used to avoid internal buffering of contents of image file of huge size.,performance,hdfs
815,dfs.namenode.support.allow.format,"Does HDFS namenode allow itself to be formatted? You may consider setting this to false for any production cluster, to avoid any possibility of formatting a running DFS.",others,hdfs
816,dfs.datanode.max.transfer.threads,Specifies the maximum number of threads to use for transferring data in and out of the DN.,performance,hdfs
817,dfs.datanode.scan.period.hours,"If this is positive, the DataNode will not scan any individual block more than once in the specified scan period. If this is negative, the block scanner is disabled. If this is set to zero, then the default value of 504 hours or 3 weeks is used. Prior versions of HDFS incorrectly documented that setting this key to zero will disable the block scanner.",reliability,hdfs
818,dfs.block.scanner.volume.bytes.per.second,"If this is 0, the DataNode's block scanner will be disabled. If this is positive, this is the number of bytes per second that the DataNode's block scanner will try to scan from each volume.",performance,hdfs
819,dfs.datanode.readahead.bytes,"While reading block files, if the Hadoop native libraries are available, the datanode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the datanode will attempt to read ahead. This feature may be disabled by configuring this property to 0. If the native libraries are not available, this configuration has no effect.",performance,hdfs
820,dfs.datanode.drop.cache.behind.reads,"In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This behavior is automatically disabled for workloads which read only short sections of a block (e.g HBase random-IO workloads). This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",performance,hdfs
821,dfs.datanode.drop.cache.behind.writes,"In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache space usage for more cacheable data. If the Hadoop native libraries are not available, this configuration has no effect.",performance,hdfs
822,dfs.datanode.sync.behind.writes,"If this configuration is enabled, the datanode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. If the Hadoop native libraries are not available, this configuration has no effect.",performance,hdfs
823,dfs.client.failover.max.attempts,Expert only. The number of client failover attempts that should be made before the failover is considered failed.,reliability,hdfs
824,dfs.client.failover.sleep.base.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least dfs.client.failover.sleep.base.millis milliseconds. And so on.",reliability,hdfs
825,dfs.client.failover.sleep.max.millis,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of dfs.client.failover.sleep.max.millis milliseconds.",reliability,hdfs
826,dfs.client.failover.connection.retries,Expert only. Indicates the number of retries a failover IPC client will make to establish a server connection.,reliability,hdfs
827,dfs.client.failover.connection.retries.on.timeouts,Expert only. The number of retry attempts a failover IPC client will make on socket timeout when establishing a server connection.,reliability,hdfs
828,dfs.client.datanode-restart.timeout,"Expert only. The time to wait, in seconds, from reception of an datanode shutdown notification for quick restart, until declaring the datanode dead and invoking the normal recovery mechanisms. The notification is sent by a datanode when it is being shutdown using the shutdownDatanode admin command with the upgrade option.",reliability,hdfs
829,dfs.nameservices,Comma-separated list of nameservices.,environment,hdfs
830,dfs.nameservice.id,The ID of this nameservice. If the nameservice ID is not configured or more than one nameservice is configured for dfs.nameservices it is determined automatically by matching the local node's address with the configured address.,environment,hdfs
831,dfs.internal.nameservices,Comma-separated list of nameservices that belong to this cluster. Datanode will report to all the nameservices in this list. By default this is set to the value of dfs.nameservices.,others,hdfs
832,dfs.ha.namenodes.EXAMPLENAMESERVICE,"The prefix for a given nameservice, contains a comma-separated list of namenodes for a given nameservice (eg EXAMPLENAMESERVICE). Unique identifiers for each NameNode in the nameservice, delimited by commas. This will be used by DataNodes to determine all the NameNodes in the cluster. For example, if you used mycluster as the nameservice ID previously, and you wanted to use nn1 and nn2 as the individual IDs of the NameNodes, you would configure a property dfs.ha.namenodes.mycluster, and its value ""nn1,nn2"".",others,hdfs
833,dfs.ha.namenode.id,The ID of this namenode. If the namenode ID is not configured it is determined automatically by matching the local node's address with the configured address.,environment,hdfs
834,dfs.ha.log-roll.period,"How often, in seconds, the StandbyNode should ask the active to roll edit logs. Since the StandbyNode only reads from finalized log segments, the StandbyNode will only be as up-to-date as how often the logs are rolled. Note that failover triggers a log roll so the StandbyNode will be up to date before it becomes active.",debuggability,hdfs
835,dfs.ha.tail-edits.period,"How often, in seconds, the StandbyNode should check for new finalized log segments in the shared edits log.",debuggability,hdfs
836,dfs.ha.tail-edits.rolledits.timeout,The timeout in seconds of calling rollEdits RPC on Active NN.,reliability,hdfs
837,dfs.ha.automatic-failover.enabled,Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration.,reliability,hdfs
838,dfs.client.use.datanode.hostname,Whether clients should use datanode hostnames when connecting to datanodes.,others,hdfs
839,dfs.datanode.use.datanode.hostname,Whether datanodes should use datanode hostnames when connecting to other datanodes for data transfer.,others,hdfs
840,dfs.client.local.interfaces,"A comma separated list of network interface names to use for data transfer between the client and datanodes. When creating a connection to read from or write to a datanode, the client chooses one of the specified interfaces at random and binds its socket to the IP of that interface. Individual names may be specified as either an interface name (eg ""eth0""), a subinterface name (eg ""eth0:0""), or an IP address (which may be specified using CIDR notation to match a range of IPs).",environment,hdfs
841,dfs.datanode.shared.file.descriptor.paths,"A comma-separated list of paths to use when creating file descriptors that will be shared between the DataNode and the DFSClient. Typically we use /dev/shm, so that the file descriptors will not be written to disk. Systems that don't have /dev/shm will fall back to /tmp by default.",environment,hdfs
842,dfs.short.circuit.shared.memory.watcher.interrupt.check.ms,The length of time in milliseconds that the short-circuit shared memory watcher will go between checking for java interruptions sent from other threads. This is provided mainly for unit tests.,reliability,hdfs
843,dfs.namenode.kerberos.principal,The NameNode service principal. This is typically set to nn/_HOST@REALM.TLD. Each NameNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on both NameNodes in an HA setup.,security,hdfs
844,dfs.namenode.keytab.file,The keytab file used by each NameNode daemon to login as its service principal. The principal name is configured with dfs.namenode.kerberos.principal.,environment,hdfs
845,dfs.datanode.kerberos.principal,The DataNode service principal. This is typically set to dn/_HOST@REALM.TLD. Each DataNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all DataNodes.,security,hdfs
846,dfs.datanode.keytab.file,The keytab file used by each DataNode daemon to login as its service principal. The principal name is configured with dfs.datanode.kerberos.principal.,environment,hdfs
847,dfs.journalnode.kerberos.principal,The JournalNode service principal. This is typically set to jn/_HOST@REALM.TLD. Each JournalNode will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on all JournalNodes.,security,hdfs
848,dfs.journalnode.keytab.file,The keytab file used by each JournalNode daemon to login as its service principal. The principal name is configured with dfs.journalnode.kerberos.principal.,environment,hdfs
849,dfs.namenode.kerberos.internal.spnego.principal,"The server principal used by the NameNode for web UI SPNEGO authentication when Kerberos security is enabled. This is typically set to HTTP/_HOST@REALM.TLD The SPNEGO server principal begins with the prefix HTTP/ by convention. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",security,hdfs
850,dfs.journalnode.kerberos.internal.spnego.principal,"The server principal used by the JournalNode HTTP Server for SPNEGO authentication when Kerberos security is enabled. This is typically set to HTTP/_HOST@REALM.TLD. The SPNEGO server principal begins with the prefix HTTP/ by convention. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab. For most deployments this can be set to ${dfs.web.authentication.kerberos.principal} i.e use the value of dfs.web.authentication.kerberos.principal.",security,hdfs
851,dfs.secondary.namenode.kerberos.internal.spnego.principal,"The server principal used by the Secondary NameNode for web UI SPNEGO authentication when Kerberos security is enabled. Like all other Secondary NameNode settings, it is ignored in an HA setup. If the value is '*', the web server will attempt to login with every principal specified in the keytab file dfs.web.authentication.kerberos.keytab.",security,hdfs
852,dfs.web.authentication.kerberos.principal,The server principal used by the NameNode for WebHDFS SPNEGO authentication. Required when WebHDFS and security are enabled. In most secure clusters this setting is also used to specify the values for dfs.namenode.kerberos.internal.spnego.principal and dfs.journalnode.kerberos.internal.spnego.principal.,security,hdfs
853,dfs.web.authentication.kerberos.keytab,The keytab file for the principal corresponding to dfs.web.authentication.kerberos.principal.,environment,hdfs
854,dfs.namenode.kerberos.principal.pattern,A client-side RegEx that can be configured to control allowed realms to authenticate with (useful in cross-realm env.),security,hdfs
855,dfs.namenode.avoid.read.stale.datanode,"Indicate whether or not to avoid reading from ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Stale datanodes will be moved to the end of the node list returned for reading. See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.",reliability,hdfs
856,dfs.namenode.avoid.write.stale.datanode,"Indicate whether or not to avoid writing to ""stale"" datanodes whose heartbeat messages have not been received by the namenode for more than a specified time interval. Writes will avoid using stale datanodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting for reads.",reliability,hdfs
857,dfs.namenode.stale.datanode.interval,"Default time interval in milliseconds for marking a datanode as ""stale"", i.e., if the namenode has not received heartbeat msg from a datanode for more than this time interval, the datanode will be marked and treated as ""stale"" by default. The stale interval cannot be too small since otherwise this may cause too frequent change of stale states. We thus set a minimum stale interval value (the default value is 3 times of heartbeat interval) and guarantee that the stale interval cannot be less than the minimum value. A stale data node is avoided during lease/block recovery. It can be conditionally avoided for reads (see dfs.namenode.avoid.read.stale.datanode) and for writes (see dfs.namenode.avoid.write.stale.datanode).",reliability,hdfs
858,dfs.namenode.write.stale.datanode.ratio,"When the ratio of number stale datanodes to total datanodes marked is greater than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.",reliability,hdfs
859,dfs.namenode.invalidate.work.pct.per.iteration,"*Note*: Advanced property. Change with caution. This determines the percentage amount of block invalidations (deletes) to do over a single DN heartbeat deletion command. The final deletion count is determined by applying this percentage to the number of live nodes in the system. The resultant number is the number of blocks from the deletion list chosen for proper invalidation over a single heartbeat of a single DN. Value should be a positive, non-zero percentage in float notation (X.Yf), with 1.0f meaning 100%.",reliability,hdfs
860,dfs.namenode.replication.work.multiplier.per.iteration,"*Note*: Advanced property. Change with caution. This determines the total amount of block transfers to begin in parallel at a DN, for replication, when such a command list is being sent over a DN heartbeat by the NN. The actual number is obtained by multiplying this multiplier with the total number of live nodes in the cluster. The result number is the number of blocks to begin transfers immediately for, per DN heartbeat. This number can be any positive, non-zero integer.",reliability,hdfs
861,nfs.server.port,Specify the port number used by Hadoop NFS.,environment,hdfs
862,nfs.mountd.port,Specify the port number used by Hadoop mount daemon.,environment,hdfs
863,nfs.dump.dir,"This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. One needs to make sure the directory has enough space.",environment,hdfs
864,nfs.rtmax,"This is the maximum size in bytes of a READ request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's rsize(add rsize= # of bytes to the mount directive).",performance,hdfs
865,nfs.wtmax,"This is the maximum size in bytes of a WRITE request supported by the NFS gateway. If you change this, make sure you also update the nfs mount's wsize(add wsize= # of bytes to the mount directive).",performance,hdfs
866,nfs.keytab.file,*Note*: Advanced property. Change with caution. This is the path to the keytab file for the hdfs-nfs gateway. This is required when the cluster is kerberized.,security,hdfs
867,nfs.kerberos.principal,*Note*: Advanced property. Change with caution. This is the name of the kerberos principal. This is required when the cluster is kerberized.It must be of this format: nfs-gateway-user/nfs-gateway-host@kerberos-realm,security,hdfs
868,nfs.allow.insecure.ports,"When set to false, client connections originating from unprivileged ports (those above 1023) will be rejected. This is to ensure that clients connecting to this NFS Gateway must have had root privilege on the machine where they're connecting from.",security,hdfs
869,dfs.webhdfs.enabled,Enable WebHDFS (REST API) in Namenodes and Datanodes.,environment,hdfs
870,hadoop.fuse.connection.timeout,The minimum number of seconds that we'll cache libhdfs connection objects in fuse_dfs. Lower values will result in lower memory consumption; higher values may speed up access by avoiding the overhead of creating new connection objects.,reliability,hdfs
871,hadoop.fuse.timer.period,The number of seconds between cache expiry checks in fuse_dfs. Lower values will result in fuse_dfs noticing changes to Kerberos ticket caches more quickly.,reliability,hdfs
872,dfs.namenode.metrics.logger.period.seconds,This setting controls how frequently the NameNode logs its metrics. The logging configuration must also define one or more appenders for NameNodeMetricsLog for the metrics to be logged. NameNode metrics logging is disabled if this value is set to zero or less than zero.,debuggability,hdfs
873,dfs.datanode.metrics.logger.period.seconds,This setting controls how frequently the DataNode logs its metrics. The logging configuration must also define one or more appenders for DataNodeMetricsLog for the metrics to be logged. DataNode metrics logging is disabled if this value is set to zero or less than zero.,debuggability,hdfs
874,dfs.client.write.max-packets-in-flight,The maximum number of DFSPackets allowed in flight.,reliability,hdfs
875,dfs.datanode.peer.stats.enabled,A switch to turn on/off tracking DataNode peer statistics.,others,hdfs
876,dfs.datanode.outliers.report.interval,This setting controls how frequently DataNodes will report their peer latencies to the NameNode via heartbeats. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed. It is ignored if dfs.datanode.peer.stats.enabled is false.,reliability,hdfs
877,dfs.datanode.fileio.profiling.sampling.percentage,This setting controls the percentage of file I/O events which will be profiled for DataNode disk statistics. The default value of 0 disables disk statistics. Set to an integer value between 1 and 100 to enable disk statistics.,performance,hdfs
878,hadoop.user.group.metrics.percentiles.intervals,"A comma-separated list of the granularity in seconds for the metrics which describe the 50/75/90/95/99th percentile latency for group resolution in milliseconds. By default, percentile latency metrics are disabled.",performance,hdfs
879,dfs.encrypt.data.transfer,"Whether or not actual block data that is read/written from/to HDFS should be encrypted on the wire. This only needs to be set on the NN and DNs, clients will deduce this automatically. It is possible to override this setting per connection by specifying custom logic via dfs.trustedchannel.resolver.class.",security,hdfs
880,dfs.encrypt.data.transfer.algorithm,"This value may be set to either ""3des"" or ""rc4"". If nothing is set, then the configured JCE default on the system is used (usually 3DES.) It is widely believed that 3DES is more cryptographically secure, but RC4 is substantially faster. Note that if AES is supported by both the client and server then this encryption algorithm will only be used to initially transfer keys for AES. (See dfs.encrypt.data.transfer.cipher.suites.)",security,hdfs
881,dfs.encrypt.data.transfer.cipher.suites,"This value may be either undefined or AES/CTR/NoPadding. If defined, then dfs.encrypt.data.transfer uses the specified cipher suite for data encryption. If not defined, then only the algorithm specified in dfs.encrypt.data.transfer.algorithm is used. By default, the property is not defined.",security,hdfs
882,dfs.encrypt.data.transfer.cipher.key.bitlength,"The key bitlength negotiated by dfsclient and datanode for encryption. This value may be set to either 128, 192 or 256.",security,hdfs
883,dfs.trustedchannel.resolver.class,"TrustedChannelResolver is used to determine whether a channel is trusted for plain data transfer. The TrustedChannelResolver is invoked on both client and server side. If the resolver indicates that the channel is trusted, then the data transfer will not be encrypted even if dfs.encrypt.data.transfer is set to true. The default implementation returns false indicating that the channel is not trusted.",security,hdfs
884,dfs.data.transfer.protection,"A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.",security,hdfs
885,dfs.data.transfer.saslproperties.resolver.class,"SaslPropertiesResolver used to resolve the QOP used for a connection to the DataNode when reading or writing block data. If not specified, the value of hadoop.security.saslproperties.resolver.class is used as the default value.",security,hdfs
886,dfs.datanode.hdfs-blocks-metadata.enabled,Boolean which enables backend datanode-side support for the experimental DistributedFileSystem#getFileVBlockStorageLocations API.,performance,hdfs
887,dfs.client.file-block-storage-locations.num-threads,Number of threads used for making parallel RPCs in DistributedFileSystem#getFileBlockStorageLocations().,performance,hdfs
888,dfs.client.file-block-storage-locations.timeout.millis,Timeout (in milliseconds) for the parallel RPCs made in DistributedFileSystem#getFileBlockStorageLocations().,reliability,hdfs
889,dfs.journalnode.rpc-address,The JournalNode RPC server address and port.,environment,hdfs
890,dfs.journalnode.rpc-bind-host,"The actual address the RPC server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.rpc-address. This is useful for making the JournalNode listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
891,dfs.journalnode.http-address,The address and port the JournalNode HTTP server listens on. If the port is 0 then the server will start on a free port.,environment,hdfs
892,dfs.journalnode.http-bind-host,"The actual address the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.http-address. This is useful for making the JournalNode HTTP server listen on allinterfaces by setting it to 0.0.0.0.",environment,hdfs
893,dfs.journalnode.https-address,The address and port the JournalNode HTTPS server listens on. If the port is 0 then the server will start on a free port.,environment,hdfs
894,dfs.journalnode.https-bind-host,"The actual address the HTTP server will bind to. If this optional address is set, it overrides only the hostname portion of dfs.journalnode.https-address. This is useful for making the JournalNode HTTP server listen on all interfaces by setting it to 0.0.0.0.",environment,hdfs
895,dfs.namenode.audit.loggers,"List of classes implementing audit loggers that will receive audit events. These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger. The special value ""default"" can be used to reference the default audit logger, which uses the configured log system. Installing custom audit loggers may affect the performance and stability of the NameNode. Refer to the custom logger's documentation for more details.",debuggability,hdfs
896,dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis.",performance,hdfs
897,dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction,"Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.",performance,hdfs
898,dfs.namenode.edits.noeditlogchannelflush,"Specifies whether to flush edit log file channel. When set, expensive FileChannel#force calls are skipped and synchronous disk writes are enabled instead by opening the edit log file with RandomAccessFile(""rws"") flags. This can significantly improve the performance of edit log writes on the Windows platform. Note that the behavior of the ""rws"" flags is platform and hardware specific and might not provide the same level of guarantees as FileChannel#force. For example, the write will skip the disk-cache on SAS and SCSI devices while it might not on SATA devices. This is an expert level setting, change with caution.",performance,hdfs
899,dfs.client.cache.drop.behind.writes,"Just like dfs.datanode.drop.cache.behind.writes, this setting causes the page cache to be dropped behind HDFS writes, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.writes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.",performance,hdfs
900,dfs.client.cache.drop.behind.reads,"Just like dfs.datanode.drop.cache.behind.reads, this setting causes the page cache to be dropped behind HDFS reads, potentially freeing up more memory for other uses. Unlike dfs.datanode.drop.cache.behind.reads, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. If the native libraries are not available to the DataNode, this configuration has no effect.",performance,hdfs
901,dfs.client.cache.readahead,"When using remote reads, this setting causes the datanode to read ahead in the block file using posix_fadvise, potentially decreasing I/O wait times. Unlike dfs.datanode.readahead.bytes, this is a client-side setting rather than a setting for the entire datanode. If present, this setting will override the DataNode default. When using local reads, this setting determines how much readahead we do in BlockReaderLocal. If the native libraries are not available to the DataNode, this configuration has no effect.",performance,hdfs
902,dfs.client.server-defaults.validity.period.ms,The amount of milliseconds after which cached server defaults are updated. By default this parameter is set to 1 hour.,reliability,hdfs
903,dfs.namenode.enable.retrycache,"This enables the retry cache on the namenode. Namenode tracks for non-idempotent requests the corresponding response. If a client retries the request, the response from the retry cache is sent. Such operations are tagged with annotation @AtMostOnce in namenode protocols. It is recommended that this flag be set to true. Setting it to false, will result in clients getting failure responses to retried request. This flag must be enabled in HA setup for transparent fail-overs. The entries in the cache have expiration time configurable using dfs.namenode.retrycache.expirytime.millis.",reliability,hdfs
904,dfs.namenode.retrycache.expirytime.millis,The time for which retry cache entries are retained.,reliability,hdfs
905,dfs.namenode.retrycache.heap.percent,"This parameter configures the heap size allocated for retry cache (excluding the response cached). This corresponds to approximately 4096 entries for every 64MB of namenode process java heap size. Assuming retry cache entry expiration time (configured using dfs.namenode.retrycache.expirytime.millis) of 10 minutes, this enables retry cache to support 7 operations per second sustained for 10 minutes. As the heap size is increased, the operation rate linearly increases.",performance,hdfs
906,dfs.metrics.percentiles.intervals,"Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics on the Namenode and Datanode. By default, percentile latency metrics are disabled.",performance,hdfs
907,dfs.client.mmap.cache.size,"When zero-copy reads are used, the DFSClient keeps a cache of recently used memory mapped regions. This parameter controls the maximum number of entries that we will keep in that cache. The larger this number is, the more file descriptors we will potentially use for memory-mapped files. mmaped files also use virtual address space. You may need to increase your ulimit virtual address space limits before increasing the client mmap cache size. Note that you can still do zero-copy reads when this size is set to 0.",performance,hdfs
908,dfs.client.mmap.enabled,"If this is set to false, the client won't attempt to perform memory-mapped reads.",performance,hdfs
909,dfs.client.mmap.retry.timeout.ms,The minimum amount of time that we will wait before retrying a failed mmap operation.,reliability,hdfs
910,dfs.client.short.circuit.replica.stale.threshold.ms,"The maximum amount of time that we will consider a short-circuit replica to be valid, if there is no communication from the DataNode. After this time has elapsed, we will re-fetch the short-circuit replica even if it is in the cache.",reliability,hdfs
911,dfs.namenode.path.based.cache.block.map.allocation.percent,The percentage of the Java heap which we will allocate to the cached blocks map. The cached blocks map is a hash map which uses chained hashing. Smaller maps may be accessed more slowly if the number of cached blocks is large; larger maps will consume more memory.,performance,hdfs
912,dfs.datanode.max.locked.memory,"The amount of memory in bytes to use for caching of block replicas in memory on the datanode. The datanode's maximum locked memory soft ulimit (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode will abort on startup. By default, this parameter is set to 0, which disables in-memory caching. If the native libraries are not available to the DataNode, this configuration has no effect.",performance,hdfs
913,dfs.namenode.list.cache.directives.num.responses,This value controls the number of cache directives that the NameNode will send over the wire in response to a listDirectives RPC.,performance,hdfs
914,dfs.namenode.list.cache.pools.num.responses,This value controls the number of cache pools that the NameNode will send over the wire in response to a listPools RPC.,performance,hdfs
915,dfs.namenode.path.based.cache.refresh.interval.ms,"The amount of milliseconds between subsequent path cache rescans. Path cache rescans are when we calculate which blocks should be cached, and on what datanodes. By default, this parameter is set to 30 seconds.",reliability,hdfs
916,dfs.namenode.path.based.cache.retry.interval.ms,"When the NameNode needs to uncache something that is cached, or cache something that is not cached, it must direct the DataNodes to do so by sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode heartbeat. This parameter controls how frequently the NameNode will resend these commands.",reliability,hdfs
917,dfs.datanode.fsdatasetcache.max.threads.per.volume,The maximum number of threads per volume to use for caching new data on the datanode. These threads consume both I/O and CPU. This can affect normal datanode operations.,performance,hdfs
918,dfs.cachereport.intervalMsec,"Determines cache reporting interval in milliseconds. After this amount of time, the DataNode sends a full report of its cache state to the NameNode. The NameNode uses the cache report to update its map of cached blocks to DataNode locations. This configuration has no effect if in-memory caching has been disabled by setting dfs.datanode.max.locked.memory to 0 (which is the default). If the native libraries are not available to the DataNode, this configuration has no effect.",reliability,hdfs
919,dfs.namenode.edit.log.autoroll.multiplier.threshold,"Determines when an active namenode will roll its own edit log. The actual threshold (in number of edits) is determined by multiplying this value by dfs.namenode.checkpoint.txns. This prevents extremely large edit files from accumulating on the active namenode, which can cause timeouts during namenode startup and pose an administrative hassle. This behavior is intended as a failsafe for when the standby or secondary namenode fail to roll the edit log by the normal checkpoint threshold.",reliability,hdfs
920,dfs.namenode.edit.log.autoroll.check.interval.ms,"How often an active namenode will check if it needs to roll its edit log, in milliseconds.",reliability,hdfs
921,dfs.webhdfs.user.provider.user.pattern,"Valid pattern for user and group names for webhdfs, it must be a valid java regex.",security,hdfs
922,dfs.webhdfs.acl.provider.permission.pattern,"Valid pattern for user and group names in webhdfs acl operations, it must be a valid java regex.",security,hdfs
923,dfs.webhdfs.socket.connect-timeout,"Socket timeout for connecting to WebHDFS servers. This prevents a WebHDFS client from hanging if the server hostname is misconfigured, or the server does not response before the timeout expires. Value is followed by a unit specifier: ns, us, ms, s, m, h, d for nanoseconds, microseconds, milliseconds, seconds, minutes, hours, days respectively. Values should provide units, but milliseconds are assumed.",reliability,hdfs
924,dfs.webhdfs.socket.read-timeout,"Socket timeout for reading data from WebHDFS servers. This prevents a WebHDFS client from hanging if the server stops sending data. Value is followed by a unit specifier: ns, us, ms, s, m, h, d for nanoseconds, microseconds, milliseconds, seconds, minutes, hours, days respectively. Values should provide units, but milliseconds are assumed.",reliability,hdfs
925,dfs.client.context,"The name of the DFSClient context that we should use. Clients that share a context share a socket cache and short-circuit cache, among other things. You should only change this if you don't want to share with another set of threads.",environment,hdfs
926,dfs.client.read.shortcircuit,This configuration parameter turns on short-circuit local reads.,performance,hdfs
927,dfs.client.socket.send.buffer.size,"Socket send buffer size for a write pipeline in DFSClient side. This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",performance,hdfs
928,dfs.domain.socket.path,"Optional. This is a path to a UNIX domain socket that will be used for communication between the DataNode and local HDFS clients. If the string ""_PORT"" is present in this path, it will be replaced by the TCP port of the DataNode.",environment,hdfs
929,dfs.client.read.shortcircuit.skip.checksum,"If this configuration parameter is set, short-circuit local reads will skip checksums. This is normally not recommended, but it may be useful for special setups. You might consider using this if you are doing your own checksumming outside of HDFS.",reliability,hdfs
930,dfs.client.read.shortcircuit.streams.cache.size,"The DFSClient maintains a cache of recently opened file descriptors. This parameter controls the maximum number of file descriptors in the cache. Setting this higher will use more file descriptors, but potentially provide better performance on workloads involving lots of seeks.",performance,hdfs
931,dfs.client.read.shortcircuit.streams.cache.expiry.ms,This controls the minimum amount of time file descriptors need to sit in the client cache context before they can be closed for being inactive for too long.,reliability,hdfs
932,dfs.datanode.shared.file.descriptor.paths,Comma separated paths to the directory on which shared memory segments are created. The client and the DataNode exchange information via this shared memory segment. It tries paths in order until creation of shared memory segment succeeds.,environment,hdfs
933,dfs.namenode.audit.log.debug.cmdlist,A comma separated list of NameNode commands that are written to the HDFS namenode audit log only if the audit log level is debug.,debuggability,hdfs
934,dfs.client.mmap.cache.timeout.ms,"The minimum length of time that we will keep an mmap entry in the cache between uses. If an entry is in the cache longer than this, and nobody uses it, it will be removed by a background thread.",reliability,hdfs
935,dfs.block.local-path-access.user,Comma separated list of the users allowed to open block files on legacy short-circuit local read.,security,hdfs
936,dfs.client.domain.socket.data.traffic,This control whether we will try to pass normal data traffic over UNIX domain socket rather than over TCP socket on node-local data transfer. This is currently experimental and turned off by default.,others,hdfs
937,dfs.namenode.reject-unresolved-dn-topology-mapping,"If the value is set to true, then namenode will reject datanode registration if the topology mapping for a datanode is not resolved and NULL is returned (script defined by net.topology.script.file.name fails to execute). Otherwise, datanode will be registered and the default rack will be assigned as the topology path. Topology paths are important for data resiliency, since they define fault domains. Thus it may be unwanted behavior to allow datanode registration with the default rack if the resolving topology failed.",security,hdfs
938,dfs.client.slow.io.warning.threshold.ms,"The threshold in milliseconds at which we will log a slow io warning in a dfsclient. By default, this parameter is set to 30000 milliseconds (30 seconds).",debuggability,hdfs
939,dfs.datanode.slow.io.warning.threshold.ms,"The threshold in milliseconds at which we will log a slow io warning in a datanode. By default, this parameter is set to 300 milliseconds.",debuggability,hdfs
940,dfs.namenode.xattrs.enabled,Whether support for extended attributes is enabled on the NameNode.,others,hdfs
941,dfs.namenode.fs-limits.max-xattrs-per-inode,Maximum number of extended attributes per inode.,reliability,hdfs
942,dfs.namenode.fs-limits.max-xattr-size,"The maximum combined size of the name and value of an extended attribute in bytes. It should be larger than 0, and less than or equal to maximum size hard limit which is 32768.",reliability,hdfs
943,dfs.namenode.lease-recheck-interval-ms,During the release of lease a lock is hold that make any operations on the namenode stuck. In order to not block them during a too long duration we stop releasing lease after this max lock limit.,reliability,hdfs
944,dfs.namenode.max-lock-hold-to-release-lease-ms,During the release of lease a lock is hold that make any operations on the namenode stuck. In order to not block them during a too long duration we stop releasing lease after this max lock limit.,reliability,hdfs
945,dfs.namenode.write-lock-reporting-threshold-ms,"When a write lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",debuggability,hdfs
946,dfs.namenode.read-lock-reporting-threshold-ms,"When a read lock is held on the namenode for a long time, this will be logged as the lock is released. This sets how long the lock must be held for logging to occur.",debuggability,hdfs
947,dfs.namenode.lock.detailed-metrics.enabled,"If true, the namenode will keep track of how long various operations hold the Namesystem lock for and emit this as metrics. These metrics have names of the form FSN(Read|Write)LockNanosOperationName, where OperationName denotes the name of the operation that initiated the lock hold (this will be OTHER for certain uncategorized operations) and they export the hold time values in nanoseconds.",debuggability,hdfs
948,dfs.namenode.fslock.fair,"If this is true, the FS Namesystem lock will be used in Fair mode, which will help to prevent writer threads from being starved, but can provide lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock for more information on fair/non-fair locks.",performance,hdfs
949,dfs.namenode.startup.delay.block.deletion.sec,"The delay in seconds at which we will pause the blocks deletion after Namenode startup. By default it's disabled. In the case a directory has large number of directories and files are deleted, suggested delay is one hour to give the administrator enough time to notice large number of pending deletion blocks and take corrective action.",reliability,hdfs
950,dfs.namenode.list.encryption.zones.num.responses,"When listing encryption zones, the maximum number of zones that will be returned in a batch. Fetching the list incrementally in batches improves namenode performance.",performance,hdfs
951,dfs.namenode.list.openfiles.num.responses,"When listing open files, the maximum number of open files that will be returned in a single batch. Fetching the list incrementally in batches improves namenode performance.",performance,hdfs
952,dfs.client.use.legacy.blockreader.local,Legacy short-circuit reader implementation based on HDFS-2246 is used if this configuration parameter is true. This is for the platforms other than Linux where the new implementation based on HDFS-347 is not available.,performance,hdfs
953,dfs.namenode.edekcacheloader.interval.ms,"When KeyProvider is configured, the interval time of warming up edek cache on NN starts up / becomes active. All edeks will be loaded from KMS into provider cache. The edek cache loader will try to warm up the cache until succeed or NN leaves active state.",security,hdfs
954,dfs.namenode.inotify.max.events.per.rpc,Maximum number of events that will be sent to an inotify client in a single RPC response. The default value attempts to amortize away the overhead for this RPC while avoiding huge memory requirements for the client and NameNode (1000 events should consume no more than 1 MB.),performance,hdfs
955,dfs.user.home.dir.prefix,The directory to prepend to user name to get the user's home direcotry.,environment,hdfs
956,dfs.datanode.cache.revocation.timeout.ms,"When the DFSClient reads from a block file which the DataNode is caching, the DFSClient can skip verifying checksums. The DataNode will keep the block file in cache until the client is done. If the client takes an unusually long time, though, the DataNode may need to evict the block file from the cache anyway. This value controls how long the DataNode will wait for the client to release a replica that it is reading without checksums.",reliability,hdfs
957,dfs.datanode.cache.revocation.polling.ms,How often the DataNode should poll to see if the clients have stopped using a replica that the DataNode wants to uncache.,reliability,hdfs
958,dfs.datanode.block.id.layout.upgrade.threads,The number of threads to use when creating hard links from current to previous blocks during upgrade of a DataNode to block ID-based block layout (see HDFS-6482 for details on the layout).,performance,hdfs
959,dfs.storage.policy.enabled,Allow users to change the storage policy on files and directories.,others,hdfs
960,dfs.namenode.legacy-oiv-image.dir,"Determines where to save the namespace in the old fsimage format during checkpointing by standby NameNode or SecondaryNameNode. Users can dump the contents of the old format fsimage by oiv_legacy command. If the value is not specified, old format fsimage will not be saved in checkpoint.",environment,hdfs
961,dfs.namenode.top.enabled,Enable nntop: reporting top users on namenode,others,hdfs
962,dfs.namenode.top.window.num.buckets,Number of buckets in the rolling window implementation of nntop,performance,hdfs
963,dfs.namenode.top.num.users,Number of top users returned by the top tool,others,hdfs
964,dfs.namenode.edekcacheloader.initial.delay.ms,"When KeyProvider is configured, the time delayed until the first attempt to warm up edek cache on NN start up / become active.",reliability,hdfs
965,dfs.webhdfs.ugi.expire.after.access,"How long in milliseconds after the last access the cached UGI will expire. With 0, never expire.",reliability,hdfs
966,dfs.namenode.blocks.per.postponedblocks.rescan,Number of blocks to rescan for each iteration of postponedMisreplicatedBlocks.,performance,hdfs
967,dfs.datanode.block-pinning.enabled,Whether pin blocks on favored DataNode.,reliability,hdfs
968,dfs.namenode.top.windows.minutes,comma separated list of nntop reporting periods in minutes,reliability,hdfs
969,dfs.ha.zkfc.nn.http.timeout.ms,"The HTTP connection and read timeout value (unit is ms ) when DFS ZKFC tries to get local NN thread dump after local NN becomes SERVICE_NOT_RESPONDING or SERVICE_UNHEALTHY. If it is set to zero, DFS ZKFC won't get local NN thread dump.",reliability,hdfs
970,dfs.namenode.quota.init-threads,"The number of concurrent threads to be used in quota initialization. The speed of quota initialization also affects the namenode fail-over latency. If the size of name space is big, try increasing this.",performance,hdfs
971,dfs.datanode.transfer.socket.send.buffer.size,"Socket send buffer size for DataXceiver (mirroring packets to downstream in pipeline). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",performance,hdfs
972,dfs.datanode.transfer.socket.recv.buffer.size,"Socket receive buffer size for DataXceiver (receiving packets from client during block writing). This may affect TCP connection throughput. If it is set to zero or negative value, no buffer size will be set explicitly, thus enable tcp auto-tuning on some system. The default value is 0.",performance,hdfs
973,dfs.client.block.write.locateFollowingBlock.initial.delay.ms,"The initial delay (unit is ms) for locateFollowingBlock, the delay time will increase exponentially(double) for each retry.",reliability,hdfs
974,dfs.ha.zkfc.port,RPC port for Zookeeper Failover Controller.,environment,hdfs
975,dfs.datanode.bp-ready.timeout,The maximum wait time for datanode to be ready before failing the received request. Setting this to 0 fails requests right away if the datanode is not yet registered with the namenode. This wait time reduces initial request failures after datanode restart.,reliability,hdfs
976,dfs.datanode.cached-dfsused.check.interval.ms,"The interval check time of loading DU_CACHE_FILE in each volume. When the cluster doing the rolling upgrade operations, it will usually lead dfsUsed cache file of each volume expired and redo the du operations in datanode and that makes datanode start slowly. Adjust this property can make cache file be available for the time as you want.",reliability,hdfs
977,dfs.webhdfs.rest-csrf.enabled,"If true, then enables WebHDFS protection against cross-site request forgery (CSRF). The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.",security,hdfs
978,dfs.namenode.upgrade.domain.factor,"This is valid only when block placement policy is set to BlockPlacementPolicyWithUpgradeDomain. It defines the number of unique upgrade domains any block's replicas should have. When the number of replicas is less or equal to this value, the policy ensures each replica has an unique upgrade domain. When the number of replicas is greater than this value, the policy ensures the number of unique domains is at least this value.",reliability,hdfs
979,dfs.webhdfs.rest-csrf.methods-to-ignore,A comma-separated list of HTTP methods that do not require HTTP requests to include a custom header when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to true. The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.,security,hdfs
980,dfs.webhdfs.rest-csrf.custom-header,The name of a custom header that HTTP requests must send when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to true. The WebHDFS client also uses this property to determine whether or not it needs to send the custom CSRF prevention header in its HTTP requests.,security,hdfs
981,dfs.xframe.enabled,"If true, then enables protection against clickjacking by returning X_FRAME_OPTIONS header value set to SAMEORIGIN. Clickjacking protection prevents an attacker from using transparent or opaque layers to trick a user into clicking on a button or link on another page.",security,hdfs
982,dfs.xframe.value,"This configration value allows user to specify the value for the X-FRAME-OPTIONS. The possible values for this field are DENY, SAMEORIGIN and ALLOW-FROM. Any other value will throw an exception when namenode and datanodes are starting up.",security,hdfs
983,dfs.http.client.retry.policy.enabled,"If ""true"", enable the retry policy of WebHDFS client. If ""false"", retry policy is turned off. Enabling the retry policy can be quite useful while using WebHDFS to copy large files between clusters that could timeout, or copy files between HA clusters that could failover during the copy.",reliability,hdfs
984,dfs.http.client.retry.policy.spec,"Specify a policy of multiple linear random retry for WebHDFS client, e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on.",reliability,hdfs
985,dfs.http.client.failover.max.attempts,Specify the max number of failover attempts for WebHDFS client in case of network exception.,reliability,hdfs
986,dfs.http.client.retry.max.attempts,"Specify the max number of retry attempts for WebHDFS client, if the difference between retried attempts and failovered attempts is larger than the max number of retry attempts, there will be no more retries.",reliability,hdfs
987,dfs.webhdfs.rest-csrf.browser-useragents-regex,"A comma-separated list of regular expressions used to match against an HTTP request's User-Agent header when protection against cross-site request forgery (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.reset-csrf.enabled to true. If the incoming User-Agent matches any of these regular expressions, then the request is considered to be sent by a browser, and therefore CSRF prevention is enforced. If the request's User-Agent does not match any of these regular expressions, then the request is considered to be sent by something other than a browser, such as scripted automation. In this case, CSRF is not a potential attack vector, so the prevention is not enforced. This helps achieve backwards-compatibility with existing automation that has not been updated to send the CSRF prevention header.",security,hdfs
988,dfs.http.client.failover.sleep.max.millis,Specify the upper bound of sleep time in milliseconds between retries or failovers for WebHDFS client.,reliability,hdfs
989,dfs.http.client.failover.sleep.base.millis,Specify the base amount of time in milliseconds upon which the exponentially increased sleep time between retries or failovers is calculated for WebHDFS client.,reliability,hdfs
990,datanode.https.port,HTTPS port for DataNode.,environment,hdfs
991,dfs.balancer.dispatcherThreads,Size of the thread pool for the HDFS balancer block mover. dispatchExecutor,performance,hdfs
992,dfs.balancer.movedWinWidth,Window of time in ms for the HDFS balancer tracking blocks and its locations.,performance,hdfs
993,dfs.balancer.moverThreads,Thread pool size for executing block moves. moverThreadAllocator,performance,hdfs
994,dfs.balancer.max-size-to-move,Maximum number of bytes that can be moved by the balancer in a single thread.,performance,hdfs
995,dfs.balancer.getBlocks.min-block-size,Minimum block threshold size in bytes to ignore when fetching a source's block list.,performance,hdfs
996,dfs.balancer.getBlocks.size,Total size in bytes of Datanode blocks to get when fetching a source's block list.,performance,hdfs
997,dfs.balancer.block-move.timeout,"Maximum amount of time in milliseconds for a block to move. If this is set greater than 0, Balancer will stop waiting for a block move completion after this time. In typical clusters, a 3 to 5 minute timeout is reasonable. If timeout happens to a large proportion of block moves, this needs to be increased. It could also be that too much work is dispatched and many nodes are constantly exceeding the bandwidth limit as a result. In that case, other balancer parameters might need to be adjusted. It is disabled (0) by default.",reliability,hdfs
998,dfs.balancer.max-no-move-interval,"If this specified amount of time has elapsed and no block has been moved out of a source DataNode, on more effort will be made to move blocks out of this DataNode in the current Balancer iteration.",reliability,hdfs
999,dfs.block.invalidate.limit,"The maximum number of invalidate blocks sent by namenode to a datanode per heartbeat deletion command. This property works with ""dfs.namenode.invalidate.work.pct.per.iteration"" to throttle block deletions.",performance,hdfs
1000,dfs.block.misreplication.processing.limit,Maximum number of blocks to process for initializing replication queues.,performance,hdfs
1001,dfs.namenode.hosts.provider.classname,"The class that provides access for host files. org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager is used by default which loads files specified by dfs.hosts and dfs.hosts.exclude. If org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager is used, it will load the JSON file defined in dfs.hosts. To change class name, nn restart is required. ""dfsadmin -refreshNodes"" only refreshes the configuration files used by the class.",environment,hdfs
1002,dfs.block.replicator.classname,"Class representing block placement policy for non-striped files. There are four block placement policies currently being supported: BlockPlacementPolicyDefault, BlockPlacementPolicyWithNodeGroup, BlockPlacementPolicyRackFaultTolerant and BlockPlacementPolicyWithUpgradeDomain. BlockPlacementPolicyDefault chooses the desired number of targets for placing block replicas in a default way. BlockPlacementPolicyWithNodeGroup places block replicas on environment with node-group layer. BlockPlacementPolicyRackFaultTolerant places the replicas to more racks. BlockPlacementPolicyWithUpgradeDomain places block replicas that honors upgrade domain policy. The details of placing replicas are documented in the javadoc of the corresponding policy classes. The default policy is BlockPlacementPolicyDefault, and the corresponding class is org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.",reliability,hdfs
1003,dfs.checksum.type,Checksum type,reliability,hdfs
1004,dfs.client.block.write.locateFollowingBlock.retries,Number of retries to use when finding the next block during HDFS writes.,reliability,hdfs
1005,dfs.client.failover.proxy.provider,"The prefix (plus a required nameservice ID) for the class name of the configured Failover proxy provider for the host. For more detailed information, please consult the ""Configuration Details"" section of the HDFS High Availability documentation.",others,hdfs
1006,dfs.client.failover.random.order,Determines if the failover proxies are picked in random order instead of the configured order. The prefix can be used with an optional nameservice ID (of form dfs.client.failover.random.order[.nameservice]) in case multiple nameservices exist and random order should be enabled for specific nameservices.,reliability,hdfs
1007,dfs.client.key.provider.cache.expiry,DFS client security key cache expiration in milliseconds.,security,hdfs
1008,dfs.client.max.block.acquire.failures,Maximum failures allowed when trying to get block information from a specific datanode.,reliability,hdfs
1009,dfs.client.read.prefetch.size,The number of bytes for the DFSClient will fetch from the Namenode during a read operation. Defaults to 10 * ${dfs.blocksize}.,performance,hdfs
1010,dfs.client.read.short.circuit.replica.stale.threshold.ms,Threshold in milliseconds for read entries during short-circuit local reads.,reliability,hdfs
1011,dfs.client.read.shortcircuit.buffer.size,Buffer size in bytes for short-circuit local reads.,performance,hdfs
1012,dfs.client.replica.accessor.builder.classes,"Comma-separated classes for building ReplicaAccessor. If the classes are specified, client will use external BlockReader that uses the ReplicaAccessor built by the builder.",reliability,hdfs
1013,dfs.client.retry.interval-ms.get-last-block-length,Retry interval in milliseconds to wait between retries in getting block lengths from the datanodes.,reliability,hdfs
1014,dfs.client.retry.max.attempts,Max retry attempts for DFSClient talking to namenodes.,reliability,hdfs
1015,dfs.blockreport.incremental.intervalMsec,"If set to a positive integer, the value in ms to wait between sending incremental block reports from the Datanode to the Namenode.",reliability,hdfs
1016,dfs.client.retry.policy.spec,Set to pairs of timeouts and retries for DFSClient.,reliability,hdfs
1017,dfs.client.retry.times.get-last-block-length,Number of retries for calls to fetchLocatedBlocksAndGetLastBlockLength().,reliability,hdfs
1018,dfs.client.retry.window.base,"Base time window in ms for DFSClient retries. For each retry attempt, this value is extended linearly (e.g. 3000 ms for first attempt and first retry, 6000 ms for second retry, 9000 ms for third retry, etc.).",reliability,hdfs
1019,dfs.client.socket-timeout,Default timeout value in milliseconds for all sockets.,reliability,hdfs
1020,dfs.client.socketcache.capacity,Socket cache capacity (in entries) for short-circuit reads.,performance,hdfs
1021,dfs.client.socketcache.expiryMsec,Socket cache expiration for short-circuit reads in msec.,reliability,hdfs
1022,dfs.client.retry.policy.enabled,"If true, turns on DFSClient retry policy.",reliability,hdfs
1023,dfs.client.hedged.read.threadpool.size,"Support 'hedged' reads in DFSClient. To enable this feature, set the parameter to a positive number. The threadpool size is how many threads to dedicate to the running of these 'hedged', concurrent reads in your client.",performance,hdfs
1024,dfs.client.hedged.read.threshold.millis,Configure 'hedged' reads in DFSClient. This is the number of milliseconds to wait before starting up a 'hedged' read.,performance,hdfs
1025,dfs.client.use.legacy.blockreader,"If true, use the RemoteBlockReader class for local read short circuit. If false, use the newer RemoteBlockReader2 class.",others,hdfs
1026,dfs.client.write.byte-array-manager.count-limit,The maximum number of arrays allowed for each array length.,reliability,hdfs
1027,dfs.client.test.drop.namenode.response.number,The number of Namenode responses dropped by DFSClient for each RPC call. Used for testing the NN retry cache.,reliability,hdfs
1028,dfs.client.write.byte-array-manager.count-threshold,"The count threshold for each array length so that a manager is created only after the allocation count exceeds the threshold. In other words, the particular array length is not managed until the allocation count exceeds the threshold.",performance,hdfs
1029,dfs.client.write.byte-array-manager.count-reset-time-period-ms,The time period in milliseconds that the allocation count for each array length is reset to zero if there is no increment.,reliability,hdfs
1030,dfs.client.write.byte-array-manager.enabled,"If true, enables byte array manager used by DFSOutputStream.",others,hdfs
1031,dfs.content-summary.limit,The maximum content summary counts allowed in one locking period. 0 or a negative number means no limit (i.e. no yielding).,reliability,hdfs
1032,dfs.content-summary.sleep-microsec,"The length of time in microseconds to put the thread to sleep, between reaquiring the locks in content summary computation.",reliability,hdfs
1033,dfs.data.transfer.client.tcpnodelay,"If true, set TCP_NODELAY to sockets for transferring data from DFS client.",others,hdfs
1034,dfs.datanode.balance.max.concurrent.moves,"Maximum number of threads for Datanode balancer pending moves. This value is reconfigurable via the ""dfsadmin -reconfig"" command.",performance,hdfs
1035,dfs.datanode.fsdataset.factory,The class name for the underlying storage that stores replicas for a Datanode. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.,others,hdfs
1036,dfs.datanode.fsdataset.volume.choosing.policy,"The class name of the policy for choosing volumes in the list of directories. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy. If you would like to take into account available disk space, set the value to ""org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy"".",performance,hdfs
1037,dfs.datanode.hostname,Optional. The hostname for the Datanode containing this configuration file. Will be different for each machine. Defaults to current hostname.,environment,hdfs
1038,dfs.datanode.lazywriter.interval.sec,Interval in seconds for Datanodes for lazy persist writes.,reliability,hdfs
1039,dfs.datanode.network.counts.cache.max.size,The maximum number of entries the datanode per-host network error count cache may contain.,performance,hdfs
1040,dfs.datanode.oob.timeout-ms,"Timeout value when sending OOB response for each OOB type, which are OOB_RESTART, OOB_RESERVED1, OOB_RESERVED2, and OOB_RESERVED3, respectively. Currently, only OOB_RESTART is used.",reliability,hdfs
1041,dfs.datanode.parallel.volumes.load.threads.num,Maximum number of threads to use for upgrading data directories. The default value is the number of storage directories in the DataNode.,performance,hdfs
1042,dfs.datanode.ram.disk.replica.tracker,Name of the class implementing the RamDiskReplicaTracker interface. Defaults to org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker.,reliability,hdfs
1043,dfs.datanode.restart.replica.expiration,"During shutdown for restart, the amount of time in seconds budgeted for datanode restart.",reliability,hdfs
1044,dfs.datanode.socket.reuse.keepalive,"The window of time in ms before the DataXceiver closes a socket for a single request. If a second request occurs within that window, the socket can be reused.",reliability,hdfs
1045,dfs.datanode.socket.write.timeout,Timeout in ms for clients socket writes to DataNodes.,reliability,hdfs
1046,dfs.datanode.sync.behind.writes.in.background,"If set to true, then sync_file_range() system call will occur asynchronously. This property is only valid when the property dfs.datanode.sync.behind.writes is true.",security,hdfs
1047,dfs.datanode.transferTo.allowed,"If false, break block transfers on 32-bit machines greater than or equal to 2GB into smaller chunks.",performance,hdfs
1048,dfs.ha.fencing.methods,A list of scripts or Java classes which will be used to fence the Active NameNode during a failover. See the HDFS High Availability documentation for details on automatic HA configuration.,environment,hdfs
1049,dfs.ha.standby.checkpoints,"If true, a NameNode in Standby state periodically takes a checkpoint of the namespace, saves it to its local storage and then upload to the remote NameNode.",reliability,hdfs
1050,dfs.ha.zkfc.port,The port number that the zookeeper failover controller RPC server binds to.,environment,hdfs
1051,dfs.http.port,"The http port for used for Hftp, HttpFS, and WebHdfs file systems.",environment,hdfs
1052,dfs.https.port,The https port for used for Hsftp and SWebHdfs file systems.,environment,hdfs
1053,dfs.journalnode.edits.dir,The directory where the journal edit files are stored.,environment,hdfs
1054,dfs.journalnode.kerberos.internal.spnego.principal,Kerberos SPNEGO principal name used by the journal node.,security,hdfs
1055,dfs.journalnode.kerberos.principal,Kerberos principal name for the journal node.,security,hdfs
1056,dfs.journalnode.keytab.file,Kerberos keytab file for the journal node.,environment,hdfs
1057,dfs.ls.limit,"Limit the number of files printed by ls. If less or equal to zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.",others,hdfs
1058,dfs.mover.movedWinWidth,"The minimum time interval, in milliseconds, that a block can be moved to another location again.",reliability,hdfs
1059,dfs.mover.moverThreads,Configure the balancer's mover thread pool size.,performance,hdfs
1060,dfs.mover.retry.max.attempts,The maximum number of retries before the mover consider the move failed.,reliability,hdfs
1061,dfs.mover.max-no-move-interval,"If this specified amount of time has elapsed and no block has been moved out of a source DataNode, on more effort will be made to move blocks out of this DataNode in the current Mover iteration.",reliability,hdfs
1062,dfs.namenode.audit.log.async,"If true, enables asynchronous audit log.",debuggability,hdfs
1063,dfs.namenode.audit.log.token.tracking.id,"If true, adds a tracking ID for all audit log events.",debuggability,hdfs
1064,dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction,"Only used when the dfs.block.replicator.classname is set to org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy. Special value between 0 and 1, noninclusive. Increases chance of placing blocks on Datanodes with less disk space used.",performance,hdfs
1065,dfs.namenode.backup.dnrpc-address,Service RPC address for the backup Namenode.,environment,hdfs
1066,dfs.namenode.delegation.token.always-use,"For testing. Setting to true always allows the DT secret manager to be used, even if security is disabled.",security,hdfs
1067,dfs.namenode.edits.asynclogging,"If set to true, enables asynchronous edit logs in the Namenode. If set to false, the Namenode uses the traditional synchronous edit logs.",debuggability,hdfs
1068,dfs.namenode.edits.dir.minimum,"dfs.namenode.edits.dir includes both required directories (specified by dfs.namenode.edits.dir.required) and optional directories. The number of usable optional directories must be greater than or equal to this property. If the number of usable optional directories falls below dfs.namenode.edits.dir.minimum, HDFS will issue an error. This property defaults to 1.",reliability,hdfs
1069,dfs.namenode.edits.journal-plugin,"When FSEditLog is creating JournalManagers from dfs.namenode.edits.dir, and it encounters a URI with a schema different to ""file"" it loads the name of the implementing class from ""dfs.namenode.edits.journal-plugin.[schema]"". This class must implement JournalManager and have a constructor which takes (Configuration, URI).",environment,hdfs
1070,dfs.namenode.file.close.num-committed-allowed,"Normally a file can only be closed with all its blocks are committed. When this value is set to a positive integer N, a file can be closed when N blocks are committed and the rest complete.",reliability,hdfs
1071,dfs.namenode.inode.attributes.provider.class,Name of class to use for delegating HDFS authorization.,environment,hdfs
1072,dfs.namenode.inode.attributes.provider.bypass.users,A list of user principals (in secure cluster) or user names (in insecure cluster) for whom the external attributes provider will be bypassed for all operations. This means file attributes stored in HDFS instead of the external provider will be used for permission checking and be returned when requested.,security,hdfs
1073,dfs.namenode.max-num-blocks-to-log,Puts a limit on the number of blocks printed to the log by the Namenode after a block report.,debuggability,hdfs
1074,dfs.namenode.max.op.size,Maximum opcode size in bytes.,others,hdfs
1075,dfs.namenode.name.cache.threshold,Frequently accessed files that are accessed more times than this threshold are cached in the FSDirectory nameCache.,performance,hdfs
1076,dfs.namenode.replication.max-streams,Hard limit for the number of highest-priority replication streams.,reliability,hdfs
1077,dfs.namenode.replication.max-streams-hard-limit,Hard limit for all replication streams.,reliability,hdfs
1078,dfs.namenode.replication.pending.timeout-sec,"Timeout in seconds for block replication. If this value is 0 or less, then it will default to 5 minutes.",reliability,hdfs
1079,dfs.namenode.stale.datanode.minimum.interval,"Minimum number of missed heartbeats intervals for a datanode to be marked stale by the Namenode. The actual interval is calculated as (dfs.namenode.stale.datanode.minimum.interval * dfs.heartbeat.interval) in seconds. If this value is greater than the property dfs.namenode.stale.datanode.interval, then the calculated value above is used.",reliability,hdfs
1080,dfs.namenode.snapshot.capture.openfiles,"If true, snapshots taken will have an immutable shared copy of the open files that have valid leases. Even after the open files grow or shrink in size, snapshot will always have the previous point-in-time version of the open files, just like all other closed files. Default is false. Note: The file length captured for open files in snapshot is whats recorded in NameNode at the time of snapshot and it may be shorter than what the client has written till then. In order to capture the latest length, the client can call hflush/hsync with the flag SyncFlag.UPDATE_LENGTH on the open files handles.",reliability,hdfs
1081,dfs.namenode.snapshot.skip.capture.accesstime-only-change,"If accessTime of a file/directory changed but there is no other modification made to the file/directory, the changed accesstime will not be captured in next snapshot. However, if there is other modification made to the file/directory, the latest access time will be captured together with the modification in next snapshot.",reliability,hdfs
1082,dfs.pipeline.ecn,"If true, allows ECN (explicit congestion notification) from the Datanode.",performance,hdfs
1083,dfs.qjournal.accept-recovery.timeout.ms,Quorum timeout in milliseconds during accept phase of recovery/synchronization for a specific segment.,reliability,hdfs
1084,dfs.qjournal.finalize-segment.timeout.ms,Quorum timeout in milliseconds during finalizing for a specific segment.,reliability,hdfs
1085,dfs.qjournal.get-journal-state.timeout.ms,Timeout in milliseconds when calling getJournalState(). JournalNodes.,reliability,hdfs
1086,dfs.qjournal.new-epoch.timeout.ms,Timeout in milliseconds when getting an epoch number for write access to JournalNodes.,reliability,hdfs
1087,dfs.qjournal.prepare-recovery.timeout.ms,Quorum timeout in milliseconds during preparation phase of recovery/synchronization for a specific segment.,reliability,hdfs
1088,dfs.qjournal.queued-edits.limit.mb,Queue size in MB for quorum journal edits.,performance,hdfs
1089,dfs.qjournal.select-input-streams.timeout.ms,Timeout in milliseconds for accepting streams from JournalManagers.,reliability,hdfs
1090,dfs.qjournal.start-segment.timeout.ms,Quorum timeout in milliseconds for starting a log segment.,debuggability,hdfs
1091,dfs.qjournal.write-txns.timeout.ms,Write timeout in milliseconds when writing to a quorum of remote journals.,reliability,hdfs
1092,dfs.quota.by.storage.type.enabled,"If true, enables quotas based on storage type.",others,hdfs
1093,dfs.secondary.namenode.kerberos.principal,Kerberos principal name for the Secondary NameNode.,security,hdfs
1094,dfs.secondary.namenode.keytab.file,Kerberos keytab file for the Secondary NameNode.,environment,hdfs
1095,dfs.support.append,Enables append support on the NameNode.,others,hdfs
1096,dfs.web.authentication.filter,Authentication filter class used for WebHDFS.,security,hdfs
1097,dfs.web.authentication.simple.anonymous.allowed,"If true, allow anonymous user to access WebHDFS. Set to false to disable anonymous authentication.",security,hdfs
1098,dfs.webhdfs.netty.high.watermark,High watermark configuration to Netty for Datanode WebHdfs.,performance,hdfs
1099,dfs.webhdfs.netty.low.watermark,Low watermark configuration to Netty for Datanode WebHdfs.,performance,hdfs
1100,dfs.webhdfs.oauth2.access.token.provider,Access token provider class for WebHDFS using OAuth2. Defaults to org.apache.hadoop.hdfs.web.oauth2.ConfCredentialBasedAccessTokenProvider.,security,hdfs
1101,dfs.webhdfs.oauth2.client.id,Client id used to obtain access token with either credential or refresh token.,environment,hdfs
1102,dfs.webhdfs.oauth2.enabled,"If true, enables OAuth2 in WebHDFS",security,hdfs
1103,dfs.webhdfs.oauth2.refresh.url,URL against which to post for obtaining bearer token with either credential or refresh token.,environment,hdfs
1104,ssl.server.keystore.keypassword,Keystore key password for HTTPS SSL configuration,security,hdfs
1105,ssl.server.keystore.location,Keystore location for HTTPS SSL configuration,environment,hdfs
1106,ssl.server.keystore.password,Keystore password for HTTPS SSL configuration,security,hdfs
1107,dfs.balancer.keytab.enabled,Set to true to enable login using a keytab for Kerberized Hadoop.,security,hdfs
1108,dfs.balancer.address,The hostname used for a keytab based Kerberos login. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,environment,hdfs
1109,dfs.balancer.keytab.file,The keytab file used by the Balancer to login as its service principal. The principal name is configured with dfs.balancer.kerberos.principal. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,environment,hdfs
1110,dfs.balancer.kerberos.principal,The Balancer principal. This is typically set to balancer/_HOST@REALM.TLD. The Balancer will substitute _HOST with its own fully qualified hostname at startup. The _HOST placeholder allows using the same configuration setting on different servers. Keytab based login can be enabled with dfs.balancer.keytab.enabled.,others,hdfs
1111,ssl.server.truststore.location,Truststore location for HTTPS SSL configuration,environment,hdfs
1112,ssl.server.truststore.password,Truststore password for HTTPS SSL configuration,security,hdfs
1113,dfs.lock.suppress.warning.interval,Instrumentation reporting long critical sections will suppress consecutive warnings within this interval.,reliability,hdfs
1114,dfs.webhdfs.use.ipc.callq,Enables routing of webhdfs calls through rpc call queue,others,hdfs
1115,httpfs.buffer.size,The size buffer to be used when creating or opening httpfs filesystem IO stream.,performance,hdfs
1116,dfs.datanode.disk.check.min.gap,The minimum gap between two successive checks of the same DataNode volume. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,reliability,hdfs
1117,dfs.datanode.disk.check.timeout,Maximum allowed time for a disk check to complete during DataNode startup. If the check does not complete within this time interval then the disk is declared as failed. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,reliability,hdfs
1118,dfs.use.dfs.network.topology,Enables DFSNetworkTopology to choose nodes for placing replicas.,others,hdfs
1119,dfs.qjm.operations.timeout,Common key to set timeout for related operations in QuorumJournalManager. This setting supports multiple time unit suffixes as described in dfs.heartbeat.interval. If no suffix is specified then milliseconds is assumed.,reliability,hdfs
1120,dfs.reformat.disabled,"Disable reformat of NameNode. If it's value is set to ""true"" and metadata directories already exist then attempt to format NameNode will throw NameNodeFormatException.",reliability,hdfs
1121,AcceptFilter,Configures optimizations for a Protocol's Listener Sockets,performance,httpd
1122,AcceptPathInfo,Resources accept trailing pathname information,debuggability,httpd
1123,AccessFileName,Name of the distributed configuration file,environment,httpd
1124,AddAlt,"Alternate text to display for a file, instead of an icon selected by filename",others,httpd
1125,AddAltByEncoding,Alternate text to display for a file instead of an icon selected by MIME-encoding,others,httpd
1126,AddAltByType,"Alternate text to display for a file, instead of an icon selected by MIME content-type",others,httpd
1127,AddCharset,Maps the given filename extensions to the specified content charset,others,httpd
1128,AddDefaultCharset,Default charset parameter to be added when a response content-type is text/plain or text/html,environment,httpd
1129,AddDescription,Description to display for a file,others,httpd
1130,AddOutputFilterByType,assigns an output filter to a particular media-type,others,httpd
1131,AddType,Maps the given filename extensions onto the specified content type,others,httpd
1132,Alias,Maps URLs to filesystem locations,others,httpd
1133,AliasMatch,Maps URLs to filesystem locations using regular expressions,others,httpd
1134,AllowCONNECT,Ports that are allowed to CONNECT through the proxy,environment,httpd
1135,AllowEncodedSlashes,Determines whether encoded path separators in URLs are allowed to be passed through,security,httpd
1136,AsyncRequestWorkerFactor,Limit concurrent connections per process,reliability,httpd
1137,AuthDigestShmemSize,The amount of shared memory to allocate for keeping track of clients,reliability,httpd
1138,AuthLDAPCharsetConfig,Language to charset conversion configuration file,environment,httpd
1139,AuthnCacheEnable,Enable Authn caching configured anywhere,others,httpd
1140,AuthnCacheSOCache,Select socache backend provider to use,performance,httpd
1141,AuthnzFcgiDefineProvider,Defines a FastCGI application as a provider for authentication and/or authorization,security,httpd
1142,BalancerGrowth,Number of additional Balancers that can be added Post-configuration,performance,httpd
1143,BalancerInherit,Inherit ProxyPassed Balancers/Workers from the main server,others,httpd
1144,BalancerPersist,Attempt to persist changes made by the Balancer Manager across restarts.,others,httpd
1145,BrotliCompressionMaxInputBlock,Maximum input block size,performance,httpd
1146,BrotliCompressionWindow,Brotli sliding compression window size,performance,httpd
1147,BrotliFilterNote,Places the compression ratio in a note for logging,debuggability,httpd
1148,BufferedLogs,Buffer log entries in memory before writing to disk,debuggability,httpd
1149,BufferSize,Maximum size in bytes to buffer by the buffer filter,performance,httpd
1150,CacheDefaultExpire,The default duration to cache a document when no expiry date is specified.,performance,httpd
1151,CacheDetailHeader,Add an X-Cache-Detail header to the response.,others,httpd
1152,CacheDirLength,The number of characters in subdirectory names,others,httpd
1153,CacheDirLevels,The number of levels of subdirectories in the cache.,performance,httpd
1154,CacheDisable,Disable caching of specified URLs,performance,httpd
1155,CacheEnable,Enable caching of specified URLs using a specified storage manager,performance,httpd
1156,CacheFile,Cache a list of file handles at startup time,performance,httpd
1157,CacheHeader,Add an X-Cache header to the response.,others,httpd
1158,CacheIgnoreCacheControl,Ignore request to not serve cached content to client,others,httpd
1159,CacheIgnoreHeaders,Do not store the given HTTP header(s) in the cache.,others,httpd
1160,CacheIgnoreNoLastMod,Ignore the fact that a response has no Last Modified header.,others,httpd
1161,CacheIgnoreQueryString,Ignore query string when caching,performance,httpd
1162,CacheIgnoreURLSessionIdentifiers,Ignore defined session identifiers encoded in the URL when caching,performance,httpd
1163,CacheKeyBaseURL,Override the base URL of reverse proxied cache keys.,environment,httpd
1164,CacheLastModifiedFactor,The factor used to compute an expiry date based on the LastModified date.,performance,httpd
1165,CacheLock,Enable the thundering herd lock.,reliability,httpd
1166,CacheLockMaxAge,Set the maximum possible age of a cache lock.,security,httpd
1167,CacheLockPath,Set the lock path directory.,environment,httpd
1168,CacheMaxExpire,The maximum time in seconds to cache a document,performance,httpd
1169,CacheMaxFileSize,The maximum size (in bytes) of a document to be placed in the cache,performance,httpd
1170,CacheMinExpire,The minimum time in seconds to cache a document,performance,httpd
1171,CacheMinFileSize,The minimum size (in bytes) of a document to be placed in the cache,performance,httpd
1172,CacheNegotiatedDocs,Allows content-negotiated documents to be cached by proxy servers,performance,httpd
1173,CacheQuickHandler,Run the cache from the quick handler.,performance,httpd
1174,CacheReadSize,The minimum size (in bytes) of the document to read and be cached before sending the data downstream,performance,httpd
1175,CacheReadTime,The minimum time (in milliseconds) that should elapse while reading before data is sent downstream,reliability,httpd
1176,CacheRoot,The directory root under which cache files are stored,environment,httpd
1177,CacheSocacheMaxSize,The maximum size (in bytes) of an entry to be placed in the cache,performance,httpd
1178,CacheSocacheReadSize,The minimum size (in bytes) of the document to read and be cached before sending the data downstream,performance,httpd
1179,CacheSocacheReadTime,The minimum time (in milliseconds) that should elapse while reading before data is sent downstream,performance,httpd
1180,CacheStaleOnError,Serve stale content in place of 5xx responses.,reliability,httpd
1181,CacheStoreExpired,Attempt to cache responses that the server reports as expired,others,httpd
1182,CacheStoreNoStore,Attempt to cache requests or responses that have been marked as no-store.,others,httpd
1183,CacheStorePrivate,Attempt to cache responses that the server has marked as private,others,httpd
1184,CGIDScriptTimeout,The length of time to wait for more output from the CGI program,reliability,httpd
1185,CharsetDefault,Charset to translate into,environment,httpd
1186,CharsetOptions,Configures charset translation behavior,others,httpd
1187,CharsetSourceEnc,Source charset of files,environment,httpd
1188,CheckCaseOnly,Limits the action of the speling module to case corrections,reliability,httpd
1189,ContentDigest,Enables the generation of Content-MD5 HTTP Response headers,security,httpd
1190,CookieExpires,Expiry time for the tracking cookie,performance,httpd
1191,CookieName,Name of the tracking cookie,environment,httpd
1192,CookieStyle,Format of the cookie header field,others,httpd
1193,DavGenericLockDB,Location of the DAV lock database,environment,httpd
1194,DavLockDB,Location of the DAV lock database,environment,httpd
1195,DavMinTimeout,Minimum amount of time the server holds a lock on a DAV resource,reliability,httpd
1196,DBDExptime,Keepalive time for idle connections,reliability,httpd
1197,DBDInitSQL,Execute an SQL statement after connecting to a database,environment,httpd
1198,DBDKeep,Maximum sustained number of connections,reliability,httpd
1199,DBDMax,Maximum number of connections,reliability,httpd
1200,DBDMin,Minimum number of connections,reliability,httpd
1201,DBDParams,Parameters for database connection,others,httpd
1202,DBDPersist,Whether to use persistent connections,reliability,httpd
1203,DBDPrepareSQL,Define an SQL prepared statement,environment,httpd
1204,DBDriver,Specify an SQL driver,environment,httpd
1205,DefaultRuntimeDir,Base directory for the server run-time files,environment,httpd
1206,DeflateBufferSize,Fragment size to be compressed at one time by zlib,performance,httpd
1207,DeflateCompressionLevel,How much compression do we apply to the output,others,httpd
1208,DeflateFilterNote,Places the compression ratio in a note for logging,debuggability,httpd
1209,DeflateInflateLimitRequestBody,Maximum size of inflated request bodies,reliability,httpd
1210,DeflateMemLevel,How much memory should be used by zlib for compression,performance,httpd
1211,DeflateWindowSize,Zlib compression window size,performance,httpd
1212,DirectoryCheckHandler,Toggle how this module responds when another handler is configured,others,httpd
1213,DocumentRoot,Directory that forms the main document tree visible from the web,environment,httpd
1214,DumpIOInput,Dump all input data to the error log,debuggability,httpd
1215,DumpIOOutput,Dump all output data to the error log,debuggability,httpd
1216,EnableMMAP,Use memory-mapping to read files during delivery,performance,httpd
1217,EnableSendfile,Use the kernel sendfile support to deliver files to the client,others,httpd
1218,Error,Abort configuration parsing with a custom error message,debuggability,httpd
1219,ErrorDocument,What the server will return to the client in case of an error,debuggability,httpd
1220,ErrorLog,Location where the server will log errors,debuggability,httpd
1221,ErrorLogFormat,Format specification for error log entries,debuggability,httpd
1222,Example,Demonstration directive to illustrate the Apache module API,environment,httpd
1223,ExpiresActive,Enables generation of Expires headers,others,httpd
1224,ExpiresDefault,Default algorithm for calculating expiration time,performance,httpd
1225,ExtendedStatus,Keep track of extended status information for each request,debuggability,httpd
1226,ExtFilterDefine,Define an external filter,security,httpd
1227,FilterProtocol,Deal with correct HTTP protocol handling,others,httpd
1228,FilterTrace,Get debug/diagnostic information from mod_filter,debuggability,httpd
1229,GprofDir,Directory to write gmon.out profiling data to.,environment,httpd
1230,GracefulShutdownTimeout,Specify a timeout after which a gracefully shutdown server will exit.,reliability,httpd
1231,H2EarlyHints,Determine sending of 103 status codes,others,httpd
1232,H2MaxSessionStreams,Maximum number of active streams per HTTP/2 session.,performance,httpd
1233,H2MaxWorkerIdleSeconds,Maximum number of seconds h2 workers remain idle until shut down.,reliability,httpd
1234,H2MaxWorkers,Maximum number of worker threads to use per child process.,performance,httpd
1235,H2MinWorkers,Minimal number of worker threads to use per child process.,performance,httpd
1236,H2ModernTLSOnly,"Require HTTP/2 connections to be ""modern TLS"" only",security,httpd
1237,H2StreamMaxMemSize,Maximum amount of output data buffered per stream.,performance,httpd
1238,H2TLSCoolDownSecs,Configure the number of seconds of idle time on TLS before shrinking writes,reliability,httpd
1239,H2TLSWarmUpSize,Configure the number of bytes on TLS connection before doing max writes,reliability,httpd
1240,HeaderName,Name of the file that will be inserted at the top of the index listing,others,httpd
1241,HeartbeatAddress,Multicast address for heartbeat packets,environment,httpd
1242,HeartbeatListen,multicast address to listen for incoming heartbeat requests,environment,httpd
1243,HeartbeatMaxServers,Specifies the maximum number of servers that will be sending heartbeat requests to this server,reliability,httpd
1244,HostnameLookups,Enables DNS lookups on client IP addresses,others,httpd
1245,IdentityCheckTimeout,Determines the timeout duration for ident requests,reliability,httpd
1246,ImapMenu,Action if no coordinates are given when calling an imagemap,others,httpd
1247,IncludeOptional,Includes other configuration files from within the server configuration files,environment,httpd
1248,IndexHeadInsert,Inserts text in the HEAD section of an index page.,others,httpd
1249,IndexIgnore,Adds to the list of files to hide when listing a directory,others,httpd
1250,IndexIgnoreReset,Empties the list of files to hide when listing a directory,others,httpd
1251,IndexOptions,Various configuration settings for directory indexing,environment,httpd
1252,IndexOrderDefault,Sets the default ordering of the directory index,others,httpd
1253,IndexStyleSheet,Adds a CSS stylesheet to the directory index,environment,httpd
1254,ISAPIAppendLogToErrors,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the error log,debuggability,httpd
1255,ISAPIAppendLogToQuery,Record HSE_APPEND_LOG_PARAMETER requests from ISAPI extensions to the query field,debuggability,httpd
1256,ISAPIReadAheadBuffer,Size of the Read Ahead Buffer sent to ISAPI extensions,performance,httpd
1257,KeepAlive,Enables HTTP persistent connections,reliability,httpd
1258,KeepAliveTimeout,Amount of time the server will wait for subsequent requests on a persistent connection,reliability,httpd
1259,LDAPCacheEntries,Maximum number of entries in the primary LDAP cache,performance,httpd
1260,LDAPCacheTTL,Time that cached items remain valid,reliability,httpd
1261,LDAPConnectionPoolTTL,Discard backend connections that have been sitting in the connection pool too long,reliability,httpd
1262,LDAPConnectionTimeout,Specifies the socket connection timeout in seconds,reliability,httpd
1263,LDAPLibraryDebug,Enable debugging in the LDAP SDK,debuggability,httpd
1264,LDAPOpCacheEntries,Number of entries used to cache LDAP compare operations,performance,httpd
1265,LDAPOpCacheTTL,Time that entries in the operation cache remain valid,performance,httpd
1266,LDAPSharedCacheFile,Sets the shared memory cache file,environment,httpd
1267,LDAPSharedCacheSize,Size in bytes of the shared-memory cache,performance,httpd
1268,LDAPTimeout,"Specifies the timeout for LDAP search and bind operations, in seconds",reliability,httpd
1269,LimitInternalRecursion,Determine maximum number of internal redirects and nested subrequests,reliability,httpd
1270,LimitRequestBody,Restricts the total size of the HTTP request body sent from the client,reliability,httpd
1271,LimitRequestFields,Limits the number of HTTP request header fields that will be accepted from the client,reliability,httpd
1272,LimitRequestFieldSize,Limits the size of the HTTP request header allowed from the client,reliability,httpd
1273,LimitRequestLine,Limit the size of the HTTP request line that will be accepted from the client,reliability,httpd
1274,LimitXMLRequestBody,Limits the size of an XML-based request body,reliability,httpd
1275,Listen,IP addresses and ports that the server listens to,environment,httpd
1276,ListenCoresBucketsRatio,Ratio between the number of CPU cores (online) and the number of listeners' buckets,performance,httpd
1277,LogLevel,Controls the verbosity of the ErrorLog,debuggability,httpd
1278,LuaCodeCache,Configure the compiled code cache.,performance,httpd
1279,LuaInherit,Controls how parent configuration sections are merged into children,others,httpd
1280,MaxConnectionsPerChild,Limit on the number of connections that an individual child server will handle during its life,reliability,httpd
1281,MaxKeepAliveRequests,Number of requests allowed on a persistent connection,reliability,httpd
1282,MaxMemFree,Maximum amount of memory that the main allocator is allowed to hold without calling free(),reliability,httpd
1283,MaxRangeOverlaps,"Number of overlapping ranges (eg: 100-200,150-300) allowed before returning the complete resource",others,httpd
1284,MaxRangeReversals,"Number of range reversals (eg: 100-200,50-70) allowed before returning the complete resource",others,httpd
1285,MaxRanges,Number of ranges allowed before returning the complete resource,others,httpd
1286,MaxRequestWorkers,Maximum number of connections that will be processed simultaneously,performance,httpd
1287,MaxSpareServers,Maximum number of idle child server processes,performance,httpd
1288,MaxSpareThreads,Maximum number of idle threads,performance,httpd
1289,MaxThreads,Set the maximum number of worker threads,performance,httpd
1290,MDBaseServer,Control if base server may be managed or only virtual hosts.,others,httpd
1291,MDCertificateProtocol,The protocol to use with the Certificate Authority.,security,httpd
1292,MDCertificateStatus,Exposes public certificate information in JSON.,security,httpd
1293,MDHttpProxy,Define a proxy for outgoing connections.,environment,httpd
1294,MDMember,Additional hostname for the managed domain.,environment,httpd
1295,MDMustStaple,Control if new certificates carry the OCSP Must Staple flag.,security,httpd
1296,MDNotifyCmd,Run a program when a Managed Domain is ready.,environment,httpd
1297,MDPortMap,Map external to internal ports for domain ownership verification.,others,httpd
1298,MDPrivateKeys,Set type and size of the private keys generated.,security,httpd
1299,MDRenewMode,Controls if certificates shall be renewed.,security,httpd
1300,MDRenewWindow,Control when a certificate will be renewed.,security,httpd
1301,MDStapling,Enable stapling for all or a particular MDomain.,environment,httpd
1302,MDStaplingKeepResponse,Controls when old responses should be removed.,reliability,httpd
1303,MDWarnWindow,Define the time window when you want to be warned about an expiring certificate.,reliability,httpd
1304,MemcacheConnTTL,Keepalive time for idle connections,reliability,httpd
1305,MergeTrailers,Determines whether trailers are merged into headers,others,httpd
1306,MetaDir,Name of the directory to find CERN-style meta information files,environment,httpd
1307,MetaFiles,Activates CERN meta-file processing,security,httpd
1308,MetaSuffix,File name suffix for the file containing CERN-style meta information,others,httpd
1309,MinSpareThreads,Minimum number of idle threads available to handle request spikes,performance,httpd
1310,NameVirtualHost,DEPRECATED: Designates an IP address for name-virtual hosting,environment,httpd
1311,NoProxy,"Hosts, domains, or networks that will be connected to directly",environment,httpd
1312,NWSSLTrustedCerts,List of additional client certificates,security,httpd
1313,Options,Configures what features are available in a particular directory,others,httpd
1314,PidFile,File where the server records the process ID of the daemon,debuggability,httpd
1315,PrivilegesMode,Trade off processing speed and efficiency vs security against malicious privileges-aware code.,security,httpd
1316,Protocol,Protocol for a listening socket,others,httpd
1317,Protocols,Protocols available for a server/virtual host,others,httpd
1318,ProtocolsHonorOrder,Determines if order of Protocols determines precedence during negotiation,security,httpd
1319,ProxyAddHeaders,Add proxy information in X-Forwarded-* headers,security,httpd
1320,ProxyBlock,"Words, hosts, or domains that are banned from being proxied",security,httpd
1321,ProxyDomain,Default domain name for proxied requests,environment,httpd
1322,ProxyFCGIBackendType,Specify the type of backend FastCGI application,environment,httpd
1323,ProxyFCGISetEnvIf,Allow variables sent to FastCGI servers to be fixed up,reliability,httpd
1324,ProxyFtpEscapeWildcards,Whether wildcards in requested filenames are escaped when sent to the FTP server,others,httpd
1325,ProxyHCTPsize,Sets the total server-wide size of the threadpool used for the health check workers,performance,httpd
1326,ProxyHTMLBufSize,Sets the buffer size increment for buffering inline scripts and stylesheets.,performance,httpd
1327,ProxyHTMLEnable,Turns the proxy_html filter on or off.,others,httpd
1328,ProxyHTMLExtended,"Determines whether to fix links in inline scripts, stylesheets, and scripting events.",others,httpd
1329,ProxyHTMLFixups,Fixes for simple HTML errors.,debuggability,httpd
1330,ProxyHTMLLinks,Specify HTML elements that have URL attributes to be rewritten.,others,httpd
1331,ProxyHTMLURLMap,Defines a rule to rewrite HTML links,others,httpd
1332,ProxyIOBufferSize,Determine size of internal data throughput buffer,performance,httpd
1333,ProxyMaxForwards,Maximium number of proxies that a request can be forwarded through,reliability,httpd
1334,ProxyPass,Maps remote servers into the local server URL-space,others,httpd
1335,ProxyPassMatch,Maps remote servers into the local server URL-space using regular expressions,others,httpd
1336,ProxyPassReverse,Adjusts the URL in HTTP response headers sent from a reverse proxied server,others,httpd
1337,ProxyReceiveBufferSize,Network buffer size for proxied HTTP and FTP connections,performance,httpd
1338,ProxySet,Set various Proxy balancer or member parameters,performance,httpd
1339,ProxyStatus,Show Proxy LoadBalancer status in mod_status,debuggability,httpd
1340,ProxyTimeout,Network timeout for proxied requests,reliability,httpd
1341,QualifyRedirectURL,Controls whether the REDIRECT_URL environment variable is fully qualified,others,httpd
1342,ReadmeName,Name of the file that will be inserted at the end of the index listing,others,httpd
1343,ReceiveBufferSize,TCP receive buffer size,performance,httpd
1344,Redirect,Sends an external redirect asking the client to fetch a different URL,others,httpd
1345,RedirectMatch,Sends an external redirect based on a regular expression match of the current URL,others,httpd
1346,RedirectPermanent,Sends an external permanent redirect asking the client to fetch a different URL,others,httpd
1347,RedirectTemp,Sends an external temporary redirect asking the client to fetch a different URL,others,httpd
1348,RedisConnPoolTTL,TTL used for the connection pool with the Redis server(s),reliability,httpd
1349,RedisTimeout,R/W timeout used for the connection with the Redis server(s),reliability,httpd
1350,RemoteIPProxyProtocol,Enable or disable PROXY protocol handling,security,httpd
1351,RemoteIPProxyProtocolExceptions,Disable processing of PROXY header for certain hosts or networks,others,httpd
1352,RewriteMap,Defines a mapping function for key-lookup,others,httpd
1353,RLimitCPU,Limits the CPU consumption of processes launched by Apache httpd children,performance,httpd
1354,RLimitMEM,Limits the memory consumption of processes launched by Apache httpd children,performance,httpd
1355,RLimitNPROC,Limits the number of processes that can be launched by processes launched by Apache httpd children,performance,httpd
1356,ScriptAlias,Maps a URL to a filesystem location and designates the target as a CGI script,others,httpd
1357,ScriptAliasMatch,Maps a URL to a filesystem location using a regular expression and designates the target as a CGI script,others,httpd
1358,ScriptInterpreterSource,Technique for locating the interpreter for CGI scripts,environment,httpd
1359,ScriptLog,Location of the CGI script error logfile,debuggability,httpd
1360,ScriptLogBuffer,Maximum amount of PUT or POST requests that will be recorded in the scriptlog,debuggability,httpd
1361,ScriptLogLength,Size limit of the CGI script logfile,debuggability,httpd
1362,ScriptSock,The filename prefix of the socket to use for communication with the cgi daemon,environment,httpd
1363,SecureListen,Enables SSL encryption for the specified port,security,httpd
1364,SeeRequestTail,"Determine if mod_status displays the first 63 characters of a request or the last 63, assuming the request itself is greater than 63 chars.",others,httpd
1365,SendBufferSize,TCP buffer size,performance,httpd
1366,ServerAdmin,Email address that the server includes in error messages sent to the client,environment,httpd
1367,ServerLimit,Upper limit on configurable number of processes,performance,httpd
1368,ServerName,Hostname and port that the server uses to identify itself,environment,httpd
1369,ServerRoot,Base directory for the server installation,environment,httpd
1370,ServerSignature,Configures the footer on server-generated documents,others,httpd
1371,ServerTokens,Configures the Server HTTP response header,others,httpd
1372,SessionCookieRemove,Control for whether session cookies should be removed from incoming HTTP headers,others,httpd
1373,SessionCryptoCipher,The crypto cipher to be used to encrypt the session,security,httpd
1374,SessionCryptoDriver,The crypto driver to be used to encrypt the session,security,httpd
1375,SessionCryptoPassphrase,The key used to encrypt the session,security,httpd
1376,SessionDBDCookieRemove,Control for whether session ID cookies should be removed from incoming HTTP headers,others,httpd
1377,SessionDBDPerUser,Enable a per user session,others,httpd
1378,SessionEnv,Control whether the contents of the session are written to the HTTP_SESSION environment variable,others,httpd
1379,SessionExpiryUpdateInterval,Define the number of seconds a session's expiry may change without the session being updated,reliability,httpd
1380,SessionInclude,Define URL prefixes for which a session is valid,environment,httpd
1381,SessionMaxAge,Define a maximum age in seconds for a session,reliability,httpd
1382,SetHandler,Forces all matching files to be processed by a handler,performance,httpd
1383,SetInputFilter,Sets the filters that will process client requests and POST input,others,httpd
1384,SSIErrorMsg,Error message displayed when there is an SSI error,debuggability,httpd
1385,SSLCertificateFile,Server PEM-encoded X.509 certificate data file or token identifier,security,httpd
1386,SSLCipherSuite,Cipher Suite available for negotiation in SSL handshake,security,httpd
1387,SSLCompression,Enable compression on the SSL level,security,httpd
1388,SSLFIPS,SSL FIPS mode Switch,security,httpd
1389,SSLOCSPDefaultResponder,Set the default responder URI for OCSP validation,security,httpd
1390,SSLOCSPEnable,Enable OCSP validation of the client certificate chain,security,httpd
1391,SSLOCSPNoverify,skip the OCSP responder certificates verification,security,httpd
1392,SSLOCSPResponderCertificateFile,Set of trusted PEM encoded OCSP responder certificates,security,httpd
1393,SSLOCSPResponderTimeout,Timeout for OCSP queries,reliability,httpd
1394,SSLOCSPResponseMaxAge,Maximum allowable age for OCSP responses,reliability,httpd
1395,SSLOCSPUseRequestNonce,Use a nonce within OCSP queries,others,httpd
1396,SSLOpenSSLConfCmd,Configure OpenSSL parameters through its SSL_CONF API,security,httpd
1397,SSLPassPhraseDialog,Type of pass phrase dialog for encrypted private keys,security,httpd
1398,SSLProxyCipherSuite,Cipher Suite available for negotiation in SSL proxy handshake,security,httpd
1399,SSLProxyEngine,SSL Proxy Engine Operation Switch,security,httpd
1400,SSLProxyMachineCertificateFile,File of concatenated PEM-encoded client certificates and keys to be used by the proxy,security,httpd
1401,SSLProxyProtocol,Configure usable SSL protocol flavors for proxy usage,environment,httpd
1402,SSLProxyVerifyDepth,Maximum depth of CA Certificates in Remote Server Certificate verification,security,httpd
1403,SSLRandomSeed,Pseudo Random Number Generator (PRNG) seeding source,environment,httpd
1404,SSLSessionCache,Type of the global/inter-process SSL Session Cache,performance,httpd
1405,SSLSessionCacheTimeout,Number of seconds before an SSL session expires in the Session Cache,reliability,httpd
1406,SSLSessionTicketKeyFile,Persistent encryption/decryption key for TLS session tickets,security,httpd
1407,SSLSRPUnknownUserSeed,SRP unknown user seed,security,httpd
1408,SSLStaplingErrorCacheTimeout,Number of seconds before expiring invalid responses in the OCSP stapling cache,reliability,httpd
1409,SSLStaplingFakeTryLater,"Synthesize ""tryLater"" responses for failed OCSP stapling queries",reliability,httpd
1410,SSLStaplingForceURL,Override the OCSP responder URI specified in the certificate's AIA extension,security,httpd
1411,SSLStaplingResponderTimeout,Timeout for OCSP stapling queries,reliability,httpd
1412,SSLStaplingResponseMaxAge,Maximum allowable age for OCSP stapling responses,reliability,httpd
1413,SSLStaplingResponseTimeSkew,Maximum allowable time skew for OCSP stapling response validation,reliability,httpd
1414,SSLStaplingStandardCacheTimeout,Number of seconds before expiring responses in the OCSP stapling cache,reliability,httpd
1415,SSLStrictSNIVHostCheck,Whether to allow non-SNI clients to access a name-based virtual host.,security,httpd
1416,SSLVerifyClient,Type of Client Certificate verification,security,httpd
1417,SSLVerifyDepth,Maximum depth of CA Certificates in Client Certificate verification,security,httpd
1418,StartServers,Number of child server processes created at startup,performance,httpd
1419,StartThreads,Number of threads created on startup,performance,httpd
1420,ThreadLimit,Sets the upper limit on the configurable number of threads per child process,reliability,httpd
1421,ThreadsPerChild,Number of threads created by each child process,performance,httpd
1422,TimeOut,Amount of time the server will wait for certain events before failing a request,reliability,httpd
1423,TraceEnable,Determines the behavior on TRACE requests,debuggability,httpd
1424,TransferLog,Specify location of a log file,debuggability,httpd
1425,UseCanonicalName,Configures how the server determines its own name and port,others,httpd
1426,UseCanonicalPhysicalPort,Configures how the server determines its own port,others,httpd
1427,User,The userid under which the server will answer requests,environment,httpd
1428,VirtualDocumentRoot,Dynamically configure the location of the document root for a given virtual host,environment,httpd
1429,VirtualDocumentRootIP,Dynamically configure the location of the document root for a given virtual host,environment,httpd
1430,VirtualScriptAlias,Dynamically configure the location of the CGI directory for a given virtual host,environment,httpd
1431,VirtualScriptAliasIP,Dynamically configure the location of the CGI directory for a given virtual host,environment,httpd
1432,WatchdogInterval,Watchdog interval in seconds,reliability,httpd
1433,CephBroker.MonAddr,Ceph monitor address to connect to,environment,hypertable
1434,CephBroker.Port,Port number on which to listen (read by CephBroker only),environment,hypertable
1435,CephBroker.Workers,"Number of Ceph broker worker threads created, maybe",performance,hypertable
1436,Comm.DispatchDelay,[TESTING ONLY] Delay dispatching of read requests by this number of milliseconds,reliability,hypertable
1437,Comm.UsePoll,Use poll() interface,others,hypertable
1438,FsBroker.DisableFileRemoval,Rename files with .deleted extension instead of removing (for testing),reliability,hypertable
1439,FsBroker.Hdfs.NameNode.Host,"Name of host on which HDFS NameNode is running (NOTE: this property is deprecated, useHdfsBroker.Hadoop.ConfDir instead)",environment,hypertable
1440,FsBroker.Hdfs.NameNode.Port,"Port number on which HDFS NameNode is running(NOTE: this property is deprecated, use HdfsBroker.Hadoop.ConfDir instead)",environment,hypertable
1441,FsBroker.Host,Host on which the FS broker is running (read by clients only),environment,hypertable
1442,FsBroker.Local.DirectIO,Read and write files using direct i/o,others,hypertable
1443,FsBroker.Local.Port,Port number on which to listen (read by LocalBroker only),environment,hypertable
1444,FsBroker.Local.Reactors,Number of local broker communication reactor threads created,performance,hypertable
1445,FsBroker.Local.Root,"Root of file and directory hierarchy for local broker (if relative path, then is relative to the Hypertable data directory root)",environment,hypertable
1446,FsBroker.Local.Workers,Number of local broker worker threads created,performance,hypertable
1447,FsBroker.Port,Port number on which FS broker is listening (read by clients only),environment,hypertable
1448,FsBroker.Timeout,"Length of time, in milliseconds, to wait before timing out FS Broker requests. This takes precedence over Hypertable.Request.Timeout",reliability,hypertable
1449,HdfsBroker.Hadoop.ConfDir,Hadoop configuration directory (e.g. /etc/hadoop/conf or /usr/lib/hadoop/conf),environment,hypertable
1450,HdfsBroker.Port,Port number on which to listen (read by HdfsBroker only),environment,hypertable
1451,HdfsBroker.Reactors,Number of HDFS broker communication reactor threads created,performance,hypertable
1452,HdfsBroker.SyncBlock,Pass SYNC_BLOCK flag to Filesystem.create() when creating files,others,hypertable
1453,HdfsBroker.Workers,Number of HDFS broker worker threads created,performance,hypertable
1454,HdfsBroker.fs.default.name,"Hadoop Filesystem default name, same as fs.default.name property in Hadoop config (e.g. hdfs://localhost:9000)(NOTE: this property is deprecated, use HdfsBroker.Hadoop.ConfDir instead)",others,hypertable
1455,Hyperspace.Checkpoint.Size,Run BerkeleyDB checkpoint when logs exceed this size limit,reliability,hypertable
1456,Hyperspace.Client.Datagram.SendPort,Client UDP send port for keepalive packets,environment,hypertable
1457,Hyperspace.GracePeriod,Hyperspace Grace period (see Chubby paper),reliability,hypertable
1458,Hyperspace.KeepAlive.Interval,Hyperspace Keepalive interval (see Chubby paper),reliability,hypertable
1459,Hyperspace.Lease.Interval,Hyperspace Lease interval (see Chubby paper),reliability,hypertable
1460,Hyperspace.LogGc.Interval,Check for unused BerkeleyDB log files after this much time,reliability,hypertable
1461,Hyperspace.LogGc.MaxUnusedLogs,Number of unused BerkeleyDB to keep around in case of lagging replicas,reliability,hypertable
1462,Hyperspace.Maintenance.Interval,"Hyperspace maintenance interval (checkpoint BerkeleyDB, log cleanup etc)",reliability,hypertable
1463,Hyperspace.Replica.Dir,"Root of hyperspace file and directory heirarchy in local filesystem (if relative path, then is relative to the Hypertable data directory root)",environment,hypertable
1464,Hyperspace.Replica.Host,Hostname of Hyperspace replica,environment,hypertable
1465,Hyperspace.Replica.Port,Port number on which Hyperspace is or should be listening for requests,environment,hypertable
1466,Hyperspace.Replica.Reactors,Number of Hyperspace Master communication reactor threads created,performance,hypertable
1467,Hyperspace.Replica.Replication.Port,Hyperspace replication port,environment,hypertable
1468,Hyperspace.Replica.Replication.Timeout,Hyperspace replication master dies if it doesn't receive replication acknowledgement within this period,reliability,hypertable
1469,Hyperspace.Replica.Workers,Number of Hyperspace Replica worker threads created,performance,hypertable
1470,Hyperspace.Session.Reconnect,Reconnect to Hyperspace on session expiry,reliability,hypertable
1471,Hyperspace.Timeout,Timeout (millisec) for hyperspace requests (preferred to Hypertable.Request.Timeout,reliability,hypertable
1472,Hypertable.Client.Workers,Number of client worker threads created,performance,hypertable
1473,Hypertable.Cluster.Name,Name of cluster used in Monitoring UI and admin notification messages,others,hypertable
1474,Hypertable.CommitLog.Compressor,"Commit log compressor to use (zlib, lzo, quicklz, snappy, bmz, none)",environment,hypertable
1475,Hypertable.CommitLog.RollLimit,Roll commit log (close current fragment file and create a new one) after writing this many bytes,debuggability,hypertable
1476,Hypertable.CommitLog.SkipErrors,Skip over any corruption encountered in the commit log,reliability,hypertable
1477,Hypertable.Connection.Retry.Interval,"Average time, in milliseconds, between connection retry atempts",reliability,hypertable
1478,Hypertable.DataDirectory,Hypertable data directory root,environment,hypertable
1479,Hypertable.Directory,Top-level hypertable directory name,environment,hypertable
1480,Hypertable.Failover.GracePeriod,Master will wait this many milliseconds before trying to recover a RangeServer,reliability,hypertable
1481,Hypertable.Failover.Quorum.Percentage,Percentage of live RangeServers required for failover to proceed,reliability,hypertable
1482,Hypertable.Failover.RecoverInSeries,Carry out USER log recovery for failed servers in series,reliability,hypertable
1483,Hypertable.Failover.Timeout,Timeout (milliseconds) for failover operations,reliability,hypertable
1484,Hypertable.HqlInterpreter.Mutator.NoLogSync,Suspends CommitLog sync operation on updates until command completion,reliability,hypertable
1485,Hypertable.LoadBalancer.BalanceDelay.Initial,Amount of time (seconds) to wait after start up before running balancer,reliability,hypertable
1486,Hypertable.LoadBalancer.BalanceDelay.NewServer,Amount of time (seconds) to wait before running balancer when a new RangeServer is detected,reliability,hypertable
1487,Hypertable.LoadBalancer.Crontab,Crontab entry to control when load balancer is run,performance,hypertable
1488,Hypertable.LoadBalancer.Enable,Enable automatic load balancing,performance,hypertable
1489,Hypertable.LoadBalancer.LoadavgThreshold,Servers with loadavg above this much above the mean will be considered by the load balancer to be overloaded,performance,hypertable
1490,Hypertable.LoadMetrics.Interval,"Period of time, in seconds, between writing metrics to sys/RS_METRICS",reliability,hypertable
1491,Hypertable.LocationCache.MaxEntries,Size of range location cache in number of entries,performance,hypertable
1492,Hypertable.LogFlushMethod.Meta,"This is a string property that can take either the value FLUSH or SYNC. It controls the flush method for writes to the METADATA commit log. When running Hypertable on top of HDFS, a value of FLUSH causes hflush() to be used and a value of SYNC causes hsync() to be used.",debuggability,hypertable
1493,Hypertable.LogFlushMethod.User,"This is a string property that can take either the value FLUSH or SYNC. It controls the flush method for writes to the user data commit log. When running Hypertable on top of HDFS, a value of FLUSH causes hflush() to be used and a value of SYNC causes hsync() to be used.",debuggability,hypertable
1494,Hypertable.Logging.Level,Set system wide logging level (default: info),debuggability,hypertable
1495,Hypertable.Master.DiskThreshold.Percentage,Stop assigning ranges to RangeServers if disk usage is above this threshold,reliability,hypertable
1496,Hypertable.Master.Gc.Interval,Garbage collection interval in milliseconds by Master,reliability,hypertable
1497,Hypertable.Master.Host,Host on which Hypertable Master is running,environment,hypertable
1498,Hypertable.Master.Locations.IncludeMasterHash,Includes master hash (host:port) in RangeServer location id,others,hypertable
1499,Hypertable.Master.NotificationInterval,Notification interval (in seconds) of abnormal state,reliability,hypertable
1500,Hypertable.Master.Port,Port number on which Hypertable Master is or should be listening,environment,hypertable
1501,Hypertable.Master.Reactors,Number of Hypertable Master communication reactor threads created,performance,hypertable
1502,Hypertable.Master.Split.SoftLimitEnabled,Enable aggressive splitting of tables with little data to spread out ranges,performance,hypertable
1503,Hypertable.Master.Workers,Number of Hypertable Master worker threads created,performance,hypertable
1504,Hypertable.Metadata.Replication,Replication factor for commit log files,reliability,hypertable
1505,Hypertable.MetaLog.HistorySize,Number of old MetaLog files to retain for historical purposes,debuggability,hypertable
1506,Hypertable.MetaLog.MaxFileSize,Maximum size a MetaLog file can grow before it is compacted,performance,hypertable
1507,Hypertable.MetaLog.SkipErrors,Skipping errors instead of throwing exceptions on metalog errors,reliability,hypertable
1508,Hypertable.Metrics.Ganglia.Disable,Disable publishing of metrics to Ganglia,others,hypertable
1509,Hypertable.Metrics.Ganglia.Port,UDP Port on which Hypertable gmond python extension module listens for metrics,environment,hypertable
1510,Hypertable.Monitoring.Disable,Disables the generation of monitoring statistics,reliability,hypertable
1511,Hypertable.Monitoring.Interval,Monitoring statistics gathering interval (in milliseconds),reliability,hypertable
1512,Hypertable.Mutator.FlushDelay,Number of milliseconds to wait prior to flushing scatter buffers (for testing),reliability,hypertable
1513,Hypertable.Mutator.ScatterBuffer.FlushLimit.Aggregate,Amount of updates (bytes) accumulated for all servers to trigger a scatter buffer flush,performance,hypertable
1514,Hypertable.Mutator.ScatterBuffer.FlushLimit.PerServer,Amount of updates (bytes) accumulated for a single server to trigger a scatter buffer flush,reliability,hypertable
1515,Hypertable.Network.Interface,Use this interface for network communication,environment,hypertable
1516,Hypertable.RangeLocator.MaxErrorQueueLength,Maximum numbers of errors to be stored,reliability,hypertable
1517,Hypertable.RangeLocator.MetadataReadaheadCount,Number of rows that the RangeLocator fetches from the METADATA,others,hypertable
1518,Hypertable.RangeLocator.MetadataRetryInterval,Retry interval when connecting to a RangeServer to fetch metadata,reliability,hypertable
1519,Hypertable.RangeLocator.RootMetadataRetryInterval,Retry interval when connecting to the Root RangeServer,reliability,hypertable
1520,Hypertable.RangeServer.AccessGroup.CellCache.PageSize,Page size for CellCache pool allocator,performance,hypertable
1521,Hypertable.RangeServer.AccessGroup.CellCache.ScannerCacheSize,CellCache scanner cache size,performance,hypertable
1522,Hypertable.RangeServer.AccessGroup.GarbageThreshold.Percentage,Perform major compaction when garbage accounts for this percentage of the data,performance,hypertable
1523,Hypertable.RangeServer.AccessGroup.MaxMemory,Maximum bytes consumed by an Access Group,reliability,hypertable
1524,Hypertable.RangeServer.AccessGroup.ShadowCache,Enable CellStore shadow caching,others,hypertable
1525,Hypertable.RangeServer.BlockCache.Compressed,Controls whether or not block cache stores compressed blocks,performance,hypertable
1526,Hypertable.RangeServer.BlockCache.MaxMemory,Maximum (target) size of block cache,performance,hypertable
1527,Hypertable.RangeServer.BlockCache.MinMemory,Minimum size of block cache,performance,hypertable
1528,Hypertable.RangeServer.CellStore.DefaultBlockSize,Default block size for cell stores,performance,hypertable
1529,Hypertable.RangeServer.CellStore.DefaultBloomFilter,Default bloom filter for cell stores,performance,hypertable
1530,Hypertable.RangeServer.CellStore.DefaultCompressor,Default compressor for cell stores,others,hypertable
1531,Hypertable.RangeServer.CellStore.Merge.RunLengthThreshold,Trigger a merge if an adjacent run of merge candidate CellStores exceeds this length,reliability,hypertable
1532,Hypertable.RangeServer.CellStore.SkipBad,"Skip over corrupt cell stores. NOTE: This property should only be used in certain disaster recovery scenarios, such as when the filesystem has become corrupt. This property leads to leaked files and hides the extent of the data loss. It is better to manually remove corrupt files and use the Hypertable.RangeServer.CellStore.SkipNotFound to skip over them.",reliability,hypertable
1533,Hypertable.RangeServer.CellStore.SkipNotFound,Skip over cell stores that are non-existent,reliability,hypertable
1534,Hypertable.RangeServer.CellStore.TargetSize.Minimum,Merging compaction target CellStore size during normal activity period,reliability,hypertable
1535,Hypertable.RangeServer.CellStore.TargetSize.Maximum,Merging compaction target CellStore size during low activity period,reliability,hypertable
1536,Hypertable.RangeServer.ClockSkew.Max,Maximum amount of clock skew (microseconds) the system will tolerate,reliability,hypertable
1537,Hypertable.RangeServer.CommitInterval,Default minimum group commit interval in milliseconds,reliability,hypertable
1538,Hypertable.RangeServer.CommitLog.Compressor,"Commit log compressor to use (zlib, lzo, quicklz, snappy, bmz, none)",environment,hypertable
1539,Hypertable.RangeServer.CommitLog.DfsBroker.Host,Host of FS Broker to use for Commit Log,environment,hypertable
1540,Hypertable.RangeServer.CommitLog.DfsBroker.Port,Port of FS Broker to use for Commit Log,environment,hypertable
1541,Hypertable.RangeServer.CommitLog.FragmentRemoval.RangeReferenceRequired,Only remove linked log fragments if they're part of a transfer log referenced by a range,others,hypertable
1542,Hypertable.RangeServer.CommitLog.PruneThreshold.Max,Upper threshold for amount of outstanding commit log before pruning,reliability,hypertable
1543,Hypertable.RangeServer.CommitLog.PruneThreshold.Max.MemoryPercentage,Upper threshold in terms of % RAM for amount of outstanding commit log before pruning,reliability,hypertable
1544,Hypertable.RangeServer.CommitLog.PruneThreshold.Min,Lower threshold for amount of outstanding commit log before pruning,reliability,hypertable
1545,Hypertable.RangeServer.ControlFile.CheckInterval,Minimum time interval (milliseconds) to check for control files in run/ directory,reliability,hypertable
1546,Hypertable.RangeServer.Data.DefaultReplication,Default replication for data,reliability,hypertable
1547,Hypertable.RangeServer.Failover.FlushLimit.Aggregate,Amount of updates (bytes) accumulated for all range to trigger a replay buffer flush,reliability,hypertable
1548,Hypertable.RangeServer.Failover.FlushLimit.PerRange,Amount of updates (bytes) accumulated for a single range to trigger a replay buffer flu\ sh,reliability,hypertable
1549,Hypertable.RangeServer.IgnoreClockSkewErrors,Ignore clock skew errors,reliability,hypertable
1550,Hypertable.RangeServer.LoadSystemTablesOnly,Instructs the RangeServer to only load system tables (for debugging),debuggability,hypertable
1551,Hypertable.RangeServer.LowMemoryLimit.Percentage,Amount of memory to free in low memory condition as percentage of RangeServer memory limit,performance,hypertable
1552,Hypertable.RangeServer.LowActivityPeriod,Crontab-style entry specifying the low activity period. This property can be specified multiple times to specify multiple low activity periods. The RangeServer performs more aggressive maintenance during this period.,others,hypertable
1553,Hypertable.RangeServer.Maintenance.Interval,Maintenance scheduling interval in milliseconds,reliability,hypertable
1554,Hypertable.RangeServer.Maintenance.LowMemoryPrioritization,Use low memory prioritization algorithm for freeing memory in low memory mode,performance,hypertable
1555,Hypertable.RangeServer.Maintenance.MaxAppQueuePause,"Each time application queue is paused, keep it paused for no more than this many milliseconds",reliability,hypertable
1556,Hypertable.RangeServer.Maintenance.MergesPerInterval,Limit on number of merging tasks to create per maintenance interval,reliability,hypertable
1557,Hypertable.RangeServer.Maintenance.MergingCompaction.Delay,Millisecond delay before scheduling merging compactions in non-low memory mode,reliability,hypertable
1558,Hypertable.RangeServer.Maintenance.MoveCompactionsPerInterval,Limit on number of major compactions due to move per maintenance interval,reliability,hypertable
1559,Hypertable.RangeServer.Maintenance.InitializationPerInterval,Limit on number of initialization tasks to create per maintenance interval,reliability,hypertable
1560,Hypertable.RangeServer.MaintenanceThreads,"Number of maintenance threads. Default is max(1.5*drive-count, number-of-cores).",performance,hypertable
1561,Hypertable.RangeServer.MemoryLimit,Absolute RangeServer memory limit,performance,hypertable
1562,Hypertable.RangeServer.MemoryLimit.EnsureUnused,Amount of unused physical memory,performance,hypertable
1563,Hypertable.RangeServer.MemoryLimit.EnsureUnused.Percentage,Amount of unused physical memory specified as percentage of physical RAM,performance,hypertable
1564,Hypertable.RangeServer.MemoryLimit.Percentage,RangeServer memory limit specified as percentage of physical RAM,performance,hypertable
1565,Hypertable.RangeServer.Monitoring.DataDirectories,Comma-separated list of directory mount points of disk volumes to monitor,environment,hypertable
1566,Hypertable.RangeServer.Port,Port number on which range servers are or should be listening,environment,hypertable
1567,Hypertable.RangeServer.ProxyName,Use this value for the proxy name (if set) instead of reading from run dir.,environment,hypertable
1568,Hypertable.RangeServer.QueryCache.EnableMutexStatistics,Enables waiter statistics on query cache mutex,performance,hypertable
1569,Hypertable.RangeServer.QueryCache.MaxMemory,Maximum size of query cache,performance,hypertable
1570,Hypertable.RangeServer.Range.MaximumSize,Maximum size of a range in bytes before updates get throttled,reliability,hypertable
1571,Hypertable.RangeServer.Range.MetadataSplitSize,Size of METADATA range in bytes before splitting (for testing),reliability,hypertable
1572,Hypertable.RangeServer.Range.RowSize.Unlimited,Marks range active and unsplittable upon encountering row overflow condition. Can cause ranges to grow extremely large. Use with caution!,reliability,hypertable
1573,Hypertable.RangeServer.Range.SplitOff,Portion of range to split off (high or low),others,hypertable
1574,Hypertable.RangeServer.Range.SplitSize,Size of range in bytes before splitting,others,hypertable
1575,Hypertable.RangeServer.Range.RowSize.Unlimited,Marks range active and unsplittable upon encountering row overflow condition. Can cause ranges to grow extremely large. Use with caution!,reliability,hypertable
1576,Hypertable.RangeServer.Reactors,Number of Range Server communication reactor threads created,performance,hypertable
1577,Hypertable.RangeServer.ReadyStatus,"Status code indicating RangeServer is ready for operation. By setting this property to OK the RangeServer startup script will wait until all deferred initialization (loading of CellStores) is complete, before returning.",environment,hypertable
1578,Hypertable.RangeServer.Scanner.BufferSize,Size of transfer buffer for scan results,performance,hypertable
1579,Hypertable.RangeServer.Scanner.Ttl,Number of milliseconds of inactivity before destroying scanners,reliability,hypertable
1580,Hypertable.RangeServer.Testing.MaintenanceNeeded.PauseInterval,"TESTING: After update, if range needs maintenance, pause for this number of milliseconds",reliability,hypertable
1581,Hypertable.RangeServer.Timer.Interval,"Timer interval in milliseconds (reaping scanners, purging commit logs, etc.)",reliability,hypertable
1582,Hypertable.RangeServer.UpdateCoalesceLimit,Amount of update data to coalesce into single commit log sync,reliability,hypertable
1583,Hypertable.RangeServer.UpdateDelay,Number of milliseconds to wait before carrying out an update (TESTING),reliability,hypertable
1584,Hypertable.RangeServer.Workers,Number of Range Server worker threads created,performance,hypertable
1585,Hypertable.Request.Timeout,"Length of time, in milliseconds, before timing out requests (system wide)",reliability,hypertable
1586,Hypertable.Scanner.QueueSize,Size of Scanner ScanBlock queue,performance,hypertable
1587,Hypertable.Silent,Disable verbose output (system wide),others,hypertable
1588,Hypertable.Verbose,Enable verbose output (system wide),others,hypertable
1589,Qfs.Broker.Reactors,Number of QFS broker reactor threads,performance,hypertable
1590,Qfs.Broker.Workers,Number of worker threads for QFS broker,performance,hypertable
1591,Qfs.MetaServer.Name,Hostname of QFS meta server,environment,hypertable
1592,Qfs.MetaServer.Port,Port number for QFS meta server,environment,hypertable
1593,ThriftBroker.API.Logging,Enable or disable Thrift API logging,debuggability,hypertable
1594,ThriftBroker.Future.Capacity,Capacity of result queue (in bytes) for Future objects,performance,hypertable
1595,ThriftBroker.Hyperspace.Session.Reconnect,ThriftBroker will reconnect to Hyperspace on session expiry,reliability,hypertable
1596,ThriftBroker.Mutator.FlushInterval,Maximum flush interval in milliseconds,reliability,hypertable
1597,ThriftBroker.NextThreshold,Total size threshold for (size of cell data) for thrift broker next calls,performance,hypertable
1598,ThriftBroker.Port,Port number for thrift broker,environment,hypertable
1599,ThriftBroker.SlowQueryLog.Enable,Enable slow query logging,debuggability,hypertable
1600,ThriftBroker.SlowQueryLog.LatencyThreshold,Latency threshold (ms) above which a query is considered slow,performance,hypertable
1601,ThriftBroker.Timeout,Timeout (ms) for thrift broker,reliability,hypertable
1602,ThriftBroker.Workers,Number of worker threads for thrift broker,performance,hypertable
1603,run_external_periodic_tasks,Some periodic tasks can be run in a separate process. Should we run them here?,others,ironic
1604,backdoor_port,"Enable eventlet backdoor.  Acceptable values are 0, <port>, and <start>:<end>, where 0 results in listening on a random tcp port number; <port> results in listening on the specified port  number (and not enabling backdoor if that port is in use); and  <start>:<end> results in listening on the smallest unused  port number within the specified range of port numbers.  The chosen port is displayed in the service's log file.",environment,ironic
1605,backdoor_socket,"Enable eventlet backdoor, using the provided path as a unix socket  that can receive connections. This option is mutually exclusive with  'backdoor_port' in that only one should be provided. If both are  provided then the existence of this option overrides the usage of that  option.",environment,ironic
1606,log_options,Enables or disables logging values of all registered options when starting a service (at DEBUG level).,debuggability,ironic
1607,graceful_shutdown_timeout,Specify a timeout after which a gracefully shutdown server will exit. Zero value means endless wait.,reliability,ironic
1608,rpc_conn_pool_size,Size of RPC connection pool.,performance,ironic
1609,conn_pool_min_size,The pool size limit for connections expiration policy,performance,ironic
1610,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,ironic
1611,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,performance,ironic
1612,rpc_response_timeout,Seconds to wait for a response from a call.,reliability,ironic
1613,transport_url,"The network address and optional user credentials for connecting to  the messaging backend, in URL format. The expected format is:",environment,ironic
1614,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,others,ironic
1615,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",debuggability,ironic
1616,log_config_append,"The name of a logging configuration file. This file is appended to  any existing logging configuration files. For details about logging  configuration files, see the Python logging module documentation. Note  that when logging configuration files are used then all logging  configuration is set in the configuration file and other logging  configuration options are ignored (for example, log-date-format).",debuggability,ironic
1617,log_date_format,Defines the format string for %(asctime)s in log records. Default:  the value above . This option is ignored if log_config_append is set.,debuggability,ironic
1618,log_file,"(Optional) Name of log file to send logging output to. If no default  is set, logging will go to stderr as defined by use_stderr. This option  is ignored if log_config_append is set.",debuggability,ironic
1619,log_dir,(Optional) The base directory used for relative log_file  paths. This option is ignored if log_config_append is set.,environment,ironic
1620,watch_log_file,Uses logging handler designed to watch file system. When log file is  moved or removed this handler will open a new log file with specified  path instantaneously. It makes sense only if log_file option is  specified and Linux platform is used. This option is ignored if  log_config_append is set.,debuggability,ironic
1621,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if  log_config_append is set.,debuggability,ironic
1622,use_journal,Enable journald for logging. If running in a systemd environment you  may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log  messages.This option is ignored if log_config_append is set.,debuggability,ironic
1623,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,debuggability,ironic
1624,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,debuggability,ironic
1625,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,debuggability,ironic
1626,use_eventlog,Log output to Windows Event Log.,debuggability,ironic
1627,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,reliability,ironic
1628,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next  rotation.,debuggability,ironic
1629,max_logfile_count,Maximum number of rotated log files.,debuggability,ironic
1630,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,debuggability,ironic
1631,log_rotation_type,Log rotation type.,debuggability,ironic
1632,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,debuggability,ironic
1633,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,debuggability,ironic
1634,logging_debug_format_suffix,Additional data to append to log message when logging level for the  message is DEBUG. Used by oslo_log.formatters.ContextFormatter,debuggability,ironic
1635,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,debuggability,ironic
1636,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in  logging_context_format_string. Used by  oslo_log.formatters.ContextFormatter,debuggability,ironic
1637,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,debuggability,ironic
1638,publish_errors,Enables or disables publication of error events.,debuggability,ironic
1639,instance_format,The format for an instance that is passed with the log message.,others,ironic
1640,instance_uuid_format,The format for an instance UUID that is passed with the log message.,others,ironic
1641,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",reliability,ironic
1642,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,reliability,ironic
1643,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to  rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",debuggability,ironic
1644,fatal_deprecations,Enables or disables fatal status of deprecations.,others,ironic
1645,manage_agent_boot,"Whether Ironic will manage booting of the agent ramdisk. If set to  False, you will need to configure your mechanism to allow booting the  agent ramdisk.",others,ironic
1646,memory_consumed_by_agent,The memory size in MiB consumed by agent when it is booted on a bare  metal node. This is used for checking if the image can be downloaded and deployed on the bare metal node after booting agent ramdisk. This may  be set according to the memory consumed by the agent ramdisk image.,performance,ironic
1647,stream_raw_images,"Whether the agent ramdisk should stream raw images directly onto the  disk or not. By streaming raw images directly onto the disk the agent  ramdisk will not spend time copying the image to a tmpfs partition  (therefore consuming less memory) prior to writing it to the disk.  Unless the disk where the image will be copied to is really slow, this  option should be set to True. Defaults to True.",performance,ironic
1648,post_deploy_get_power_state_retries,Number of times to retry getting power state to check if bare metal node has been powered off after a soft power off.,reliability,ironic
1649,post_deploy_get_power_state_retry_interval,Amount of time (in seconds) to wait between polling power state after trigger soft poweroff.,reliability,ironic
1650,agent_api_version,API version to use for communicating with the ramdisk agent.,environment,ironic
1651,deploy_logs_collect,"Whether Ironic should collect the deployment logs on deployment failure (on_failure), always or never.",debuggability,ironic
1652,deploy_logs_storage_backend,The name of the storage backend where the logs will be stored.,debuggability,ironic
1653,deploy_logs_local_path,"The path to the directory where the logs should be stored, used when the deploy_logs_storage_backend is configured to 'local'.",environment,ironic
1654,deploy_logs_swift_container,"The name of the Swift container to store the logs, used when the deploy_logs_storage_backend is configured to 'swift'.",debuggability,ironic
1655,deploy_logs_swift_days_to_expire,"Number of days before a log object is marked as expired in Swift. If  None, the logs will be kept forever or until manually deleted. Used when the deploy_logs_storage_backend is configured to 'swift'.",debuggability,ironic
1656,image_download_source,Specifies whether direct deploy interface should try to use the image source directly or if ironic should cache the image on the conductor  and serve it from ironic's own http server. This option takes effect  only when instance image is provided from the Image service.,others,ironic
1657,command_timeout,Timeout (in seconds) for IPA commands.,reliability,ironic
1658,max_command_attempts,This is the maximum number of attempts that will be done for IPA commands that fails due to network problems.,reliability,ironic
1659,neutron_agent_poll_interval,The number of seconds Neutron agent will wait between polling for  device changes. This value should be the same as  CONF.AGENT.polling_interval in Neutron configuration.,reliability,ironic
1660,neutron_agent_max_attempts,Max number of attempts to validate a Neutron agent status before raising network error for a dead agent.,reliability,ironic
1661,neutron_agent_status_retry_interval,Wait time in seconds between attempts for validating Neutron agent status.,reliability,ironic
1662,ansible_extra_args,Extra arguments to pass on every invocation of Ansible.,others,ironic
1663,verbosity,Set ansible verbosity level requested when invoking  'ansible-playbook' command. 4 includes detailed SSH session logging.  Default is 4 when global debug is enabled and 0 otherwise.,debuggability,ironic
1664,ansible_playbook_script,Path to 'ansible-playbook' script. Default will search the $PATH  configured for user running ironic-conductor process. Provide the full  path when ansible-playbook is not in $PATH or installed in not default  location.,environment,ironic
1665,playbooks_path,"Path to directory with playbooks, roles and local inventory.",environment,ironic
1666,config_file_path,"Path to ansible configuration file. If set to empty, system default will be used.",environment,ironic
1667,post_deploy_get_power_state_retries,Number of times to retry getting power state to check if bare metal  node has been powered off after a soft power off. Value of 0 means do  not retry on failure.,reliability,ironic
1668,post_deploy_get_power_state_retry_interval,Amount of time (in seconds) to wait between polling power state after trigger soft poweroff.,reliability,ironic
1669,extra_memory,Extra amount of memory in MiB expected to be consumed by  Ansible-related processes on the node. Affects decision whether image  will fit into RAM.,performance,ironic
1670,image_store_insecure,Skip verifying SSL connections to the image store when downloading  the image. Setting it to 'True' is only recommended for testing  environments that use self-signed certificates.,security,ironic
1671,image_store_cafile,"Specific CA bundle to use for validating SSL connections to the image store. If not specified, CA available in the ramdisk will be used. Is  not used by default playbooks included with the driver. Suitable for  environments that use self-signed certificates.",security,ironic
1672,image_store_certfile,Client cert to use for SSL connections to image store. Is not used by default playbooks included with the driver.,security,ironic
1673,image_store_keyfile,Client key to use for SSL connections to image store. Is not used by default playbooks included with the driver.,security,ironic
1674,default_username,Name of the user to use for Ansible when connecting to the ramdisk  over SSH. It may be overridden by per-node 'ansible_username' option in  node's 'driver_info' field.,environment,ironic
1675,default_key_file,Absolute path to the private SSH key file to use by Ansible by  default when connecting to the ramdisk over SSH. Default is to use  default SSH keys configured for the user running the ironic-conductor  service. Private keys with password must be pre-loaded into 'ssh-agent'. It may be overridden by per-node 'ansible_key_file' option in node's  'driver_info' field.,environment,ironic
1676,default_deploy_playbook,Path (relative to $playbooks_path or absolute) to the default  playbook used for deployment. It may be overridden by per-node  'ansible_deploy_playbook' option in node's 'driver_info' field.,environment,ironic
1677,default_shutdown_playbook,Path (relative to $playbooks_path or absolute) to the default  playbook used for graceful in-band shutdown of the node. It may be  overridden by per-node 'ansible_shutdown_playbook' option in node's  'driver_info' field.,environment,ironic
1678,default_clean_playbook,Path (relative to $playbooks_path or absolute) to the default  playbook used for node cleaning. It may be overridden by per-node  'ansible_clean_playbook' option in node's 'driver_info' field.,environment,ironic
1679,default_clean_steps_config,Path (relative to $playbooks_path or absolute) to the default  auxiliary cleaning steps file used during the node cleaning. It may be  overridden by per-node 'ansible_clean_steps_config' option in node's  'driver_info' field.,environment,ironic
1680,default_python_interpreter,"Absolute path to the python interpreter on the managed machines. It  may be overridden by per-node 'ansible_python_interpreter' option in  node's 'driver_info' field. By default, ansible uses /usr/bin/python",environment,ironic
1681,host_ip,The IP address or hostname on which ironic-api listens.,environment,ironic
1682,port,The TCP port on which ironic-api listens.,environment,ironic
1683,max_limit,The maximum number of items returned in a single response from a collection resource.,performance,ironic
1684,public_endpoint,"Public URL to use when building the links to the API resources (for example, 'https://ironic.rocks:6384'). If None the links will be built using the request's host URL. If the  API is operating behind a proxy, you will want to change this to  represent the proxy's URL. Defaults to None.",environment,ironic
1685,api_workers,"Number of workers for OpenStack Ironic API service. The default is  equal to the number of CPUs available if that can be determined, else a  default worker count of 1 is returned.",performance,ironic
1686,enable_ssl_api,"Enable the integrated stand-alone API to service requests via HTTPS  instead of HTTP. If there is a front-end service performing HTTPS  offloading from the service, this option should be False; note, you will want to change public API endpoint to represent SSL termination URL  with 'public_endpoint' option.",security,ironic
1687,restrict_lookup,Whether to restrict the lookup API to only nodes in certain states.,security,ironic
1688,ramdisk_heartbeat_timeout,Maximum interval (in seconds) for agent heartbeats.,reliability,ironic
1689,enabled,Enable auditing of API requests (for ironic-api service).,security,ironic
1690,audit_map_file,Path to audit map file for ironic-api service. Used only when API audit is enabled.,security,ironic
1691,ignore_req_list,"Comma separated list of Ironic REST API HTTP methods to be ignored  during audit logging. For example: auditing will not be done on any GET  or POST requests if this is set to 'GET,POST'. It is used only when API  audit is enabled.",reliability,ironic
1692,max_retry,Number of times a power operation needs to be retried,reliability,ironic
1693,action_interval,Amount of time in seconds to wait in between power operations,reliability,ironic
1694,action_retries,Number of retries in the case of a failed action (currently only used when detaching volumes).,reliability,ironic
1695,action_retry_interval,Retry interval in seconds in the case of a failed action (only specific actions are retried).,reliability,ironic
1696,auth_url,Authentication URL,security,ironic
1697,auth_type,Authentication type to load,security,ironic
1698,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
1699,certfile,PEM encoded client certificate cert file,security,ironic
1700,collect_timing,Collect per-API call timing information.,debuggability,ironic
1701,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1702,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1703,domain_id,Domain ID to scope to,environment,ironic
1704,domain_name,Domain name to scope to,environment,ironic
1705,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
1706,insecure,Verify HTTPS connections.,security,ironic
1707,keyfile,PEM encoded client certificate key file,environment,ironic
1708,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
1709,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
1710,password,User's password,security,ironic
1711,project_domain_id,Domain ID containing project,environment,ironic
1712,project_domain_name,Domain name containing project,environment,ironic
1713,project_id,Project ID to scope to,environment,ironic
1714,project_name,Project name to scope to,environment,ironic
1715,region_name,The default region_name for endpoint URL discovery.,environment,ironic
1716,retries,Client retries in the case of a failed request connection.,reliability,ironic
1717,service_name,The default service_name for endpoint URL discovery.,environment,ironic
1718,service_type,The default service_type for endpoint URL discovery.,others,ironic
1719,split_loggers,Log requests to multiple loggers.,debuggability,ironic
1720,system_scope,Scope for system operations,environment,ironic
1721,tenant_id,Tenant ID,environment,ironic
1722,tenant_name,Tenant Name,environment,ironic
1723,timeout,Timeout value for http requests,reliability,ironic
1724,trust_id,Trust ID,security,ironic
1725,url,"URL for connecting to cinder. If set, the value must start with either http:// or https://.",environment,ironic
1726,user_domain_id,User's domain id,environment,ironic
1727,user_domain_name,User's domain name,environment,ironic
1728,user_id,User id,environment,ironic
1729,username,Username,environment,ironic
1730,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
1731,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
1732,max_retry,Number of times a power operation needs to be retried,reliability,ironic
1733,action_interval,Amount of time in seconds to wait in between power operations,reliability,ironic
1734,workers_pool_size,"The size of the workers greenthread pool. Note that 2 threads will be reserved by the conductor itself for handling heart beats and periodic  tasks. On top of that, sync_power_state_workers will take up to 7 green threads with the default value of 8.",performance,ironic
1735,heartbeat_interval,Seconds between conductor heart beats.,reliability,ironic
1736,api_url,"URL of Ironic API service. If not set ironic can get the current  value from the keystone service catalog. If set, the value must start  with either http:// or https://.",environment,ironic
1737,heartbeat_timeout,Maximum time (in seconds) since the last check-in of a conductor. A  conductor is considered inactive when this time has been exceeded.,reliability,ironic
1738,sync_power_state_interval,"Interval between syncing the node power state to the database, in seconds. Set to 0 to disable syncing.",reliability,ironic
1739,check_provision_state_interval,"Interval between checks of provision timeouts, in seconds. Set to 0 to disable checks.",reliability,ironic
1740,check_rescue_state_interval,Interval (seconds) between checks of rescue timeouts.,reliability,ironic
1741,check_allocations_interval,"Interval between checks of orphaned allocations, in seconds. Set to 0 to disable checks.",reliability,ironic
1742,deploy_callback_timeout,Timeout (seconds) to wait for a callback from a deploy ramdisk. Set to 0 to disable timeout.,reliability,ironic
1743,force_power_state_during_sync,"During sync_power_state, should the hardware power state be set to  the state recorded in the database (True) or should the database be  updated based on the hardware state (False).",others,ironic
1744,power_state_sync_max_retries,"During sync_power_state failures, limit the number of times Ironic  should try syncing the hardware node power state with the node power  state in DB",reliability,ironic
1745,sync_power_state_workers,The maximum number of worker threads that can be started simultaneously to sync nodes power states from the periodic task.,performance,ironic
1746,periodic_max_workers,Maximum number of worker threads that can be started simultaneously  by a periodic task. Should be less than RPC thread pool size.,performance,ironic
1747,node_locked_retry_attempts,Number of attempts to grab a node lock.,reliability,ironic
1748,node_locked_retry_interval,Seconds to sleep between node lock attempts.,reliability,ironic
1749,send_sensor_data,Enable sending sensor data message via the notification bus,others,ironic
1750,send_sensor_data_interval,Seconds between conductor sending sensor data message to ceilometer via the notification bus.,reliability,ironic
1751,send_sensor_data_workers,The maximum number of workers that can be started simultaneously for send data from sensors periodic task.,performance,ironic
1752,send_sensor_data_wait_timeout,The time in seconds to wait for send sensors data periodic task to be finished before allowing periodic call to happen again. Should be less  than send_sensor_data_interval value.,reliability,ironic
1753,send_sensor_data_types,"List of comma separated meter types which need to be sent to  Ceilometer. The default value, 'ALL', is a special value meaning send  all the sensor data.",reliability,ironic
1754,sync_local_state_interval,"When conductors join or leave the cluster, existing conductors may  need to update any persistent local state as nodes are moved around the  cluster. This option controls how often, in seconds, each conductor will check for nodes that it should 'take over'. Set it to 0 (or a negative  value) to disable the check entirely.",reliability,ironic
1755,configdrive_swift_container,Name of the Swift container to store config drive data. Used when configdrive_use_object_store is True.,environment,ironic
1756,configdrive_swift_temp_url_duration,"The timeout (in seconds) after which a configdrive temporary URL  becomes invalid. Defaults to deploy_callback_timeout if it is set,  otherwise to 1800 seconds. Used when configdrive_use_object_store is  True.",reliability,ironic
1757,inspect_wait_timeout,Timeout (seconds) for waiting for node inspection. 0 - unlimited.,reliability,ironic
1758,automated_clean,"Enables or disables automated cleaning. Automated cleaning is a  configurable set of steps, such as erasing disk drives, that are  performed on the node to ensure it is in a baseline state and ready to  be deployed to. This is done after instance deletion as well as during  the transition from a 'manageable' to 'available' state. When enabled,  the particular steps performed to clean a node depend on which driver  that node is managed by; see the individual driver's documentation for  details. NOTE: The introduction of the cleaning operation causes  instance deletion to take significantly longer. In an environment where  all tenants are trusted (eg, because there is only one tenant), this  option could be safely disabled.",others,ironic
1759,clean_callback_timeout,Timeout (seconds) to wait for a callback from the ramdisk doing the  cleaning. If the timeout is reached the node will be put in the 'clean  failed' provision state. Set to 0 to disable timeout.,reliability,ironic
1760,rescue_callback_timeout,Timeout (seconds) to wait for a callback from the rescue ramdisk. If  the timeout is reached the node will be put in the 'rescue failed'  provision state. Set to 0 to disable timeout.,reliability,ironic
1761,soft_power_off_timeout,Timeout (in seconds) of soft reboot and soft power off operation. This value always has to be positive.,reliability,ironic
1762,power_state_change_timeout,"Number of seconds to wait for power operations to complete, i.e., so  that a baremetal node is in the desired power state. If timed out, the  power operation is considered a failure.",reliability,ironic
1763,power_failure_recovery_interval,Interval (in seconds) between checking the power state for nodes  previously put into maintenance mode due to power synchronization  failure. A node is automatically moved out of maintenance mode once its  power state is retrieved successfully. Set to 0 to disable this check.,reliability,ironic
1764,conductor_group,Name of the conductor group to join. Can be up to 255 characters and  is case insensitive. This conductor will only manage nodes with a  matching 'conductor_group' field set on the node.,environment,ironic
1765,allow_deleting_available_nodes,Allow deleting nodes which are in state 'available'. Defaults to True.,security,ironic
1766,terminal,Path to serial console terminal program. Used only by Shell In A Box console.,environment,ironic
1767,terminal_cert_dir,Directory containing the terminal SSL cert (PEM) for serial console access. Used only by Shell In A Box console.,environment,ironic
1768,terminal_pid_dir,"Directory for holding terminal pid files. If not specified, the temporary directory will be used.",environment,ironic
1769,terminal_timeout,Timeout (in seconds) for the terminal session to be closed on  inactivity. Set to 0 to disable timeout. Used only by Socat console.,reliability,ironic
1770,subprocess_checking_interval,Time interval (in seconds) for checking the status of console subprocess.,reliability,ironic
1771,subprocess_timeout,Time (in seconds) to wait for the console subprocess to start.,reliability,ironic
1772,kill_timeout,Time (in seconds) to wait for the shellinabox console subprocess to exit before sending SIGKILL signal.,reliability,ironic
1773,socat_address,IP address of Socat service running on the host of ironic conductor. Used only by Socat console.,environment,ironic
1774,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests 'origin' header. Format:  '<protocol>://<host>[:<port>]', no trailing slash.  Example: https://horizon.example.com",others,ironic
1775,allow_credentials,Indicate that the actual request can include user credentials,security,ironic
1776,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,security,ironic
1777,max_age,Maximum cache age of CORS preflight requests.,performance,ironic
1778,allow_methods,Indicate which methods can be used during the actual request.,security,ironic
1779,allow_headers,Indicate which header field names may be used during the actual request.,security,ironic
1780,sqlite_synchronous,"If True, SQLite uses synchronous mode.",others,ironic
1781,backend,The back end to use for the database.,environment,ironic
1782,connection,The SQLAlchemy connection string to use to connect to the database.,environment,ironic
1783,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,environment,ironic
1784,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,ironic
1785,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",others,ironic
1786,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,ironic
1787,min_pool_size,Minimum number of SQL connections to keep open in a pool.,performance,ironic
1788,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,performance,ironic
1789,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,ironic
1790,retry_interval,Interval between retries of opening a SQL connection.,reliability,ironic
1791,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",others,ironic
1792,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,ironic
1793,connection_trace,Add Python stack traces to SQL as comment strings.,debuggability,ironic
1794,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,ironic
1795,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,reliability,ironic
1796,db_retry_interval,Seconds between retries of a database transaction.,reliability,ironic
1797,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",reliability,ironic
1798,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",reliability,ironic
1799,db_max_retries,Maximum retries in case of connection error or deadlock error before  error is raised. Set to -1 to specify an infinite retry count.,reliability,ironic
1800,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',others,ironic
1801,mysql_engine,MySQL engine to use.,others,ironic
1802,http_url,ironic-conductor node's HTTP server URL. Example: http://192.1.2.3:8080,environment,ironic
1803,http_root,ironic-conductor node's HTTP root path.,environment,ironic
1804,enable_ata_secure_erase,Whether to support the use of ATA Secure Erase during the cleaning process. Defaults to True.,security,ironic
1805,erase_devices_priority,"Priority to run in-band erase devices via the Ironic Python Agent  ramdisk. If unset, will use the priority set in the ramdisk (defaults to 10 for the GenericHardwareManager). If set to 0, will not run during  cleaning.",security,ironic
1806,erase_devices_metadata_priority,"Priority to run in-band clean step that erases metadata from devices, via the Ironic Python Agent ramdisk. If unset, will use the priority  set in the ramdisk (defaults to 99 for the GenericHardwareManager). If  set to 0, will not run during cleaning.",security,ironic
1807,shred_random_overwrite_iterations,"During shred, overwrite all block devices N times with random data.  This is only used if a device could not be ATA Secure Erased. Defaults  to 1.",others,ironic
1808,shred_final_overwrite_with_zeros,Whether to write zeros to a node's block devices after writing random data. This will write zeros to the device even when  deploy.shred_random_overwrite_iterations is 0. This option is only used  if a device could not be ATA Secure Erased. Defaults to True.,security,ironic
1809,continue_if_disk_secure_erase_fails,"Defines what to do if an ATA secure erase operation fails during  cleaning in the Ironic Python Agent. If False, the cleaning operation  will fail and the node will be put in clean failed state. If True, shred will be invoked and cleaning will continue.",security,ironic
1810,disk_erasure_concurrency,Defines the target pool size used by Ironic Python Agent ramdisk to  erase disk devices. The number of threads created to erase disks will  not exceed this value or the number of disks to be erased.,performance,ironic
1811,power_off_after_deploy_failure,Whether to power off a node after deploy failure. Defaults to True.,reliability,ironic
1812,default_boot_option,"Default boot option to use when no boot option is requested in node's driver_info. Currently the default is 'netboot', but it will be changed to 'local' in the future. It is recommended to set an explicit value  for this option.",others,ironic
1813,default_boot_mode,"Default boot mode to use when no boot mode is requested in node's driver_info, capabilities or in the instance_info configuration. Currently the default boot mode is 'bios', but it will  be changed to 'uefi in the future. It is recommended to set an explicit  value for this option. This option only has effect when management  interface supports boot mode management",others,ironic
1814,configdrive_use_object_store,Whether to upload the config drive to object store. Set this option to True to store config drive in a swift endpoint.,others,ironic
1815,http_image_subdir,"The name of subdirectory under ironic-conductor node's HTTP root path which is used to place instance images for the direct deploy interface, when local HTTP service is incorporated to provide instance image  instead of swift tempurls.",environment,ironic
1816,fast_track,"Whether to allow deployment agents to perform lookup, heartbeat  operations during initial states of a machine lifecycle and by-pass the  normal setup procedures for a ramdisk. This feature also enables power  operations which are part of deployment processes to be bypassed if the  ramdisk has performed a heartbeat operation using the fast_track_timeout setting.",reliability,ironic
1817,fast_track_timeout,Seconds for which the last heartbeat event is to be considered valid  for the purpose of a fast track sequence. This setting should generally  be less than the number of seconds for 'Power-On Self Test' and typical  ramdisk start-up. This value should not exceed the  [api]ramdisk_heartbeat_timeout setting.,reliability,ironic
1818,dhcp_provider,"DHCP provider to use. 'neutron' uses Neutron, and 'none' uses a no-op provider.",environment,ironic
1819,check_device_interval,"After Ironic has completed creating the partition table, it continues to check for activity on the attached iSCSI device status at this  interval prior to copying the image to the node, in seconds",reliability,ironic
1820,check_device_max_retries,"The maximum number of times to check that the device is not accessed  by another process. If the device is still busy after that, the disk  partitioning will be treated as having failed.",reliability,ironic
1821,efi_system_partition_size,Size of EFI system partition in MiB when configuring UEFI systems for local boot.,performance,ironic
1822,bios_boot_partition_size,Size of BIOS Boot partition in MiB when configuring GPT partitioned systems for local boot in BIOS.,performance,ironic
1823,dd_block_size,Block size to use when writing to the nodes disk.,performance,ironic
1824,iscsi_verify_attempts,"Maximum attempts to verify an iSCSI connection is active, sleeping 1 second between attempts.",reliability,ironic
1825,partprobe_attempts,Maximum number of attempts to try to read the partition.,reliability,ironic
1826,query_raid_config_job_status_interval,Interval (in seconds) between periodic RAID job status checks to  determine whether the asynchronous RAID configuration was successfully  finished or not.,reliability,ironic
1827,allowed_direct_url_schemes,A list of URL schemes that can be downloaded directly via the direct_url.  Currently supported schemes: [file].,security,ironic
1828,auth_url,Authentication URL,security,ironic
1829,auth_strategy,Authentication strategy to use when connecting to glance.,security,ironic
1830,auth_type,Authentication type to load,security,ironic
1831,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
1832,certfile,PEM encoded client certificate cert file,security,ironic
1833,collect_timing,Collect per-API call timing information.,debuggability,ironic
1834,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1835,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1836,domain_id,Domain ID to scope to,environment,ironic
1837,domain_name,Domain name to scope to,environment,ironic
1838,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
1839,glance_api_insecure,Allow to perform insecure SSL (https) requests to glance.,security,ironic
1840,glance_api_servers,A list of the glance api servers available to ironic. Prefix with https:// for SSL-based glance API servers. Format is [hostname|IP]:port.,environment,ironic
1841,glance_cafile,Optional path to a CA certificate bundle to be used to validate the  SSL certificate served by glance. It is used when glance_api_insecure is set to False.,security,ironic
1842,glance_num_retries,Number of retries when downloading an image from glance.,reliability,ironic
1843,insecure,Verify HTTPS connections.,security,ironic
1844,keyfile,PEM encoded client certificate key file,environment,ironic
1845,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
1846,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
1847,password,User's password,security,ironic
1848,project_domain_id,Domain ID containing project,environment,ironic
1849,project_domain_name,Domain name containing project,environment,ironic
1850,project_id,Project ID to scope to,environment,ironic
1851,project_name,Project name to scope to,environment,ironic
1852,region_name,The default region_name for endpoint URL discovery.,environment,ironic
1853,service_name,The default service_name for endpoint URL discovery.,environment,ironic
1854,service_type,The default service_type for endpoint URL discovery.,others,ironic
1855,split_loggers,Log requests to multiple loggers.,debuggability,ironic
1856,swift_account,"The account that Glance uses to communicate with Swift. The format is 'AUTH_uuid'. 'uuid' is the UUID for the account configured in the  glance-api.conf. For example:  'AUTH_a422b2-91f3-2f46-74b7-d7c9e8958f5d30'. If not set, the default  value is calculated based on the ID of the project used to access Swift  (as set in the [swift] section). Swift temporary URL format:  'endpoint_url/api_version/account/container/object_id'",environment,ironic
1857,swift_api_version,The Swift API version to create a temporary URL for. Defaults to  'v1'. Swift temporary URL format:  'endpoint_url/api_version/account/container/object_id',environment,ironic
1858,swift_container,"The Swift container Glance is configured to store its images in.  Defaults to 'glance', which is the default in glance-api.conf. Swift  temporary URL format:  'endpoint_url/api_version/account/container/object_id'",environment,ironic
1859,swift_endpoint_url,"The 'endpoint' (scheme, hostname, optional port) for the Swift URL of the form 'endpoint_url/api_version/account/container/object_id'. Do not include trailing '/'. For example, use 'https://swift.example.com'. If using RADOS Gateway, endpoint may also contain /swift path; if it  does not, it will be appended. Used for temporary URLs, will be fetched  from the service catalog, if not provided.",environment,ironic
1860,swift_store_multiple_containers_seed,"This should match a config by the same name in the Glance  configuration file. When set to 0, a single-tenant store will only use  one container to store all images. When set to an integer value between 1 and 32, a single-tenant store will use multiple containers to store  images, and this value will determine how many containers are created.",others,ironic
1861,swift_temp_url_cache_enabled,Whether to cache generated Swift temporary URLs. Setting it to true  is only useful when an image caching proxy is used. Defaults to False.,performance,ironic
1862,swift_temp_url_duration,"The length of time in seconds that the temporary URL will be valid  for. Defaults to 20 minutes. If some deploys get a 401 response code  when trying to download from the temporary URL, try raising this  duration. This value must be greater than or equal to the value for  swift_temp_url_expected_download_start_delay",reliability,ironic
1863,swift_temp_url_expected_download_start_delay,This is the delay (in seconds) from the time of the deploy request  (when the Swift temporary URL is generated) to when the IPA ramdisk  starts up and URL is used for the image download. This value is used to  check if the Swift temporary URL duration is large enough to let the  image download begin. Also if temporary URL caching is enabled this will determine if a cached entry will still be valid when the download  starts. swift_temp_url_duration value must be greater than or equal to  this option's value. Defaults to 0.,reliability,ironic
1864,swift_temp_url_key,"The secret token given to Swift to allow temporary URL downloads.  Required for temporary URLs. For the Swift backend, the key on the  service project (as set in the [swift] section) is used by default.",security,ironic
1865,system_scope,Scope for system operations,environment,ironic
1866,tenant_id,Tenant ID,environment,ironic
1867,tenant_name,Tenant Name,environment,ironic
1868,timeout,Timeout value for http requests,reliability,ironic
1869,trust_id,Trust ID,security,ironic
1870,user_domain_id,User's domain id,environment,ironic
1871,user_domain_name,User's domain name,environment,ironic
1872,user_id,User id,environment,ironic
1873,username,Username,environment,ironic
1874,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
1875,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
1876,path,The path to respond to healtcheck requests on.,environment,ironic
1877,detailed,Show more detailed information as part of the response. Security  note: Enabling this option may expose sensitive details about the  service being monitored. Be sure to verify that it will not violate your security policies.,security,ironic
1878,backends,Additional backends that can perform health checks and report that information back as part of a request.,reliability,ironic
1879,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,others,ironic
1880,disable_by_file_paths,Check the presence of a file based on a port to determine if an  application is running on a port. Expects a 'port:path' list of strings. Used by DisableByFilesPortsHealthcheck plugin.,environment,ironic
1881,enabled,Enable the health check endpoint at /healthcheck. Note that this is unauthenticated. More information is available at https://docs.openstack.org/oslo.middleware/latest/reference/healthcheck_plugins.html.,security,ironic
1882,client_timeout,Timeout (in seconds) for iLO operations,reliability,ironic
1883,client_port,Port to be used for iLO operations,environment,ironic
1884,swift_ilo_container,The Swift iLO container to store data.,environment,ironic
1885,swift_object_expiry_timeout,Amount of time in seconds for Swift objects to auto-expire.,reliability,ironic
1886,use_web_server_for_images,"Set this to True to use http web server to host floppy images and  generated boot ISO. This requires http_root and http_url to be  configured in the [deploy] section of the config file. If this is set to False, then Ironic will use Swift to host the floppy images and  generated boot_iso.",others,ironic
1887,clean_priority_reset_ilo,Priority for reset_ilo clean step.,security,ironic
1888,clean_priority_reset_bios_to_default,Priority for reset_bios_to_default clean step.,security,ironic
1889,clean_priority_reset_secure_boot_keys_to_default,Priority for reset_secure_boot_keys clean step. This step will reset the secure boot keys to manufacturing defaults.,security,ironic
1890,clean_priority_clear_secure_boot_keys,Priority for clear_secure_boot_keys clean step. This step is not  enabled by default. It can be enabled to clear all secure boot keys  enrolled with iLO.,security,ironic
1891,clean_priority_reset_ilo_credential,Priority for reset_ilo_credential clean step. This step requires  'ilo_change_password' parameter to be updated in nodes's driver_info  with the new password.,security,ironic
1892,power_retry,Number of times a power operation needs to be retried,reliability,ironic
1893,power_wait,Amount of time in seconds to wait in between power operations,reliability,ironic
1894,ca_file,CA certificate file to validate iLO.,security,ironic
1895,default_boot_mode,"Default boot mode to be used in provisioning when 'boot_mode'  capability is not provided in the 'properties/capabilities' of the node. The default is 'auto' for backward compatibility. When 'auto' is  specified, default boot mode will be selected based on boot mode  settings on the system.",others,ironic
1896,auth_url,Authentication URL,security,ironic
1897,auth_type,Authentication type to load,security,ironic
1898,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
1899,certfile,PEM encoded client certificate cert file,security,ironic
1900,collect_timing,Collect per-API call timing information.,debuggability,ironic
1901,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1902,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1903,domain_id,Domain ID to scope to,environment,ironic
1904,domain_name,Domain name to scope to,environment,ironic
1905,enabled,This option has no affect since the classic drivers removal.,others,ironic
1906,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
1907,insecure,Verify HTTPS connections.,security,ironic
1908,keyfile,PEM encoded client certificate key file,environment,ironic
1909,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
1910,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
1911,password,User's password,security,ironic
1912,project_domain_id,Domain ID containing project,environment,ironic
1913,project_domain_name,Domain name containing project,environment,ironic
1914,project_id,Project ID to scope to,environment,ironic
1915,project_name,Project name to scope to,environment,ironic
1916,region_name,The default region_name for endpoint URL discovery.,environment,ironic
1917,service_name,The default service_name for endpoint URL discovery.,environment,ironic
1918,service_type,The default service_type for endpoint URL discovery.,others,ironic
1919,service_url,"ironic-inspector HTTP endpoint. If this is not set, the service catalog will be used.",environment,ironic
1920,split_loggers,Log requests to multiple loggers.,debuggability,ironic
1921,status_check_period,period (in seconds) to check status of nodes on inspection,reliability,ironic
1922,system_scope,Scope for system operations,environment,ironic
1923,tenant_id,Tenant ID,environment,ironic
1924,tenant_name,Tenant Name,environment,ironic
1925,timeout,Timeout value for http requests,reliability,ironic
1926,trust_id,Trust ID,security,ironic
1927,user_domain_id,User's domain id,environment,ironic
1928,user_domain_name,User's domain name,environment,ironic
1929,user_id,User id,environment,ironic
1930,username,Username,environment,ironic
1931,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
1932,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
1933,command_retry_timeout,"Maximum time in seconds to retry retryable IPMI operations. (An  operation is retryable, for example, if the requested operation fails  because the BMC is busy.) Setting this too high can cause the sync power state periodic task to hang when there are slow or unresponsive BMCs.",reliability,ironic
1934,min_command_interval,"Minimum time, in seconds, between IPMI operations sent to a server.  There is a risk with some hardware that setting this too low may cause  the BMC to crash. Recommended setting is 5 seconds.",reliability,ironic
1935,kill_on_timeout,Kill ipmitool process invoked by ironic to read node power state if ipmitool process does not exit after command_retry_timeout timeout expires. Recommended setting is True,reliability,ironic
1936,disable_boot_timeout,"Default timeout behavior whether ironic sends a raw IPMI command to  disable the 60 second timeout for booting. Setting this option to False  will NOT send that command, the default value is True. It may be  overridden by per-node 'ipmi_disable_boot_timeout' option in node's  'driver_info' field.",reliability,ironic
1937,debug,Enables all ipmi commands to be executed with an additional debugging output. This is a separate option as ipmitool can log a substantial  amount of misleading text when in this mode.,debuggability,ironic
1938,remote_image_share_root,Ironic conductor node's 'NFS' or 'CIFS' root path,environment,ironic
1939,remote_image_server,IP of remote image server,environment,ironic
1940,remote_image_share_type,Share type of virtual media,others,ironic
1941,remote_image_share_name,share name of remote_image_server,others,ironic
1942,remote_image_user_name,User name of remote_image_server,environment,ironic
1943,remote_image_user_password,Password of remote_image_user_name,security,ironic
1944,remote_image_user_domain,Domain name of remote_image_user_name,environment,ironic
1945,port,Port to be used for iRMC operations,environment,ironic
1946,auth_method,Authentication method to be used for iRMC operations,security,ironic
1947,client_timeout,Timeout (in seconds) for iRMC operations,reliability,ironic
1948,sensor_method,Sensor data retrieval method.,others,ironic
1949,snmp_version,SNMP protocol version,environment,ironic
1950,snmp_port,SNMP port,environment,ironic
1951,snmp_community,SNMP community. Required for versions 'v1' and 'v2c',others,ironic
1952,snmp_security,SNMP security name. Required for version 'v3',security,ironic
1953,snmp_polling_interval,SNMP polling interval in seconds,reliability,ironic
1954,clean_priority_restore_irmc_bios_config,Priority for restore_irmc_bios_config clean step.,security,ironic
1955,gpu_ids,"List of vendor IDs and device IDs for GPU device to inspect. List  items are in format vendorID/deviceID and separated by commas. GPU  inspection will use this value to count the number of GPU device in a  node. If this option is not defined, then leave out pci_gpu_devices in  capabilities property. Sample gpu_ids value: 0x1000/0x0079,0x2100/0x0080",environment,ironic
1956,fpga_ids,"List of vendor IDs and device IDs for CPU FPGA to inspect. List items are in format vendorID/deviceID and separated by commas. CPU inspection will use this value to find existence of CPU FPGA in a node. If this  option is not defined, then leave out CUSTOM_CPU_FPGA in node traits.  Sample fpga_ids value: 0x1000/0x0079,0x2100/0x0080",environment,ironic
1957,query_raid_config_fgi_status_interval,Interval (in seconds) between periodic RAID status checks to  determine whether the asynchronous RAID configuration was successfully  finished or not. Foreground Initialization (FGI) will start 5 minutes  after creating virtual drives.,reliability,ironic
1958,root_helper,"Command that is prefixed to commands that are run as root. If not specified, no commands are run as root.",security,ironic
1959,portal_port,The port number on which the iSCSI portal listens for incoming connections.,environment,ironic
1960,conv_flags,"Flags that need to be sent to the dd command, to control the  conversion of the original file when copying to the host. It can contain several options separated by commas.",others,ironic
1961,auth_strategy,Authentication strategy used by JSON RPC. Defaults to the global auth_strategy setting.,security,ironic
1962,host_ip,The IP address or hostname on which JSON RPC will listen.,environment,ironic
1963,port,The port to use for JSON RPC,environment,ironic
1964,use_ssl,Whether to use TLS for JSON RPC,security,ironic
1965,auth_url,Authentication URL,security,ironic
1966,auth_type,Authentication type to load,security,ironic
1967,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
1968,certfile,PEM encoded client certificate cert file,security,ironic
1969,collect_timing,Collect per-API call timing information.,debuggability,ironic
1970,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1971,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
1972,domain_id,Domain ID to scope to,environment,ironic
1973,domain_name,Domain name to scope to,environment,ironic
1974,insecure,Verify HTTPS connections.,security,ironic
1975,keyfile,PEM encoded client certificate key file,environment,ironic
1976,password,User's password,security,ironic
1977,project_domain_id,Domain ID containing project,environment,ironic
1978,project_domain_name,Domain name containing project,environment,ironic
1979,project_id,Project ID to scope to,environment,ironic
1980,project_name,Project name to scope to,environment,ironic
1981,split_loggers,Log requests to multiple loggers.,debuggability,ironic
1982,system_scope,Scope for system operations,environment,ironic
1983,tenant_id,Tenant ID,environment,ironic
1984,tenant_name,Tenant Name,environment,ironic
1985,timeout,Timeout value for http requests,reliability,ironic
1986,trust_id,Trust ID,security,ironic
1987,user_domain_id,User's domain id,environment,ironic
1988,user_domain_name,User's domain name,environment,ironic
1989,user_id,User id,environment,ironic
1990,username,Username,environment,ironic
1991,www_authenticate_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint.",security,ironic
1992,auth_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint. This option is deprecated in favor of  www_authenticate_uri and will be removed in the S release.",security,ironic
1993,auth_version,API version of the admin Identity API endpoint.,environment,ironic
1994,delay_auth_decision,"Do not handle authorization requests within the middleware, but  delegate the authorization decision to downstream WSGI components.",security,ironic
1995,http_connect_timeout,Request timeout value for communicating with Identity API server.,reliability,ironic
1996,http_request_max_retries,How many times are we trying to reconnect when communicating with Identity API Server.,reliability,ironic
1997,cache,"Request environment key where the Swift cache object is stored. When  auth_token middleware is deployed with a Swift cache, use this option to have the middleware share a caching backend with swift. Otherwise, use  the memcached_servers option instead.",performance,ironic
1998,certfile,Required if identity server requires client certificate,security,ironic
1999,keyfile,Required if identity server requires client certificate,environment,ironic
2000,cafile,A PEM encoded Certificate Authority to use when verifying HTTPs connections. Defaults to system CAs.,security,ironic
2001,insecure,Verify HTTPS connections.,security,ironic
2002,region_name,The region in which the identity server can be found.,environment,ironic
2003,signing_dir,Directory used to cache files related to PKI tokens. This option has  been deprecated in the Ocata release and will be removed in the P  release.,environment,ironic
2004,memcached_servers,"Optionally specify a list of memcached server(s) to use for caching.  If left undefined, tokens will instead be cached in-process.",performance,ironic
2005,token_cache_time,"In order to prevent excessive effort spent validating tokens, the  middleware caches previously-seen tokens for a configurable duration (in seconds). Set to -1 to disable caching completely.",reliability,ironic
2006,memcache_security_strategy,"(Optional) If defined, indicate whether token data should be  authenticated or authenticated and encrypted. If MAC, token data is  authenticated (with HMAC) in the cache. If ENCRYPT, token data is  encrypted and authenticated in the cache. If the value is not one of  these options or empty, auth_token will raise an exception on  initialization.",security,ironic
2007,memcache_secret_key,"(Optional, mandatory if memcache_security_strategy is defined) This string is used for key derivation.",security,ironic
2008,memcache_pool_dead_retry,(Optional) Number of seconds memcached server is considered dead before it is tried again.,reliability,ironic
2009,memcache_pool_maxsize,(Optional) Maximum total number of open connections to every memcached server.,performance,ironic
2010,memcache_pool_socket_timeout,(Optional) Socket timeout in seconds for communicating with a memcached server.,reliability,ironic
2011,memcache_pool_unused_timeout,(Optional) Number of seconds a connection to memcached is held unused in the pool before it is closed.,reliability,ironic
2012,memcache_pool_conn_get_timeout,(Optional) Number of seconds that an operation will wait to get a memcached client connection from the pool.,reliability,ironic
2013,memcache_use_advanced_pool,(Optional) Use the advanced (eventlet safe) memcached client pool. The advanced pool will only work under python 2.x.,security,ironic
2014,include_service_catalog,"(Optional) Indicate whether to set the X-Service-Catalog header. If  False, middleware will not ask for service catalog on token validation  and will not set the X-Service-Catalog header.",others,ironic
2015,enforce_token_bind,Used to control the use and type of token binding. Can be set to:  'disabled' to not check token binding. 'permissive' (default) to  validate binding information if the bind type is of a form known to the  server and ignore it if not. 'strict' like 'permissive' but if the bind  type is unknown the token will be rejected. 'required' any form of token binding is needed to be allowed. Finally the name of a binding method  that must be present in tokens.,security,ironic
2016,hash_algorithms,"Hash algorithms to use for hashing PKI tokens. This may be a single  algorithm or multiple. The algorithms are those supported by Python  standard hashlib.new(). The hashes will be tried in the order given, so  put the preferred one first for performance. The result of the first  hash will be stored in the cache. This will typically be set to multiple values only while migrating from a less secure algorithm to a more  secure one. Once all the old tokens are expired this option should be  set to a single value for better performance.",performance,ironic
2017,service_token_roles,A choice of roles that must be present in a service token. Service  tokens are allowed to request that an expired token can be used and so  this check should tightly control that only actual services should be  sending this token. Roles here are applied as an ANY check so any role  in this list must be present. For backwards compatibility reasons this  currently only affects the allow_expired check.,others,ironic
2018,service_token_roles_required,For backwards compatibility reasons we must let valid service tokens  pass that don't pass the service_token_roles check as valid. Setting  this true will become the default in a future release and should be  enabled if possible.,environment,ironic
2019,auth_type,Authentication type to load,security,ironic
2020,auth_section,Config Section from which to load plugin specific options,environment,ironic
2021,backend,Backend to use for the metrics system.,environment,ironic
2022,prepend_host,Prepend the hostname to all metric names. The format of metric names is [global_prefix.][host_name.]prefix.metric_name.,others,ironic
2023,prepend_host_reverse,Split the prepended host value by '.' and reverse it (to better match the reverse hierarchical form of domain names).,others,ironic
2024,global_prefix,"Prefix all metric names with this value. By default, there is no  global prefix. The format of metric names is  [global_prefix.][host_name.]prefix.metric_name.",environment,ironic
2025,agent_backend,Backend for the agent ramdisk to use for metrics. Default possible backends are 'noop' and 'statsd'.,environment,ironic
2026,agent_prepend_host,Prepend the hostname to all metric names sent by the agent ramdisk.  The format of metric names is  [global_prefix.][uuid.][host_name.]prefix.metric_name.,others,ironic
2027,agent_prepend_uuid,Prepend the node's Ironic uuid to all metric names sent by the agent  ramdisk. The format of metric names is  [global_prefix.][uuid.][host_name.]prefix.metric_name.,others,ironic
2028,agent_prepend_host_reverse,Split the prepended host value by '.' and reverse it for metrics sent by the agent ramdisk (to better match the reverse hierarchical form of  domain names).,others,ironic
2029,agent_global_prefix,Prefix all metric names sent by the agent ramdisk with this value.  The format of metric names is  [global_prefix.][uuid.][host_name.]prefix.metric_name.,others,ironic
2030,statsd_host,Host for use with the statsd backend.,environment,ironic
2031,statsd_port,Port to use with the statsd backend.,environment,ironic
2032,agent_statsd_host,Host for the agent ramdisk to use with the statsd backend. This must be accessible from networks the agent is booted on.,environment,ironic
2033,agent_statsd_port,Port for the agent ramdisk to use with the statsd backend.,environment,ironic
2034,auth_url,Authentication URL,security,ironic
2035,auth_strategy,Authentication strategy to use when connecting to neutron. Running  neutron in noauth mode (related to but not affected by this setting) is  insecure and should only be used for testing.,security,ironic
2036,auth_type,Authentication type to load,security,ironic
2037,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
2038,certfile,PEM encoded client certificate cert file,security,ironic
2039,cleaning_network,"Neutron network UUID or name for the ramdisk to be booted into for  cleaning nodes. Required for 'neutron' network interface. It is also  required if cleaning nodes when using 'flat' network interface or  'neutron' DHCP provider. If a name is provided, it must be unique among  all networks or cleaning will fail.",environment,ironic
2040,cleaning_network_security_groups,"List of Neutron Security Group UUIDs to be applied during cleaning of the nodes. Optional for the 'neutron' network interface and not used  for the 'flat' or 'noop' network interfaces. If not specified, default  security group is used.",security,ironic
2041,collect_timing,Collect per-API call timing information.,debuggability,ironic
2042,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2043,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2044,domain_id,Domain ID to scope to,environment,ironic
2045,domain_name,Domain name to scope to,environment,ironic
2046,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
2047,insecure,Verify HTTPS connections.,security,ironic
2048,keyfile,PEM encoded client certificate key file,environment,ironic
2049,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
2050,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
2051,password,User's password,security,ironic
2052,port_setup_delay,Delay value to wait for Neutron agents to setup sufficient DHCP configuration for port.,reliability,ironic
2053,project_domain_id,Domain ID containing project,environment,ironic
2054,project_domain_name,Domain name containing project,environment,ironic
2055,project_id,Project ID to scope to,environment,ironic
2056,project_name,Project name to scope to,environment,ironic
2057,provisioning_network,"Neutron network UUID or name for the ramdisk to be booted into for  provisioning nodes. Required for 'neutron' network interface. If a name  is provided, it must be unique among all networks or deploy will fail.",environment,ironic
2058,provisioning_network_security_groups,"List of Neutron Security Group UUIDs to be applied during  provisioning of the nodes. Optional for the 'neutron' network interface  and not used for the 'flat' or 'noop' network interfaces. If not  specified, default security group is used.",security,ironic
2059,region_name,The default region_name for endpoint URL discovery.,environment,ironic
2060,rescuing_network,"Neutron network UUID or name for booting the ramdisk for rescue mode. This is not the network that the rescue ramdisk will use post-boot the tenant network is used for that. Required for 'neutron' network  interface, if rescue mode will be used. It is not used for the 'flat' or 'noop' network interfaces. If a name is provided, it must be unique  among all networks or rescue will fail.",reliability,ironic
2061,rescuing_network_security_groups,"List of Neutron Security Group UUIDs to be applied during the node  rescue process. Optional for the 'neutron' network interface and not  used for the 'flat' or 'noop' network interfaces. If not specified, the  default security group is used.",security,ironic
2062,retries,Client retries in the case of a failed request.,reliability,ironic
2063,service_name,The default service_name for endpoint URL discovery.,environment,ironic
2064,service_type,The default service_type for endpoint URL discovery.,others,ironic
2065,split_loggers,Log requests to multiple loggers.,debuggability,ironic
2066,system_scope,Scope for system operations,environment,ironic
2067,tenant_id,Tenant ID,environment,ironic
2068,tenant_name,Tenant Name,environment,ironic
2069,timeout,Timeout value for http requests,reliability,ironic
2070,trust_id,Trust ID,security,ironic
2071,url,"URL for connecting to neutron. Default value translates to 'http://$my_ip:9696' when auth_strategy is 'noauth', and to discovery from Keystone catalog when auth_strategy is 'keystone'.",environment,ironic
2072,url_timeout,Timeout value for connecting to neutron in seconds.,reliability,ironic
2073,user_domain_id,User's domain id,environment,ironic
2074,user_domain_name,User's domain name,environment,ironic
2075,user_id,User id,environment,ironic
2076,username,Username,environment,ironic
2077,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
2078,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
2079,disable_process_locking,Enables or disables inter-process locks.,security,ironic
2080,lock_path,"Directory to use for lock files.  For security, the specified  directory should only be writable by the user running the processes that need locking. Defaults to environment variable OSLO_LOCK_PATH. If  external locks are used, a lock path must be set.",environment,ironic
2081,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,environment,ironic
2082,idle_timeout,Timeout for inactive connections (in seconds),reliability,ironic
2083,trace,Debug: dump AMQP frames to stdout,debuggability,ironic
2084,ssl,"Attempt to connect via SSL. If no other ssl-related parameters are  given, it will use the system's CA-bundle to verify the server's  certificate.",security,ironic
2085,ssl_ca_file,CA certificate PEM file used to verify the server's certificate,security,ironic
2086,ssl_cert_file,Self-identifying certificate PEM file for client authentication,security,ironic
2087,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),security,ironic
2088,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),security,ironic
2089,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate  matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the  server uses the Server Name Indication TLS extension (rfc6066) to  provide a certificate per virtual host. Set ssl_verify_vhost to True if  the server's SSL certificate uses the virtual host name instead of the  DNS name.",security,ironic
2090,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,reliability,ironic
2091,sasl_config_dir,Path to directory that contains the SASL configuration,environment,ironic
2092,sasl_config_name,Name of configuration file (without .conf suffix),environment,ironic
2093,sasl_default_realm,SASL realm to use if no realm present in username,security,ironic
2094,connection_retry_interval,Seconds to pause before attempting to re-connect.,reliability,ironic
2095,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,reliability,ironic
2096,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,reliability,ironic
2097,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,reliability,ironic
2098,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,reliability,ironic
2099,default_reply_timeout,The deadline for an rpc reply message delivery.,reliability,ironic
2100,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,reliability,ironic
2101,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,reliability,ironic
2102,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,reliability,ironic
2103,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy'   - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic'  - use legacy addresses if the message bus does not support routing otherwise use routable addressing,security,ironic
2104,pseudo_vhost,"Enable virtual host support for those message buses that do not  natively support virtual hosting (such as qpidd). When set to true the  virtual host name will be added to all message bus addresses,  effectively creating a private 'subnet' per virtual host. Set to False  if the message bus supports virtual hosting using the 'hostname' field  in the AMQP 1.0 Open performative as the name of the virtual host.",security,ironic
2105,server_request_prefix,address prefix used when sending to a specific server,environment,ironic
2106,broadcast_prefix,address prefix used when broadcasting to all servers,others,ironic
2107,group_request_prefix,address prefix when sending to any server in group,environment,ironic
2108,rpc_address_prefix,Address prefix for all generated RPC addresses,environment,ironic
2109,notify_address_prefix,Address prefix for all generated Notification addresses,environment,ironic
2110,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,environment,ironic
2111,unicast_address,Appended to the address prefix when sending to a particular  RPC/Notification server. Used by the message bus to identify messages  sent to a single destination.,environment,ironic
2112,anycast_address,Appended to the address prefix when sending to a group of consumers.  Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,environment,ironic
2113,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',others,ironic
2114,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',others,ironic
2115,reply_link_credit,Window size for incoming RPC Reply messages.,performance,ironic
2116,rpc_server_credit,Window size for incoming RPC Request messages,performance,ironic
2117,notify_server_credit,Window size for incoming Notification messages,performance,ironic
2118,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify'   - Send Notifications pre-settled,others,ironic
2119,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,performance,ironic
2120,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,reliability,ironic
2121,pool_size,Pool Size for Kafka Consumers,performance,ironic
2122,conn_pool_min_size,The pool size limit for connections expiration policy,performance,ironic
2123,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,ironic
2124,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,environment,ironic
2125,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,reliability,ironic
2126,producer_batch_size,Size of batch for the producer async send,performance,ironic
2127,enable_auto_commit,Enable asynchronous consumer commits,security,ironic
2128,max_poll_records,The maximum number of records returned in a poll call,others,ironic
2129,security_protocol,Protocol used to communicate with brokers,security,ironic
2130,sasl_mechanism,Mechanism when security protocol is SASL,security,ironic
2131,ssl_cafile,CA certificate PEM file used to verify the server certificate,security,ironic
2132,driver,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",others,ironic
2133,transport_url,"A URL representing the messaging driver to use for notifications. If  not set, we fall back to the same configuration used for RPC.",environment,ironic
2134,topics,AMQP topic used for OpenStack notifications.,others,ironic
2135,retry,"The maximum number of attempts to re-send a notification message  which failed to be delivered due to a recoverable error. 0 - No retry,  -1 - indefinite",reliability,ironic
2136,amqp_durable_queues,Use durable queues in AMQP.,others,ironic
2137,amqp_auto_delete,Auto-delete queues in AMQP.,others,ironic
2138,ssl,Connect over SSL.,security,ironic
2139,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",security,ironic
2140,ssl_key_file,SSL key file (valid only if SSL enabled).,security,ironic
2141,ssl_cert_file,SSL cert file (valid only if SSL enabled).,security,ironic
2142,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,security,ironic
2143,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,reliability,ironic
2144,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression  will not be used. This option may not be available in future versions.",others,ironic
2145,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its  replies. This value should not be longer than rpc_response_timeout.,reliability,ironic
2146,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we  are currently connected to becomes unavailable. Takes effect only if  more than one RabbitMQ node is provided in config.,others,ironic
2147,rabbit_login_method,The RabbitMQ login method.,others,ironic
2148,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,reliability,ironic
2149,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,reliability,ironic
2150,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,reliability,ironic
2151,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change  this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when  declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run:  'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",environment,ironic
2152,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL  (x-expires). Queues which are unused for the duration of the TTL are  automatically deleted. The parameter affects only reply and fanout  queues.,reliability,ironic
2153,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,others,ironic
2154,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,reliability,ironic
2155,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,reliability,ironic
2156,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",security,ironic
2157,policy_file,The file that defines policies.,security,ironic
2158,policy_default_rule,Default rule. Enforced when a requested rule is not found.,others,ironic
2159,policy_dirs,"Directories where policy configuration files are stored. They can be  relative to any directory in the search path defined by the config_dir  option, or absolute paths. The file defined by policy_file must exist  for these directories to be searched.  Missing or empty directories are  ignored.",environment,ironic
2160,remote_content_type,Content Type to send and receive data for REST based policy check,others,ironic
2161,remote_ssl_verify_server_crt,server identity verification for REST based policy check,security,ironic
2162,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,environment,ironic
2163,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,environment,ironic
2164,remote_ssl_client_key_file,Absolute path client key file REST based policy check,environment,ironic
2165,enabled,Enable the profiling for all services on this node.,debuggability,ironic
2166,trace_sqlalchemy,Enable SQL requests profiling in services.,debuggability,ironic
2167,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,security,ironic
2168,connection_string,Connection string for a notifier backend.,environment,ironic
2169,es_doc_type,Document type for notification indexing in elasticsearch.,others,ironic
2170,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",reliability,ironic
2171,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,performance,ironic
2172,socket_timeout,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,reliability,ironic
2173,sentinel_service_name,Redissentinel uses a service name to identify a master redis service. This parameter defines the name (for example: sentinal_service_name=mymaster).,environment,ironic
2174,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,debuggability,ironic
2175,pxe_append_params,Additional append parameters for baremetal PXE boot.,others,ironic
2176,default_ephemeral_format,"Default file system format for ephemeral partition, if one is created.",others,ironic
2177,images_path,"On the ironic-conductor node, directory where images are stored on disk.",environment,ironic
2178,instance_master_path,"On the ironic-conductor node, directory where master instance images  are stored on disk. Setting to the empty string disables image caching.",environment,ironic
2179,image_cache_size,"Maximum size (in MiB) of cache for master images, including those in use.",performance,ironic
2180,image_cache_ttl,Maximum TTL (in minutes) for old master images in cache.,reliability,ironic
2181,pxe_config_template,"On ironic-conductor node, template file for PXE configuration.",environment,ironic
2182,uefi_pxe_config_template,"On ironic-conductor node, template file for PXE configuration for UEFI boot loader.",environment,ironic
2183,pxe_config_template_by_arch,"On ironic-conductor node, template file for PXE configuration per  node architecture. For example:  aarch64:/opt/share/grubaa64_pxe_config.template",environment,ironic
2184,tftp_server,IP address of ironic-conductor node's TFTP server.,environment,ironic
2185,tftp_root,ironic-conductor node's TFTP root path. The ironic-conductor must have read/write access to this path.,environment,ironic
2186,tftp_master_path,"On ironic-conductor node, directory where master TFTP images are  stored on disk. Setting to the empty string disables image caching.",environment,ironic
2187,dir_permission,The permission that will be applied to the TFTP folders upon  creation. This should be set to the permission such that the tftpserver  has access to read the contents of the configured TFTP folder. This  setting is only required when the operating system's umask is  restrictive such that ironic-conductor is creating files that cannot be  read by the TFTP server. Setting to <None> will result in the  operating system's umask to be utilized for the creation of new tftp  folders. It is recommended that an octal representation is specified.  For example: 0o755,security,ironic
2188,pxe_bootfile_name,Bootfile DHCP parameter.,others,ironic
2189,pxe_config_subdir,Directory in which to create symbolic links which represent the MAC  or IP address of the ports on a node and allow boot loaders to load the  PXE file for the node. This directory name is relative to the PXE or  iPXE folders.,environment,ironic
2190,uefi_pxe_bootfile_name,Bootfile DHCP parameter for UEFI boot mode.,environment,ironic
2191,pxe_bootfile_name_by_arch,Bootfile DHCP parameter per node architecture. For example: aarch64:grubaa64.efi,others,ironic
2192,ipxe_enabled,Defaults the PXE interface to only use iPXE.,others,ironic
2193,ipxe_boot_script,"On ironic-conductor node, the path to the main iPXE script file.",environment,ironic
2194,ipxe_timeout,Timeout value (in seconds) for downloading an image via iPXE. Defaults to 0 (no timeout),reliability,ironic
2195,ip_version,The IP version that will be used for PXE booting. Defaults to 4. EXPERIMENTAL,environment,ironic
2196,ipxe_use_swift,"Download deploy and rescue images directly from swift using temporary URLs. If set to false (default), images are downloaded to the  ironic-conductor node and served over its local HTTP server. Applicable  only when 'ipxe_enabled' option is set to true.",environment,ironic
2197,auth_url,Authentication URL,security,ironic
2198,auth_type,Authentication type to load,security,ironic
2199,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
2200,certfile,PEM encoded client certificate cert file,security,ironic
2201,collect_timing,Collect per-API call timing information.,debuggability,ironic
2202,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2203,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2204,domain_id,Domain ID to scope to,environment,ironic
2205,domain_name,Domain name to scope to,environment,ironic
2206,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
2207,insecure,Verify HTTPS connections.,security,ironic
2208,keyfile,PEM encoded client certificate key file,environment,ironic
2209,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
2210,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
2211,password,User's password,security,ironic
2212,project_domain_id,Domain ID containing project,environment,ironic
2213,project_domain_name,Domain name containing project,environment,ironic
2214,project_id,Project ID to scope to,environment,ironic
2215,project_name,Project name to scope to,environment,ironic
2216,region_name,The default region_name for endpoint URL discovery.,environment,ironic
2217,service_name,The default service_name for endpoint URL discovery.,environment,ironic
2218,service_type,The default service_type for endpoint URL discovery.,environment,ironic
2219,split_loggers,Log requests to multiple loggers.,debuggability,ironic
2220,system_scope,Scope for system operations,environment,ironic
2221,tenant_id,Tenant ID,environment,ironic
2222,tenant_name,Tenant Name,environment,ironic
2223,timeout,Timeout value for http requests,reliability,ironic
2224,trust_id,Trust ID,security,ironic
2225,user_domain_id,User's domain id,environment,ironic
2226,user_domain_name,User's domain name,environment,ironic
2227,user_id,User id,environment,ironic
2228,username,Username,environment,ironic
2229,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
2230,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
2231,power_timeout,Seconds to wait for power action to be completed,reliability,ironic
2232,reboot_delay,Time (in seconds) to sleep between when rebooting (powering off and on again),reliability,ironic
2233,udp_transport_timeout,Response timeout in seconds used for UDP transport. Timeout should be a multiple of 0.5 seconds and is applicable to each retry.,reliability,ironic
2234,udp_transport_retries,"Maximum number of UDP request retries, 0 means no retries.",reliability,ironic
2235,ca_file,CA certificate file to use to verify connecting clients.,security,ironic
2236,cert_file,Certificate file to use when starting the server securely.,security,ironic
2237,key_file,Private key file to use when starting the server securely.,security,ironic
2238,version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",environment,ironic
2239,ciphers,Sets the list of available ciphers. value should be a string in the OpenSSL cipher list format.,security,ironic
2240,auth_url,Authentication URL,security,ironic
2241,auth_type,Authentication type to load,security,ironic
2242,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,ironic
2243,certfile,PEM encoded client certificate cert file,security,ironic
2244,collect_timing,Collect per-API call timing information.,debuggability,ironic
2245,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2246,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,ironic
2247,domain_id,Domain ID to scope to,environment,ironic
2248,domain_name,Domain name to scope to,environment,ironic
2249,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",environment,ironic
2250,insecure,Verify HTTPS connections.,security,ironic
2251,keyfile,PEM encoded client certificate key file,environment,ironic
2252,max_version,"The maximum major version of a given API, intended to be used as the  upper bound of a range with min_version. Mutually exclusive with  version.",environment,ironic
2253,min_version,"The minimum major version of a given API, intended to be used as the  lower bound of a range with max_version. Mutually exclusive with  version. If min_version is given with no max_version it is as if max  version is 'latest'.",environment,ironic
2254,password,User's password,security,ironic
2255,project_domain_id,Domain ID containing project,environment,ironic
2256,project_domain_name,Domain name containing project,environment,ironic
2257,project_id,Project ID to scope to,environment,ironic
2258,project_name,Project name to scope to,environment,ironic
2259,region_name,The default region_name for endpoint URL discovery.,environment,ironic
2260,service_name,The default service_name for endpoint URL discovery.,environment,ironic
2261,service_type,The default service_type for endpoint URL discovery.,environment,ironic
2262,split_loggers,Log requests to multiple loggers.,debuggability,ironic
2263,swift_max_retries,"Maximum number of times to retry a Swift request, before failing.",reliability,ironic
2264,system_scope,Scope for system operations,environment,ironic
2265,tenant_id,Tenant ID,environment,ironic
2266,tenant_name,Tenant Name,environment,ironic
2267,timeout,Timeout value for http requests,reliability,ironic
2268,trust_id,Trust ID,security,ironic
2269,user_domain_id,User's domain id,environment,ironic
2270,user_domain_name,User's domain name,environment,ironic
2271,user_id,User id,environment,ironic
2272,username,Username,environment,ironic
2273,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",environment,ironic
2274,version,Minimum Major API version within a given Major API version for  endpoint URL discovery. Mutually exclusive with min_version and  max_version,environment,ironic
2275,manager_ip,IP address of the XClarity Controller. Configuration here is  deprecated and will be removed in the Stein release. Please update the  driver_info field to use 'xclarity_manager_ip' instead,environment,ironic
2276,username,Username for the XClarity Controller. Configuration here is  deprecated and will be removed in the Stein release. Please update the  driver_info field to use 'xclarity_username' instead,environment,ironic
2277,password,Password for XClarity Controller username. Configuration here is  deprecated and will be removed in the Stein release. Please update the  driver_info field to use 'xclarity_password' instead,security,ironic
2278,port,Port to be used for XClarity Controller connection.,environment,ironic
2279,rpc_conn_pool_size,Size of RPC connection pool.,performance,keystone
2280,conn_pool_min_size,The pool size limit for connections expiration policy,performance,keystone
2281,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,keystone
2282,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,performance,keystone
2283,rpc_response_timeout,Seconds to wait for a response from a call.,reliability,keystone
2284,transport_url,"The network address and optional user credentials for connecting to  the messaging backend, in URL format. The expected format is:",environment,keystone
2285,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,security,keystone
2286,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",debuggability,keystone
2287,log_config_append,"The name of a logging configuration file. This file is appended to  any existing logging configuration files. For details about logging  configuration files, see the Python logging module documentation. Note  that when logging configuration files are used then all logging  configuration is set in the configuration file and other logging  configuration options are ignored (for example, log-date-format).",environment,keystone
2288,log_date_format,Defines the format string for %(asctime)s in log records. Default:  the value above . This option is ignored if log_config_append is set.,debuggability,keystone
2289,log_file,"(Optional) Name of log file to send logging output to. If no default  is set, logging will go to stderr as defined by use_stderr. This option  is ignored if log_config_append is set.",debuggability,keystone
2290,log_dir,(Optional) The base directory used for relative log_file  paths. This option is ignored if log_config_append is set.,environment,keystone
2291,watch_log_file,Uses logging handler designed to watch file system. When log file is  moved or removed this handler will open a new log file with specified  path instantaneously. It makes sense only if log_file option is  specified and Linux platform is used. This option is ignored if  log_config_append is set.,debuggability,keystone
2292,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if  log_config_append is set.,debuggability,keystone
2293,use_journal,Enable journald for logging. If running in a systemd environment you  may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log  messages.This option is ignored if log_config_append is set.,debuggability,keystone
2294,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,debuggability,keystone
2295,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,others,keystone
2296,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,debuggability,keystone
2297,use_eventlog,Log output to Windows Event Log.,debuggability,keystone
2298,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,reliability,keystone
2299,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next  rotation.,reliability,keystone
2300,max_logfile_count,Maximum number of rotated log files.,performance,keystone
2301,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,performance,keystone
2302,log_rotation_type,Log rotation type.,reliability,keystone
2303,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,debuggability,keystone
2304,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,debuggability,keystone
2305,logging_debug_format_suffix,Additional data to append to log message when logging level for the  message is DEBUG. Used by oslo_log.formatters.ContextFormatter,debuggability,keystone
2306,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,debuggability,keystone
2307,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in  logging_context_format_string. Used by  oslo_log.formatters.ContextFormatter,debuggability,keystone
2308,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,debuggability,keystone
2309,publish_errors,Enables or disables publication of error events.,debuggability,keystone
2310,instance_format,The format for an instance that is passed with the log message.,debuggability,keystone
2311,instance_uuid_format,The format for an instance UUID that is passed with the log message.,debuggability,keystone
2312,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",reliability,keystone
2313,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,reliability,keystone
2314,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to  rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",debuggability,keystone
2315,fatal_deprecations,Enables or disables fatal status of deprecations.,debuggability,keystone
2316,admin_token,"Using this feature is NOT recommended. Instead, use the keystone-manage bootstrap command. The value of this option is treated as a 'shared secret' that  can be used to bootstrap Keystone through the API. This 'token' does not represent a user (it has no identity), and carries no explicit  authorization (it effectively bypasses most authorization checks). If  set to None, the value is ignored and the admin_token middleware is effectively disabled.",security,keystone
2317,public_endpoint,"The base public endpoint URL for Keystone that is advertised to  clients (NOTE: this does NOT affect how Keystone listens for  connections). Defaults to the base host URL of the request. For example, if keystone receives a request to http://server:5000/v3/users, then this will option will be automatically treated as http://server:5000. You should only need to set option if either the value of the base URL  contains a path that keystone does not automatically infer (/prefix/v3), or if the endpoint should be found on a different host.",environment,keystone
2318,admin_endpoint,"The base admin endpoint URL for Keystone that is advertised to  clients (NOTE: this does NOT affect how Keystone listens for  connections). Defaults to the base host URL of the request. For example, if keystone receives a request to http://server:35357/v3/users, then this will option will be automatically treated as http://server:35357. You should only need to set option if either the value of the base URL  contains a path that keystone does not automatically infer (/prefix/v3), or if the endpoint should be found on a different host.",environment,keystone
2319,max_project_tree_depth,"Maximum depth of the project hierarchy, excluding the project acting  as a domain at the top of the hierarchy. WARNING: Setting it to a large  value may adversely impact performance.",reliability,keystone
2320,max_param_size,Limit the sizes of user & project ID/names.,others,keystone
2321,max_token_size,"Similar to [DEFAULT] max_param_size, but provides an  exception for token values. With Fernet tokens, this can be set as low  as 255. With UUID tokens, this should be set to 32).",performance,keystone
2322,list_limit,"The maximum number of entities that will be returned in a collection. This global limit may be then overridden for a specific driver, by  specifying a list_limit in the appropriate section (for example, [assignment]). No limit is set by default. In larger deployments, it is recommended  that you set this to a reasonable number to prevent operations like  listing all users and projects from placing an unnecessary load on the  system.",performance,keystone
2323,strict_password_check,"If set to true, strict password length checking is performed for  password manipulation. If a password exceeds the maximum length, the  operation will fail with an HTTP 403 Forbidden error. If set to false,  passwords are automatically truncated to the maximum length.",security,keystone
2324,insecure_debug,"If set to true, then the server will return information in HTTP  responses that may allow an unauthenticated or authenticated user to get more information than normal, such as additional details about why  authentication failed. This may be useful for debugging but is insecure.",security,keystone
2325,default_publisher_id,"Default publisher_id for outgoing notifications. If left undefined, Keystone will default to using the server's host name.",environment,keystone
2326,notification_format,"Define the notification format for identity service events. A basic notification only has information about the resource being operated on. A cadf notification has the same information, as well as information about the initiator of the event. The cadf option is entirely backwards compatible with the basic option, but is fully CADF-compliant, and is recommended for auditing use cases.",debuggability,keystone
2327,notification_opt_out,"You can reduce the number of notifications keystone emits by  explicitly opting out. Keystone will not emit notifications that match  the patterns expressed in this list. Values are expected to be in the  form of identity.<resource_type>.<operation>.  By default, all notifications related to authentication are  automatically suppressed. This field can be set multiple times in order  to opt-out of multiple notification topics. For example, the following  suppresses notifications describing user creation or successful  authentication events: notification_opt_out=identity.user.create  notification_opt_out=identity.authenticate.success",others,keystone
2328,driver,"Entry point for the access rules config backend driver in the keystone.access_rules_config namespace.  Keystone only provides a json driver, so there is no reason to change this unless you are providing a custom entry point.",environment,keystone
2329,caching,Toggle for access rules caching. This has no effect unless global caching is enabled.,performance,keystone
2330,cache_time,Time to cache access rule data in seconds. This has no effect unless global caching is enabled.,performance,keystone
2331,rules_file,"Path to access rules configuration. If not present, no access rule  configuration will be loaded and application credential access rules  will be unavailable.",environment,keystone
2332,permissive,"Toggles permissive mode for access rules. When enabled, application  credentials can be created with any access rules regardless of  operator's configuration.",security,keystone
2333,driver,"Entry point for the application credential backend driver in the keystone.application_credential namespace.  Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",environment,keystone
2334,caching,Toggle for application credential caching. This has no effect unless global caching is enabled.,performance,keystone
2335,cache_time,Time to cache application credential data in seconds. This has no effect unless global caching is enabled.,performance,keystone
2336,user_limit,"Maximum number of application credentials a user is permitted to  create. A value of -1 means unlimited. If a limit is not set, users are  permitted to create application credentials at will, which could lead to bloat in the keystone database or open keystone to a DoS attack.",others,keystone
2337,driver,"Entry point for the assignment backend driver (where role assignments are stored) in the keystone.assignment namespace. Only a SQL driver is supplied by keystone itself. Unless you are writing proprietary drivers for keystone, you do not need to set  this option.",environment,keystone
2338,prohibited_implied_role,A list of role names which are prohibited from being an implied role.,security,keystone
2339,methods,"Allowed authentication methods. Note: You should disable the external auth method if you are currently using federation. External auth and  federation both use the REMOTE_USER variable. Since both the mapped and  external plugin are being invoked to validate attributes in the request  environment, it can cause conflicts.",security,keystone
2340,password,Entry point for the password auth plugin module in the keystone.auth.password namespace. You do not need to set this unless you are overriding keystone's own password authentication plugin.,security,keystone
2341,token,Entry point for the token auth plugin module in the keystone.auth.token namespace. You do not need to set this unless you are overriding keystone's own token authentication plugin.,security,keystone
2342,external,"Entry point for the external (REMOTE_USER) auth plugin module in the keystone.auth.external namespace. Supplied drivers are DefaultDomain and Domain. The default driver is DefaultDomain, which assumes that all users identified by the username specified to keystone in the REMOTE_USER variable exist within the context of the default domain. The Domain option expects an additional environment variable be presented to keystone, REMOTE_DOMAIN, containing the domain name of the REMOTE_USER (if REMOTE_DOMAIN is not set, then the default domain will be used instead). You do not  need to set this unless you are taking advantage of 'external  authentication', where the application server (such as Apache) is  handling authentication instead of keystone.",security,keystone
2343,oauth1,Entry point for the OAuth 1.0a auth plugin module in the keystone.auth.oauth1 namespace. You do not need to set this unless you are overriding keystone's own oauth1 authentication plugin.,security,keystone
2344,mapped,Entry point for the mapped auth plugin module in the keystone.auth.mapped namespace. You do not need to set this unless you are overriding keystone's own mapped authentication plugin.,security,keystone
2345,application_credential,Entry point for the application_credential auth plugin module in the keystone.auth.application_credential namespace. You do not need to set this unless you are overriding keystone's own application_credential authentication plugin.,security,keystone
2346,config_prefix,Prefix for building the configuration dictionary for the cache  region. This should not need to be changed unless there is another  dogpile.cache region with the same configuration name.,others,keystone
2347,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache  region. This applies to any cached method that doesn't have an explicit  cache expiration time defined for it.",reliability,keystone
2348,backend,"Cache backend module. For eventlet-based or environments with  hundreds of threaded servers, Memcache with pooling  (oslo_cache.memcache_pool) is recommended. For environments with less  than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis  (dogpile.cache.redis) is recommended. Test environments with a single  instance of the server can use the dogpile.cache.memory backend.",environment,keystone
2349,backend_argument,Arguments supplied to the backend module. Specify this option once  per argument to be passed to the dogpile.cache backend. Example format:  '<argname>:<value>'.,others,keystone
2350,proxies,Proxy classes to import that will affect the way the dogpile.cache  backend functions. See the dogpile.cache documentation on  changing-backend-behavior.,security,keystone
2351,enabled,Global toggle for caching.,performance,keystone
2352,debug_cache_backend,"Extra debugging from the cache backend (cache keys,  get/set/delete/etc calls). This is only really useful if you need to see the specific cache-backend get/set/delete calls with the keys/values.   Typically this should be left set to false.",debuggability,keystone
2353,memcache_servers,Memcache servers in the format of 'host:port'. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,environment,keystone
2354,memcache_dead_retry,Number of seconds memcached server is considered dead before it is  tried again. (dogpile.cache.memcache and oslo_cache.memcache_pool  backends only).,reliability,keystone
2355,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,reliability,keystone
2356,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,performance,keystone
2357,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the  pool before it is closed. (oslo_cache.memcache_pool backend only).,reliability,keystone
2358,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,reliability,keystone
2359,template_file,Absolute path to the file used for the templated catalog backend. This option is only used if the [catalog] driver is set to templated.,environment,keystone
2360,driver,"Entry point for the catalog driver in the keystone.catalog namespace. Keystone provides a sql option (which supports basic CRUD operations through SQL), a templated option (which loads the catalog from a templated catalog file on disk), and a endpoint_filter.sql option (which supports arbitrary service catalogs per project).",environment,keystone
2361,caching,"Toggle for catalog caching. This has no effect unless global caching  is enabled. In a typical deployment, there is no reason to disable this.",performance,keystone
2362,cache_time,"Time to cache catalog data (in seconds). This has no effect unless  global and catalog caching are both enabled. Catalog data (services,  endpoints, etc.) typically does not change frequently, and so a longer  duration than the global default may be desirable.",performance,keystone
2363,list_limit,"Maximum number of entities that will be returned in a catalog  collection. There is typically no reason to set this, as it would be  unusual for a deployment to have enough services or endpoints to exceed a reasonable limit.",performance,keystone
2364,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests 'origin' header. Format:  '<protocol>://<host>[:<port>]', no trailing slash.  Example: https://horizon.example.com",others,keystone
2365,allow_credentials,Indicate that the actual request can include user credentials,security,keystone
2366,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,security,keystone
2367,max_age,Maximum cache age of CORS preflight requests.,performance,keystone
2368,allow_methods,Indicate which methods can be used during the actual request.,others,keystone
2369,allow_headers,Indicate which header field names may be used during the actual request.,others,keystone
2370,driver,"Entry point for the credential backend driver in the keystone.credential namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",environment,keystone
2371,provider,"Entry point for credential encryption and decryption operations in the keystone.credential.provider namespace. Keystone only provides a fernet driver, so there's no reason to change this unless you are providing a custom entry point to encrypt and decrypt credentials.",security,keystone
2372,key_repository,Directory containing Fernet keys used to encrypt and decrypt  credentials stored in the credential backend. Fernet keys used to  encrypt credentials have no relationship to Fernet keys used to encrypt  Fernet tokens. Both sets of keys should be managed separately and  require different rotation policies. Do not share this repository with  the repository used to manage keys for Fernet tokens.,security,keystone
2373,auth_ttl,The length of time in minutes for which a signed EC2 or S3 token  request is valid from the timestamp contained in the token request.,security,keystone
2374,sqlite_synchronous,"If True, SQLite uses synchronous mode.",reliability,keystone
2375,backend,The back end to use for the database.,environment,keystone
2376,connection,The SQLAlchemy connection string to use to connect to the database.,others,keystone
2377,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,environment,keystone
2378,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,keystone
2379,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",others,keystone
2380,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,keystone
2381,min_pool_size,Minimum number of SQL connections to keep open in a pool.,performance,keystone
2382,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,reliability,keystone
2383,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,keystone
2384,retry_interval,Interval between retries of opening a SQL connection.,reliability,keystone
2385,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",reliability,keystone
2386,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,keystone
2387,connection_trace,Add Python stack traces to SQL as comment strings.,others,keystone
2388,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,keystone
2389,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,reliability,keystone
2390,db_retry_interval,Seconds between retries of a database transaction.,reliability,keystone
2391,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",reliability,keystone
2392,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",reliability,keystone
2393,db_max_retries,Maximum retries in case of connection error or deadlock error before  error is raised. Set to -1 to specify an infinite retry count.,reliability,keystone
2394,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',others,keystone
2395,driver,"Entry point for the domain-specific configuration driver in the keystone.resource.domain_config namespace. Only a sql option is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",environment,keystone
2396,caching,Toggle for caching of the domain-specific configuration backend. This has no effect unless global caching is enabled. There is normally no  reason to disable this.,performance,keystone
2397,cache_time,"Time-to-live (TTL, in seconds) to cache domain-specific configuration data. This has no effect unless [domain_config] caching is enabled.",performance,keystone
2398,driver,"Entry point for the endpoint filter driver in the keystone.endpoint_filter namespace. Only a sql option is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",environment,keystone
2399,return_all_endpoints_if_no_filter,"This controls keystone's behavior if the configured endpoint filters  do not result in any endpoints for a user + project pair (and therefore a potentially empty service catalog). If set to true, keystone will  return the entire service catalog. If set to false, keystone will return an empty service catalog.",security,keystone
2400,driver,"Entry point for the endpoint policy driver in the keystone.endpoint_policy namespace. Only a sql driver is provided by keystone, so there is no reason to set this unless you are providing a custom entry point.",environment,keystone
2401,public_bind_host,The IP address of the network interface for the public service to listen on.,environment,keystone
2402,public_port,The port number for the public service to listen on.,environment,keystone
2403,admin_bind_host,The IP address of the network interface for the admin service to listen on.,environment,keystone
2404,admin_port,The port number for the admin service to listen on.,environment,keystone
2405,driver,"Entry point for the federation backend driver in the keystone.federation namespace. Keystone only provides a sql driver, so there is no reason to set this option unless you are providing a custom entry point.",environment,keystone
2406,assertion_prefix,Prefix to use when filtering environment variable names for federated assertions. Matched variables are passed into the federated mapping  engine.,environment,keystone
2407,remote_id_attribute,"Value to be used to obtain the entity ID of the Identity Provider from the environment. For mod_shib, this would be Shib-Identity-Provider. For mod_auth_openidc, this could be HTTP_OIDC_ISS. For mod_auth_mellon, this could be MELLON_IDP.",others,keystone
2408,federated_domain_name,An arbitrary domain name that is reserved to allow federated  ephemeral users to have a domain concept. Note that an admin will not be able to create a domain with this name or update an existing domain to  this name. You are not advised to change this value unless you really  have to.,environment,keystone
2409,trusted_dashboard,"A list of trusted dashboard hosts. Before accepting a Single Sign-On  request to return a token, the origin host must be a member of this  list. This configuration option may be repeated for multiple values. You must set this in order to use web-based SSO flows. For example:  trusted_dashboard=https://acme.example.com/auth/websso  trusted_dashboard=https://beta.example.com/auth/websso",security,keystone
2410,sso_callback_template,"Absolute path to an HTML file used as a Single Sign-On callback  handler. This page is expected to redirect the user from keystone back  to a trusted dashboard host, by form encoding a token in a POST request. Keystone's default value should be sufficient for most deployments.",environment,keystone
2411,caching,Toggle for federation caching. This has no effect unless global  caching is enabled. There is typically no reason to disable this.,performance,keystone
2412,key_repository,"Directory containing Fernet receipt keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The  repository may contain keys in one of three states: a single staged key  (always index 0) used for receipt validation, a single primary key  (always the highest index) used for receipt creation and validation, and any number of secondary keys (all other index values) used for receipt  validation. With multiple keystone nodes, each node must share the same  key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on  any one node to promote a staged key (index 0) to be the new primary  (incremented from the previous highest index), and produce a new staged  key (a new key with index 0); the resulting repository can then be  atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory  on host A, unpack it on host B to a temporary location, and atomically  move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in receipts that can not be validated by all nodes.",environment,keystone
2413,max_active_keys,"This controls how many keys are held in rotation by keystone-manage fernet_rotate before they are discarded. The default value of 3 means that keystone  will maintain one staged key (always index 0), one primary key (the  highest numerical index), and one secondary key (every other index).  Increasing this value means that additional secondary keys will be kept  in the rotation.",reliability,keystone
2414,key_repository,"Directory containing Fernet token keys. This directory must exist before using keystone-manage fernet_setup for the first time, must be writable by the user running keystone-manage fernet_setup or keystone-manage fernet_rotate, and of course must be readable by keystone's server process. The  repository may contain keys in one of three states: a single staged key  (always index 0) used for token validation, a single primary key (always the highest index) used for token creation and validation, and any  number of secondary keys (all other index values) used for token  validation. With multiple keystone nodes, each node must share the same  key repository contents, with the exception of the staged key (index 0). It is safe to run keystone-manage fernet_rotate once on  any one node to promote a staged key (index 0) to be the new primary  (incremented from the previous highest index), and produce a new staged  key (a new key with index 0); the resulting repository can then be  atomically replicated to other nodes without any risk of race conditions (for example, it is safe to run keystone-manage fernet_rotate on host A, wait any amount of time, create a tarball of the directory  on host A, unpack it on host B to a temporary location, and atomically  move (mv) the directory into place on host B). Running keystone-manage fernet_rotate twice on a key repository without syncing other nodes will result in tokens that can not be validated by all nodes.",environment,keystone
2415,max_active_keys,"This controls how many keys are held in rotation by keystone-manage fernet_rotate before they are discarded. The default value of 3 means that keystone  will maintain one staged key (always index 0), one primary key (the  highest numerical index), and one secondary key (every other index).  Increasing this value means that additional secondary keys will be kept  in the rotation.",reliability,keystone
2416,path,The path to respond to healtcheck requests on.,environment,keystone
2417,detailed,Show more detailed information as part of the response. Security  note: Enabling this option may expose sensitive details about the  service being monitored. Be sure to verify that it will not violate your security policies.,security,keystone
2418,backends,Additional backends that can perform health checks and report that information back as part of a request.,debuggability,keystone
2419,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,reliability,keystone
2420,disable_by_file_paths,Check the presence of a file based on a port to determine if an  application is running on a port. Expects a 'port:path' list of strings. Used by DisableByFilesPortsHealthcheck plugin.,reliability,keystone
2421,default_domain_id,"This references the domain to use for all Identity API v2 requests  (which are not aware of domains). A domain with this ID can optionally  be created for you by keystone-manage bootstrap. The domain referenced by this ID cannot be deleted on the v3 API, to prevent  accidentally breaking the v2 API. There is nothing special about this  domain, other than the fact that it must exist to order to maintain  support for your v2 clients. There is typically no reason to change this value.",environment,keystone
2422,domain_specific_drivers_enabled,"A subset (or all) of domains can have their own identity driver, each with their own partial configuration options, stored in either the  resource backend or in a file in a domain configuration directory  (depending on the setting of [identity] domain_configurations_from_database). Only values specific to the domain need to be specified in this manner. This feature is disabled by default, but may be enabled by default in a future release; set to true to enable.",environment,keystone
2423,domain_configurations_from_database,"By default, domain-specific configuration data is read from files in the directory identified by [identity] domain_config_dir. Enabling this configuration option allows you to instead manage  domain-specific configurations through the API, which are then persisted in the backend (typically, a SQL database), rather than using  configuration files on disk.",others,keystone
2424,domain_config_dir,Absolute path where keystone should locate domain-specific [identity] configuration files. This option has no effect unless [identity] domain_specific_drivers_enabled is set to true. There is typically no reason to change this value.,environment,keystone
2425,driver,"Entry point for the identity backend driver in the keystone.identity namespace. Keystone provides a sql and ldap driver. This option is also used as the default driver selection (along with the other configuration variables in this section) in the event  that [identity] domain_specific_drivers_enabled is enabled, but no applicable domain-specific configuration is defined for the  domain in question. Unless your deployment primarily relies on ldap AND is not using domain-specific configuration, you should typically leave this set to sql.",environment,keystone
2426,caching,Toggle for identity caching. This has no effect unless global caching is enabled. There is typically no reason to disable this.,performance,keystone
2427,cache_time,Time to cache identity data (in seconds). This has no effect unless global and identity caching are enabled.,performance,keystone
2428,max_password_length,Maximum allowed length for user passwords. Decrease this value to  improve performance. Changing this value does not effect existing  passwords.,security,keystone
2429,list_limit,Maximum number of entities that will be returned in an identity collection.,performance,keystone
2430,password_hash_algorithm,The password hashing algorithm to use for passwords stored within keystone.,security,keystone
2431,password_hash_rounds,"This option represents a trade off between security and performance.  Higher values lead to slower performance, but higher security. Changing  this option will only affect newly created passwords as existing  password hashes already have a fixed number of rounds applied, so it is  safe to tune this option in a running cluster.  The default for bcrypt  is 12, must be between 4 and 31, inclusive.  The default for scrypt is  16, must be within range(1,32).  The default for pbkdf_sha512 is 60000, must be within range(1,1<<32)  WARNING: If using scrypt, increasing this value increases BOTH time AND memory requirements to hash a password.",security,keystone
2432,scrypt_block_size,Optional block size to pass to scrypt hash function (the r parameter). Useful for tuning scrypt to optimal performance for your CPU architecture. This option is only used when the password_hash_algorithm option is set to scrypt. Defaults to 8.,performance,keystone
2433,scrypt_parallelism,Optional parallelism to pass to scrypt hash function (the p parameter). This option is only used when the password_hash_algorithm option is set to scrypt. Defaults to 1.,security,keystone
2434,salt_bytesize,Number of bytes to use in scrypt and pbkfd2_sha512 hashing salt.   Default for scrypt is 16 bytes. Default for pbkfd2_sha512 is 16 bytes.   Limited to a maximum of 96 bytes due to the size of the column used to  store password hashes.,performance,keystone
2435,driver,"Entry point for the identity mapping backend driver in the keystone.identity.id_mapping namespace. Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",environment,keystone
2436,generator,"Entry point for the public ID generator for user and group entities in the keystone.identity.id_generator namespace. The Keystone identity mapper only supports generators that produce 64 bytes or less. Keystone only provides a sha256 entry point, so there is no reason to change this value unless you're providing a custom entry point.",others,keystone
2437,backward_compatible_ids,"The format of user and group IDs changed in Juno for backends that do not generate UUIDs (for example, LDAP), with keystone providing a hash  mapping to the underlying attribute in LDAP. By default this mapping is  disabled, which ensures that existing IDs will not change. Even when the mapping is enabled by using domain-specific drivers ([identity] domain_specific_drivers_enabled), any users and groups from the default domain being handled by LDAP will still not be mapped to ensure their IDs remain backward compatible.  Setting this value to false will enable the new mapping for all  backends, including the default LDAP driver. It is only guaranteed to be safe to enable this option if you do not already have assignments for  users and groups from the default LDAP domain, and you consider it to be acceptable for Keystone to provide the different IDs to clients than it did previously (existing IDs in the API will suddenly change).  Typically this means that the only time you can set this value to false  is when configuring a fresh installation, although that is the  recommended value.",reliability,keystone
2438,jws_public_key_repository,Directory containing public keys for validating JWS token signatures. This directory must exist in order for keystone's server process to  start. It must also be readable by keystone's server process. It must  contain at least one public key that corresponds to a private key in keystone.conf [jwt_tokens] jws_private_key_repository. This option is only applicable in deployments issuing JWS tokens and setting keystone.conf [tokens] provider = jws.,security,keystone
2439,jws_private_key_repository,"Directory containing private keys for signing JWS tokens. This  directory must exist in order for keystone's server process to start. It must also be readable by keystone's server process. It must contain at  least one private key that corresponds to a public key in keystone.conf [jwt_tokens] jws_public_key_repository. In the event there are multiple private keys in this directory, keystone will use a key named private.pem to sign tokens. In the future, keystone may support the ability to sign tokens with multiple private keys. For now, only a key named private.pem within this directory is required to issue JWS tokens. This option is  only applicable in deployments issuing JWS tokens and setting keystone.conf [tokens] provider = jws.",security,keystone
2440,url,URL(s) for connecting to the LDAP server. Multiple LDAP URLs may be  specified as a comma separated string. The first URL to successfully  bind is used for the connection.,environment,keystone
2441,user,"The user name of the administrator bind DN to use when querying the LDAP server, if your LDAP server requires it.",environment,keystone
2442,password,"The password of the administrator bind DN to use when querying the LDAP server, if your LDAP server requires it.",security,keystone
2443,suffix,"The default LDAP server suffix to use, if a DN is not defined via either [ldap] user_tree_dn or [ldap] group_tree_dn.",others,keystone
2444,query_scope,"The search scope which defines how deep to search within the search base. A value of one (representing oneLevel or singleLevel) indicates a search of objects immediately below to the base object, but does not include the base object itself. A value of sub (representing subtree or wholeSubtree) indicates a search of both the base object itself and the entire subtree below it.",others,keystone
2445,page_size,Defines the maximum number of results per page that keystone should  request from the LDAP server when listing objects. A value of zero (0) disables paging.,performance,keystone
2446,alias_dereferencing,The LDAP dereferencing option to use for queries involving aliases. A value of default falls back to using default dereferencing behavior configured by your ldap.conf. A value of never prevents aliases from being dereferenced at all. A value of searching dereferences aliases only after name resolution. A value of finding dereferences aliases only during name resolution. A value of always dereferences aliases in all cases.,reliability,keystone
2447,debug_level,"Sets the LDAP debugging level for LDAP calls. A value of 0 means that debugging is not enabled. This value is a bitmask, consult your LDAP  documentation for possible values.",debuggability,keystone
2448,chase_referrals,"Sets keystone's referral chasing behavior across directory  partitions. If left unset, the system's default behavior will be used.",others,keystone
2449,user_tree_dn,The search base to use for users. Defaults to the [ldap] suffix value.,others,keystone
2450,user_filter,The LDAP search filter to use for users.,others,keystone
2451,user_objectclass,The LDAP object class to use for users.,others,keystone
2452,user_id_attribute,The LDAP attribute mapped to user IDs in keystone. This must NOT be a multivalued attribute. User IDs are expected to be globally unique  across keystone domains and URL-safe.,others,keystone
2453,user_name_attribute,The LDAP attribute mapped to user names in keystone. User names are  expected to be unique only within a keystone domain and are not expected to be URL-safe.,others,keystone
2454,user_description_attribute,The LDAP attribute mapped to user descriptions in keystone.,others,keystone
2455,user_mail_attribute,The LDAP attribute mapped to user emails in keystone.,others,keystone
2456,user_pass_attribute,The LDAP attribute mapped to user passwords in keystone.,security,keystone
2457,user_enabled_attribute,"The LDAP attribute mapped to the user enabled attribute in keystone. If setting this option to userAccountControl, then you may be interested in setting [ldap] user_enabled_mask and [ldap] user_enabled_default as well.",others,keystone
2458,user_enabled_invert,Logically negate the boolean value of the enabled attribute obtained  from the LDAP server. Some LDAP servers use a boolean lock attribute  where 'true' means an account is disabled. Setting [ldap] user_enabled_invert = true will allow these lock attributes to be used. This option will have no effect if either the [ldap] user_enabled_mask or [ldap] user_enabled_emulation options are in use.,others,keystone
2459,user_enabled_mask,Bitmask integer to select which bit indicates the enabled value if  the LDAP server represents 'enabled' as a bit on an integer rather than  as a discrete boolean. A value of 0 indicates that the mask is not used. If this is not set to 0 the typical value is 2. This is typically used when [ldap] user_enabled_attribute = userAccountControl. Setting this option causes keystone to ignore the value of [ldap] user_enabled_invert.,others,keystone
2460,user_enabled_default,"The default value to enable users. This should match an appropriate  integer value if the LDAP server uses non-boolean (bitmask) values to  indicate if a user is enabled or disabled. If this is not set to True, then the typical value is 512. This is typically used when [ldap] user_enabled_attribute = userAccountControl.",others,keystone
2461,user_attribute_ignore,"List of user attributes to ignore on create and update, or whether a  specific user attribute should be filtered for list or show user.",others,keystone
2462,user_default_project_id_attribute,The LDAP attribute mapped to a user's default_project_id in keystone. This is most commonly used when keystone has write access to LDAP.,others,keystone
2463,user_enabled_emulation,"If enabled, keystone uses an alternative method to determine if a  user is enabled or not by checking if they are a member of the group  defined by the [ldap] user_enabled_emulation_dn option. Enabling this option causes keystone to ignore the value of [ldap] user_enabled_invert.",performance,keystone
2464,user_enabled_emulation_dn,DN of the group entry to hold enabled users when using enabled emulation. Setting this option has no effect unless [ldap] user_enabled_emulation is also enabled.,performance,keystone
2465,user_enabled_emulation_use_group_config,Use the [ldap] group_member_attribute and [ldap] group_objectclass settings to determine membership in the emulated enabled group. Enabling this option has no effect unless [ldap] user_enabled_emulation is also enabled.,performance,keystone
2466,user_additional_attribute_mapping,"A list of LDAP attribute to keystone user attribute pairs used for  mapping additional attributes to users in keystone. The expected format  is <ldap_attr>:<user_attr>, where ldap_attr is the attribute in the LDAP object and user_attr is the attribute which should appear in the identity API.",others,keystone
2467,group_tree_dn,The search base to use for groups. Defaults to the [ldap] suffix value.,others,keystone
2468,group_filter,The LDAP search filter to use for groups.,others,keystone
2469,group_objectclass,"The LDAP object class to use for groups. If setting this option to posixGroup, you may also be interested in enabling the [ldap] group_members_are_ids option.",others,keystone
2470,group_id_attribute,The LDAP attribute mapped to group IDs in keystone. This must NOT be a multivalued attribute. Group IDs are expected to be globally unique  across keystone domains and URL-safe.,others,keystone
2471,group_name_attribute,The LDAP attribute mapped to group names in keystone. Group names are expected to be unique only within a keystone domain and are not  expected to be URL-safe.,others,keystone
2472,group_member_attribute,The LDAP attribute used to indicate that a user is a member of the group.,others,keystone
2473,group_members_are_ids,Enable this option if the members of the group object class are  keystone user IDs rather than LDAP DNs. This is the case when using posixGroup as the group object class in Open Directory.,others,keystone
2474,group_desc_attribute,The LDAP attribute mapped to group descriptions in keystone.,others,keystone
2475,group_attribute_ignore,List of group attributes to ignore on create and update. or whether a specific group attribute should be filtered for list or show group.,others,keystone
2476,group_additional_attribute_mapping,"A list of LDAP attribute to keystone group attribute pairs used for  mapping additional attributes to groups in keystone. The expected format is <ldap_attr>:<group_attr>, where ldap_attr is the attribute in the LDAP object and group_attr is the attribute which should appear in the identity API.",others,keystone
2477,group_ad_nesting,"If enabled, group queries will use Active Directory specific filters for nested groups.",security,keystone
2478,tls_cacertfile,"An absolute path to a CA certificate file to use when communicating with LDAP servers. This option will take precedence over [ldap] tls_cacertdir, so there is no reason to set both.",environment,keystone
2479,tls_cacertdir,An absolute path to a CA certificate directory to use when  communicating with LDAP servers. There is no reason to set this option  if you've also set [ldap] tls_cacertfile.,environment,keystone
2480,use_tls,Enable TLS when communicating with LDAP servers. You should also set the [ldap] tls_cacertfile and [ldap] tls_cacertdir options when using this option. Do not set this option if you are using LDAP over SSL (LDAPS) instead of TLS.,security,keystone
2481,tls_req_cert,"Specifies which checks to perform against client certificates on incoming TLS sessions. If set to demand, then a certificate will always be requested and required from the LDAP server. If set to allow, then a certificate will always be requested but not required from the LDAP server. If set to never, then a certificate will never be requested.",security,keystone
2482,connection_timeout,The connection timeout to use with the LDAP server. A value of -1 means that connections will never timeout.,reliability,keystone
2483,use_pool,Enable LDAP connection pooling for queries to the LDAP server. There is typically no reason to disable this.,performance,keystone
2484,pool_size,The size of the LDAP connection pool. This option has no effect unless [ldap] use_pool is also enabled.,performance,keystone
2485,pool_retry_max,The maximum number of times to attempt reconnecting to the LDAP  server before aborting. A value of zero prevents retries. This option  has no effect unless [ldap] use_pool is also enabled.,reliability,keystone
2486,pool_retry_delay,The number of seconds to wait before attempting to reconnect to the LDAP server. This option has no effect unless [ldap] use_pool is also enabled.,reliability,keystone
2487,pool_connection_timeout,The connection timeout to use when pooling LDAP connections. A value of -1 means that connections will never timeout. This option has no effect unless [ldap] use_pool is also enabled.,reliability,keystone
2488,pool_connection_lifetime,"The maximum connection lifetime to the LDAP server in seconds. When  this lifetime is exceeded, the connection will be unbound and removed  from the connection pool. This option has no effect unless [ldap] use_pool is also enabled.",reliability,keystone
2489,use_auth_pool,Enable LDAP connection pooling for end user authentication. There is typically no reason to disable this.,security,keystone
2490,auth_pool_size,The size of the connection pool to use for end user authentication. This option has no effect unless [ldap] use_auth_pool is also enabled.,security,keystone
2491,auth_pool_connection_lifetime,"The maximum end user authentication connection lifetime to the LDAP  server in seconds. When this lifetime is exceeded, the connection will  be unbound and removed from the connection pool. This option has no  effect unless [ldap] use_auth_pool is also enabled.",security,keystone
2492,dead_retry,Number of seconds memcached server is considered dead before it is tried again. This is used by the key value store system.,reliability,keystone
2493,socket_timeout,Timeout in seconds for every call to a server. This is used by the key value store system.,reliability,keystone
2494,pool_maxsize,Max total number of open connections to every memcached server. This is used by the key value store system.,performance,keystone
2495,pool_unused_timeout,Number of seconds a connection to memcached is held unused in the  pool before it is closed. This is used by the key value store system.,reliability,keystone
2496,pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection. This is used by the key value store system.,reliability,keystone
2497,driver,"Entry point for the OAuth backend driver in the keystone.oauth1 namespace. Typically, there is no reason to set this option unless you are providing a custom entry point.",environment,keystone
2498,request_token_duration,Number of seconds for the OAuth Request Token to remain valid after  being created. This is the amount of time the user has to authorize the  token. Setting this option to zero means that request tokens will last  forever.,reliability,keystone
2499,access_token_duration,Number of seconds for the OAuth Access Token to remain valid after  being created. This is the amount of time the consumer has to interact  with the service provider (which is typically keystone). Setting this  option to zero means that access tokens will last forever.,reliability,keystone
2500,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,environment,keystone
2501,idle_timeout,Timeout for inactive connections (in seconds),reliability,keystone
2502,trace,Debug: dump AMQP frames to stdout,debuggability,keystone
2503,ssl,"Attempt to connect via SSL. If no other ssl-related parameters are  given, it will use the system's CA-bundle to verify the server's  certificate.",security,keystone
2504,ssl_ca_file,CA certificate PEM file used to verify the server's certificate,security,keystone
2505,ssl_cert_file,Self-identifying certificate PEM file for client authentication,security,keystone
2506,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),security,keystone
2507,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),security,keystone
2508,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate  matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the  server uses the Server Name Indication TLS extension (rfc6066) to  provide a certificate per virtual host. Set ssl_verify_vhost to True if  the server's SSL certificate uses the virtual host name instead of the  DNS name.",security,keystone
2509,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,security,keystone
2510,sasl_config_dir,Path to directory that contains the SASL configuration,environment,keystone
2511,sasl_config_name,Name of configuration file (without .conf suffix),security,keystone
2512,sasl_default_realm,SASL realm to use if no realm present in username,security,keystone
2513,connection_retry_interval,Seconds to pause before attempting to re-connect.,reliability,keystone
2514,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,reliability,keystone
2515,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,reliability,keystone
2516,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,reliability,keystone
2517,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,reliability,keystone
2518,default_reply_timeout,The deadline for an rpc reply message delivery.,reliability,keystone
2519,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,reliability,keystone
2520,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,reliability,keystone
2521,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,reliability,keystone
2522,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy'   - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic'  - use legacy addresses if the message bus does not support routing otherwise use routable addressing,security,keystone
2523,pseudo_vhost,"Enable virtual host support for those message buses that do not  natively support virtual hosting (such as qpidd). When set to true the  virtual host name will be added to all message bus addresses,  effectively creating a private 'subnet' per virtual host. Set to False  if the message bus supports virtual hosting using the 'hostname' field  in the AMQP 1.0 Open performative as the name of the virtual host.",security,keystone
2524,server_request_prefix,address prefix used when sending to a specific server,environment,keystone
2525,broadcast_prefix,address prefix used when broadcasting to all servers,environment,keystone
2526,group_request_prefix,address prefix when sending to any server in group,environment,keystone
2527,rpc_address_prefix,Address prefix for all generated RPC addresses,environment,keystone
2528,notify_address_prefix,Address prefix for all generated Notification addresses,environment,keystone
2529,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,environment,keystone
2530,unicast_address,Appended to the address prefix when sending to a particular  RPC/Notification server. Used by the message bus to identify messages  sent to a single destination.,environment,keystone
2531,anycast_address,Appended to the address prefix when sending to a group of consumers.  Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,environment,keystone
2532,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',environment,keystone
2533,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',environment,keystone
2534,reply_link_credit,Window size for incoming RPC Reply messages.,performance,keystone
2535,rpc_server_credit,Window size for incoming RPC Request messages,performance,keystone
2536,notify_server_credit,Window size for incoming Notification messages,performance,keystone
2537,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify'   - Send Notifications pre-settled,others,keystone
2538,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,performance,keystone
2539,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,reliability,keystone
2540,pool_size,Pool Size for Kafka Consumers,performance,keystone
2541,conn_pool_min_size,The pool size limit for connections expiration policy,performance,keystone
2542,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,keystone
2543,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,others,keystone
2544,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,reliability,keystone
2545,producer_batch_size,Size of batch for the producer async send,performance,keystone
2546,enable_auto_commit,Enable asynchronous consumer commits,reliability,keystone
2547,max_poll_records,The maximum number of records returned in a poll call,performance,keystone
2548,security_protocol,Protocol used to communicate with brokers,security,keystone
2549,sasl_mechanism,Mechanism when security protocol is SASL,security,keystone
2550,ssl_cafile,CA certificate PEM file used to verify the server certificate,security,keystone
2551,driver,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",environment,keystone
2552,transport_url,"A URL representing the messaging driver to use for notifications. If  not set, we fall back to the same configuration used for RPC.",environment,keystone
2553,topics,AMQP topic used for OpenStack notifications.,others,keystone
2554,retry,"The maximum number of attempts to re-send a notification message  which failed to be delivered due to a recoverable error. 0 - No retry,  -1 - indefinite",reliability,keystone
2555,amqp_durable_queues,Use durable queues in AMQP.,performance,keystone
2556,amqp_auto_delete,Auto-delete queues in AMQP.,performance,keystone
2557,ssl,Connect over SSL.,security,keystone
2558,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",security,keystone
2559,ssl_key_file,SSL key file (valid only if SSL enabled).,security,keystone
2560,ssl_cert_file,SSL cert file (valid only if SSL enabled).,security,keystone
2561,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,security,keystone
2562,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,reliability,keystone
2563,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression  will not be used. This option may not be available in future versions.",performance,keystone
2564,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its  replies. This value should not be longer than rpc_response_timeout.,reliability,keystone
2565,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we  are currently connected to becomes unavailable. Takes effect only if  more than one RabbitMQ node is provided in config.,reliability,keystone
2566,rabbit_login_method,The RabbitMQ login method.,others,keystone
2567,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,reliability,keystone
2568,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,reliability,keystone
2569,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,reliability,keystone
2570,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change  this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when  declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run:  'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",reliability,keystone
2571,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL  (x-expires). Queues which are unused for the duration of the TTL are  automatically deleted. The parameter affects only reply and fanout  queues.,reliability,keystone
2572,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,reliability,keystone
2573,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,reliability,keystone
2574,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,reliability,keystone
2575,max_request_body_size,"The maximum body size for each  request, in bytes.",performance,keystone
2576,secure_proxy_ssl_header,"The HTTP Header that will be used to determine what the original  request protocol scheme was, even if it was hidden by a SSL termination  proxy.",security,keystone
2577,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,others,keystone
2578,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",security,keystone
2579,policy_file,The file that defines policies.,others,keystone
2580,policy_default_rule,Default rule. Enforced when a requested rule is not found.,security,keystone
2581,policy_dirs,"Directories where policy configuration files are stored. They can be  relative to any directory in the search path defined by the config_dir  option, or absolute paths. The file defined by policy_file must exist  for these directories to be searched.  Missing or empty directories are  ignored.",environment,keystone
2582,remote_content_type,Content Type to send and receive data for REST based policy check,others,keystone
2583,remote_ssl_verify_server_crt,server identity verification for REST based policy check,security,keystone
2584,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,environment,keystone
2585,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,environment,keystone
2586,remote_ssl_client_key_file,Absolute path client key file REST based policy check,environment,keystone
2587,driver,"Entry point for the policy backend driver in the keystone.policy namespace. Supplied drivers are rules (which does not support any CRUD operations for the v3 policy API) and sql. Typically, there is no reason to set this option unless you are providing a custom entry point.",environment,keystone
2588,list_limit,Maximum number of entities that will be returned in a policy collection.,performance,keystone
2589,enabled,Enable the profiling for all services on this node.,debuggability,keystone
2590,trace_sqlalchemy,Enable SQL requests profiling in services.,debuggability,keystone
2591,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,security,keystone
2592,connection_string,Connection string for a notifier backend.,environment,keystone
2593,es_doc_type,Document type for notification indexing in elasticsearch.,others,keystone
2594,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",reliability,keystone
2595,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,performance,keystone
2596,socket_timeout,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,reliability,keystone
2597,sentinel_service_name,Redissentinel uses a service name to identify a master redis service. This parameter defines the name (for example: sentinal_service_name=mymaster).,environment,keystone
2598,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,debuggability,keystone
2599,expiration,"The amount of time that a receipt should remain valid (in seconds).  This value should always be very short, as it represents how long a user has to reattempt auth with the missing auth methods.",reliability,keystone
2600,provider,"Entry point for the receipt provider in the keystone.receipt.provider namespace. The receipt provider controls the receipt construction and validation operations. Keystone includes just the fernet receipt provider for now. fernet receipts do not need to be persisted at all, but require that you run keystone-manage fernet_setup (also see the keystone-manage fernet_rotate command).",security,keystone
2601,caching,"Toggle for caching receipt creation and validation data. This has no  effect unless global caching is enabled, or if cache_on_issue is  disabled as we only cache receipts on issue.",performance,keystone
2602,cache_time,The number of seconds to cache receipt creation and validation data. This has no effect unless both global and [receipt] caching are enabled.,performance,keystone
2603,cache_on_issue,Enable storing issued receipt data to receipt validation cache so  that first receipt validation doesn't actually cause full validation  cycle. This option has no effect unless global caching and receipt  caching are enabled.,performance,keystone
2604,driver,"Entry point for the resource driver in the keystone.resource namespace. Only a sql driver is supplied by keystone. Unless you are writing proprietary drivers for keystone, you do not need to set this option.",environment,keystone
2605,caching,Toggle for resource caching. This has no effect unless global caching is enabled.,performance,keystone
2606,cache_time,Time to cache resource data in seconds. This has no effect unless global caching is enabled.,performance,keystone
2607,list_limit,Maximum number of entities that will be returned in a resource collection.,performance,keystone
2608,admin_project_domain_name,"Name of the domain that owns the admin_project_name. If left unset, then there is no admin project. [resource] admin_project_name must also be set to use this option.",environment,keystone
2609,admin_project_name,"This is a special project which represents cloud-level administrator  privileges across services. Tokens scoped to this project will contain a true is_admin_project attribute to indicate to policy  systems that the role assignments on that specific project should apply  equally across every project. If left unset, then there is no admin  project, and thus no explicit means of cross-project role assignments. [resource] admin_project_domain_name must also be set to use this option.",environment,keystone
2610,project_name_url_safe,"This controls whether the names of projects are restricted from containing URL-reserved characters. If set to new, attempts to create or update a project with a URL-unsafe name will fail. If set to strict, attempts to scope a token with a URL-unsafe project name will fail,  thereby forcing all project names to be updated to be URL-safe.",security,keystone
2611,domain_name_url_safe,"This controls whether the names of domains are restricted from containing URL-reserved characters. If set to new, attempts to create or update a domain with a URL-unsafe name will fail. If set to strict, attempts to scope a token with a URL-unsafe domain name will fail,  thereby forcing all domain names to be updated to be URL-safe.",security,keystone
2612,driver,"Entry point for the token revocation backend driver in the keystone.revoke namespace. Keystone only provides a sql driver, so there is no reason to set this option unless you are providing a custom entry point.",environment,keystone
2613,expiration_buffer,The number of seconds after a token has expired before a corresponding revocation event may be purged from the backend.,reliability,keystone
2614,caching,Toggle for revocation event caching. This has no effect unless global caching is enabled.,performance,keystone
2615,cache_time,Time to cache the revocation list and the revocation events (in seconds). This has no effect unless global and [revoke] caching are both enabled.,performance,keystone
2616,driver,"Entry point for the role backend driver in the keystone.role namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",environment,keystone
2617,caching,"Toggle for role caching. This has no effect unless global caching is  enabled. In a typical deployment, there is no reason to disable this.",performance,keystone
2618,cache_time,"Time to cache role data, in seconds. This has no effect unless both global caching and [role] caching are enabled.",performance,keystone
2619,list_limit,Maximum number of entities that will be returned in a role  collection. This may be useful to tune if you have a large number of  discrete roles in your deployment.,performance,keystone
2620,assertion_expiration_time,"Determines the lifetime for any SAML assertions generated by keystone, using NotOnOrAfter attributes.",reliability,keystone
2621,xmlsec1_binary,"Name of, or absolute path to, the binary to be used for XML signing. Although only the XML Security Library (xmlsec1) is supported, it may have a non-standard name or path on your system.  If keystone cannot find the binary itself, you may need to install the  appropriate package, use this option to specify an absolute path, or  adjust keystone's PATH environment variable.",environment,keystone
2622,certfile,"Absolute path to the public certificate file to use for SAML signing. The value cannot contain a comma (,).",security,keystone
2623,keyfile,"Absolute path to the private key file to use for SAML signing. The value cannot contain a comma (,).",environment,keystone
2624,idp_entity_id,This is the unique entity identifier of the identity provider  (keystone) to use when generating SAML assertions. This value is  required to generate identity provider metadata and must be a URI (a URL is recommended). For example: https://keystone.example.com/v3/OS-FEDERATION/saml2/idp.,environment,keystone
2625,idp_sso_endpoint,This is the single sign-on (SSO) service location of the identity  provider which accepts HTTP POST requests. A value is required to  generate identity provider metadata. For example: https://keystone.example.com/v3/OS-FEDERATION/saml2/sso.,environment,keystone
2626,idp_lang,This is the language used by the identity provider's organization.,others,keystone
2627,idp_organization_name,This is the name of the identity provider's organization.,others,keystone
2628,idp_organization_display_name,This is the name of the identity provider's organization to be displayed.,others,keystone
2629,idp_organization_url,This is the URL of the identity provider's organization. The URL referenced here should be useful to humans.,environment,keystone
2630,idp_contact_company,This is the company name of the identity provider's contact person.,others,keystone
2631,idp_contact_name,This is the given name of the identity provider's contact person.,others,keystone
2632,idp_contact_surname,This is the surname of the identity provider's contact person.,others,keystone
2633,idp_contact_email,This is the email address of the identity provider's contact person.,environment,keystone
2634,idp_contact_telephone,This is the telephone number of the identity provider's contact person.,environment,keystone
2635,idp_contact_type,This is the type of contact that best describes the identity provider's contact person.,others,keystone
2636,idp_metadata_path,Absolute path to the identity provider metadata file. This file should be generated with the keystone-manage saml_idp_metadata command. There is typically no reason to change this value.,environment,keystone
2637,relay_state_prefix,"The prefix of the RelayState SAML attribute to use when generating  enhanced client and proxy (ECP) assertions. In a typical deployment,  there is no reason to change this value.",security,keystone
2638,disable_user_account_days_inactive,"The maximum number of days a user can go without authenticating  before being considered 'inactive' and automatically disabled (locked).  This feature is disabled by default; set any value to enable it. This  feature depends on the sql backend for the [identity] driver. When a user exceeds this threshold and is considered 'inactive', the user's enabled attribute in the HTTP API may not match the value of the user's enabled column in the user table.",reliability,keystone
2639,lockout_failure_attempts,"The maximum number of times that a user can fail to authenticate  before the user account is locked for the number of seconds specified by [security_compliance] lockout_duration. This feature is disabled by default. If this feature is enabled and [security_compliance] lockout_duration is not set, then users may be locked out indefinitely until the user is explicitly enabled via the API. This feature depends on the sql backend for the [identity] driver.",security,keystone
2640,lockout_duration,The number of seconds a user account will be locked when the maximum number of failed authentication attempts (as specified by [security_compliance] lockout_failure_attempts) is exceeded. Setting this option will have no effect unless you also set [security_compliance] lockout_failure_attempts to a non-zero value. This feature depends on the sql backend for the [identity] driver.,reliability,keystone
2641,password_expires_days,"The number of days for which a password will be considered valid  before requiring it to be changed. This feature is disabled by default.  If enabled, new password changes will have an expiration date, however  existing passwords would not be impacted. This feature depends on the sql backend for the [identity] driver.",reliability,keystone
2642,unique_last_password_count,"This controls the number of previous user password iterations to keep in history, in order to enforce that newly created passwords are  unique. The total number which includes the new password should not be  greater or equal to this value. Setting the value to zero (the default)  disables this feature. Thus, to enable this feature, values must be  greater than 0. This feature depends on the sql backend for the [identity] driver.",security,keystone
2643,minimum_password_age,"The number of days that a password must be used before the user can  change it. This prevents users from changing their passwords immediately in order to wipe out their password history and reuse an old password.  This feature does not prevent administrators from manually resetting  passwords. It is disabled by default and allows for immediate password  changes. This feature depends on the sql backend for the [identity] driver. Note: If [security_compliance] password_expires_days is set, then the value for this option should be less than the password_expires_days.",security,keystone
2644,password_regex,"The regular expression used to validate password strength  requirements. By default, the regular expression will match any  password. The following is an example of a pattern which requires at  least 1 letter, 1 digit, and have a minimum length of 7 characters:  ^(?=.*d)(?=.*[a-zA-Z]).{7,}$ This feature depends on the sql backend for the [identity] driver.",security,keystone
2645,password_regex_description,"Describe your password regular expression here in language for  humans. If a password fails to match the regular expression, the  contents of this configuration variable will be returned to users to  explain why their requested password was insufficient.",security,keystone
2646,change_password_upon_first_use,"Enabling this option requires users to change their password when the user is created, or upon administrative reset. Before accessing any  services, affected users will have to change their password. To ignore  this requirement for specific users, such as service users, set the options attribute ignore_change_password_upon_first_use to True for the desired user via the update user API. This feature is disabled by default. This feature is only applicable with the sql backend for the [identity] driver.",security,keystone
2647,driver,"Entry point for the shadow users backend driver in the keystone.identity.shadow_users namespace. This driver is used for persisting local user references to  externally-managed identities (via federation, LDAP, etc). Keystone only provides a sql driver, so there is no reason to change this option unless you are providing a custom entry point.",environment,keystone
2648,certfile,"Absolute path to the public certificate file to use for signing responses to revocation lists requests. Set this together with [signing] keyfile. For non-production environments, you may be interested in using keystone-manage pki_setup to generate self-signed certificates.",security,keystone
2649,keyfile,Absolute path to the private key file to use for signing responses to revocation lists requests. Set this together with [signing] certfile.,environment,keystone
2650,ca_certs,Absolute path to the public certificate authority (CA) file to use when creating self-signed certificates with keystone-manage pki_setup. Set this together with [signing] ca_key. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,security,keystone
2651,ca_key,Absolute path to the private certificate authority (CA) key file to use when creating self-signed certificates with keystone-manage pki_setup. Set this together with [signing] ca_certs. There is no reason to set this option unless you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,security,keystone
2652,key_size,Key size (in bits) to use when generating a self-signed token signing certificate. There is no reason to set this option unless you are  requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,security,keystone
2653,valid_days,The validity period (in days) to use when generating a self-signed  token signing certificate. There is no reason to set this option unless  you are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,reliability,keystone
2654,cert_subject,The certificate subject to use when generating a self-signed token  signing certificate. There is no reason to set this option unless you  are requesting revocation lists in a non-production environment. Use a [signing] certfile issued from a trusted certificate authority instead.,security,keystone
2655,expiration,"The amount of time that a token should remain valid (in seconds).  Drastically reducing this value may break 'long-running' operations that involve multiple services to coordinate together, and will force users  to authenticate with keystone more frequently. Drastically increasing  this value will increase the number of tokens that will be  simultaneously valid. Keystone tokens are also bearer tokens, so a  shorter duration will also reduce the potential security impact of a  compromised token.",reliability,keystone
2656,provider,"Entry point for the token provider in the keystone.token.provider namespace. The token provider controls the token construction,  validation, and revocation operations. Supported upstream providers are fernet and jws. Neither fernet or jws tokens require persistence and both require additional setup. If using fernet, you're required to run keystone-manage fernet_setup, which creates symmetric keys used to encrypt tokens. If using jws, you're required to generate an ECDSA keypair using a SHA-256 hash  algorithm for signing and validating token, which can be done with keystone-manage create_jws_keypair. Note that fernet tokens are encrypted and jws tokens are only signed. Please be sure to consider this if your  deployment has security requirements regarding payload contents used to  generate token IDs.",security,keystone
2657,caching,Toggle for caching token creation and validation data. This has no effect unless global caching is enabled.,performance,keystone
2658,cache_time,The number of seconds to cache token creation and validation data. This has no effect unless both global and [token] caching are enabled.,performance,keystone
2659,revoke_by_id,This toggles support for revoking individual tokens by the token  identifier and thus various token enumeration operations (such as  listing all tokens issued to a specific user). These operations are used to determine the list of tokens to consider revoked. Do not disable  this option if you're using the kvs [revoke] driver.,security,keystone
2660,allow_rescope_scoped_token,"This toggles whether scoped tokens may be re-scoped to a new project  or domain, thereby preventing users from exchanging a scoped token  (including those with a default project scope) for any other token. This forces users to either authenticate for unscoped tokens (and later  exchange that unscoped token for tokens with a more specific scope) or  to provide their credentials in every request for a scoped token to  avoid re-scoping altogether.",security,keystone
2661,infer_roles,"This controls whether roles should be included with tokens that are  not directly assigned to the token's scope, but are instead linked  implicitly to other role assignments.",security,keystone
2662,cache_on_issue,Enable storing issued token data to token validation cache so that  first token validation doesn't actually cause full validation cycle.  This option has no effect unless global caching is enabled and will  still cache tokens even if [token] caching = False.,performance,keystone
2663,allow_expired_window,This controls the number of seconds that a token can be retrieved for beyond the built-in expiry time. This allows long running operations to succeed. Defaults to two days.,reliability,keystone
2664,trusted_issuer,"The list of distinguished names which identify trusted issuers of  client certificates allowed to use X.509 tokenless authorization. If the option is absent then no certificates will be allowed. The format for  the values of a distinguished name (DN) must be separated by a comma and contain no spaces. Furthermore, because an individual DN may contain  commas, this configuration option may be repeated multiple times to  represent multiple values. For example, keystone.conf would include two  consecutive lines in order to trust two different DNs, such as trusted_issuer = CN=john,OU=keystone,O=openstack and trusted_issuer = CN=mary,OU=eng,O=abc.",security,keystone
2665,protocol,"The federated protocol ID used to represent X.509 tokenless authorization. This is used in combination with the value of [tokenless_auth] issuer_attribute to find a corresponding federated mapping. In a typical deployment, there is no reason to change this value.",security,keystone
2666,issuer_attribute,"The name of the WSGI environment variable used to pass the issuer of  the client certificate to keystone. This attribute is used as an  identity provider ID for the X.509 tokenless authorization along with  the protocol to look up its corresponding mapping. In a typical  deployment, there is no reason to change this value.",environment,keystone
2667,allow_redelegation,"Allows authorization to be redelegated from one user to another, effectively chaining trusts together. When disabled, the remaining_uses attribute of a trust is constrained to be zero.",security,keystone
2668,max_redelegation_count,Maximum number of times that authorization can be redelegated from  one user to another in a chain of trusts. This number may be reduced  further for a specific trust.,reliability,keystone
2669,driver,"Entry point for the trust backend driver in the keystone.trust namespace. Keystone only provides a sql driver, so there is no reason to change this unless you are providing a custom entry point.",environment,keystone
2670,driver,"Entry point for the unified limit backend driver in the keystone.unified_limit namespace. Keystone only provides a sql driver, so there's no reason to change this unless you are providing a custom entry point.",environment,keystone
2671,caching,"Toggle for unified limit caching. This has no effect unless global  caching is enabled. In a typical deployment, there is no reason to  disable this.",performance,keystone
2672,cache_time,"Time to cache unified limit data, in seconds. This has no effect unless both global caching and [unified_limit] caching are enabled.",performance,keystone
2673,list_limit,Maximum number of entities that will be returned in a role  collection. This may be useful to tune if you have a large number of  unified limits in your deployment.,performance,keystone
2674,enforcement_model,"The enforcement model to use when validating limits associated to  projects. Enforcement models will behave differently depending on the  existing limits, which may result in backwards incompatible changes if a model is switched in a running deployment.",performance,keystone
2675,debug_middleware,"If set to true, this enables the oslo debug middleware in Keystone.  This Middleware prints a lot of information about the request and the  response. It is useful for getting information about the data on the  wire (decoded) and passed to the WSGI application pipeline. This  middleware has no effect on the 'debug' setting in the [DEFAULT] section of the config file or setting Keystone's log-level to 'DEBUG'; it is  specific to debugging the WSGI data as it enters and leaves Keystone  (specific request-related data). This option is used for introspection  on the request and response data between the web server (apache, nginx,  etc) and Keystone.  This middleware is inserted as the first element in  the middleware chain and will show the data closest to the wire.   WARNING: NOT INTENDED FOR USE IN PRODUCTION. THIS MIDDLEWARE CAN AND  WILL EMIT SENSITIVE/PRIVILEGED DATA.",debuggability,keystone
2676,block_cache_capacity_mb,block cache capacity in MB,performance,kudu
2677,log_force_fsync_all,Whether the Log/WAL should explicitly call fsync() after each write.,performance,kudu
2678,fs_data_dirs,"Comma-separated list of directories with data blocks. If this is not specified, fs_wal_dir will be used as the sole data block directory",environment,kudu
2679,fs_metadata_dir,Directory with metadata.,environment,kudu
2680,fs_wal_dir,Directory with write-ahead logs.,debuggability,kudu
2681,master_addresses,"Comma-separated list of the RPC addresses belonging to all Masters in this cluster. NOTE: if not specified, configures a non-replicated Master.",environment,kudu
2682,rpc_bind_addresses,Comma-separated list of addresses to bind to for RPC connections.,environment,kudu
2683,keytab_file,Path to the Kerberos Keytab file for this server.,environment,kudu
2684,superuser_acl,"The list of usernames to allow as super users, comma-separated. ",security,kudu
2685,user_acl,"The list of usernames who may access the cluster, comma-separated.",security,kudu
2686,webserver_certificate_file,"The location of the debug webserver's SSL certificate file, in PEM format.",security,kudu
2687,webserver_port,Port to bind to for the web server,environment,kudu
2688,webserver_private_key_file,"The full path to the private key used as a counterpart to the public key contained in --webserver_certificate_file. If --webserver_certificate_file is set, this option must be set as well.",security,kudu
2689,webserver_private_key_password_cmd,"A Unix command whose output returns the password used to decrypt the Webserver's certificate private key file specified in --webserver_private_key_file. If the PEM key file is not password-protected, this flag does not need to be set.",security,kudu
2690,log_filename,Prefix of log filename ,debuggability,kudu
2691,maintenance_manager_num_threads,Size of the maintenance manager thread pool. ,performance,kudu
2692,memory_limit_hard_bytes,"Maximum amount of memory this daemon should use, in bytes.",performance,kudu
2693,version,show version and build info and exit,others,kudu
2694,log_dir,"If specified, logfiles are written into this directory instead of the default logging directory",environment,kudu
2695,enable_process_lifetime_heap_profiling,Enables heap profiling for the lifetime of the process. ,debuggability,kudu
2696,heap_profile_path,Output path to store heap profiles. ,environment,kudu
2697,unlock_unsafe_flags,Unlock flags marked as 'unsafe'.  Use at your own risk.,security,kudu
2698,helpxml,produce an xml version of help,others,kudu
2699,minloglevel,Messages logged at a lower level than this don't actually get logged anywhere,debuggability,kudu
2700,stderrthreshold,log messages at or above this level are copied to stderr in addition to logfiles. ,debuggability,kudu
2701,stop_logging_if_full_disk,Stop attempting to log to disk if the disk is full.,reliability,kudu
2702,symbolize_stacktrace,Symbolize the stack trace in the tombstone,debuggability,kudu
2703,cfile_verify_checksums,Verify the checksum for each block on read if one exists,reliability,kudu
2704,cfile_default_block_size,The default block size to use in cfiles,performance,kudu
2705,cfile_default_compression_codec,Default cfile block compression codec.,performance,kudu
2706,cfile_write_checksums,Write CRC32 checksums for each block,reliability,kudu
2707,max_clock_sync_error_usec,Maximum allowed clock synchronization error as reported by NTP before the server will abort.,reliability,kudu
2708,consensus_rpc_timeout_ms,Timeout used for all consensus internal RPC communications.,reliability,kudu
2709,consensus_max_batch_size_bytes,The maximum per-tablet RPC batch size when updating peers.,performance,kudu
2710,follower_unavailable_considered_failed_sec,Seconds that a leader is unable to successfully heartbeat to a follower after which the follower is considered to be failed and evicted from the config,reliability,kudu
2711,fs_wal_dir_reserved_bytes,Number of bytes to reserve on the log directory filesystem for non-Kudu usage. ,reliability,kudu
2712,global_log_cache_size_limit_mb,The total memory used for caching log entries across all tablets is kept under this threshold.,performance,kudu
2713,log_cache_size_limit_mb,The total per-tablet size of consensus entries which may be kept in memory. ,performance,kudu
2714,log_async_preallocate_segments,Whether the WAL segments preallocation should happen asynchronously,reliability,kudu
2715,log_preallocate_segments,Whether the WAL should preallocate the entire segment before writing to it,performance,kudu
2716,log_segment_size_mb,"The default size for log segments, in MB",performance,kudu
2717,evict_failed_followers,Whether to evict followers from the Raft config ,others,kudu
2718,leader_failure_max_missed_heartbeat_periods,Maximum heartbeat periods that the leader can fail to heartbeat in before we consider the leader to be failed. The total failure timeout in milliseconds is raft_heartbeat_interval_ms times,reliability,kudu
2719,safe_time_advancement_without_writes,"Whether to enable the advancement of ""safe"" time in the absense of write operations",reliability,kudu
2720,block_manager_max_open_files,Maximum number of open file descriptors to be used for data blocks.,reliability,kudu
2721,fs_data_dirs_full_disk_cache_seconds,"Number of seconds we cache the full-disk status in the block manager. During this time, writes to the corresponding root path will not be attempted",reliability,kudu
2722,fs_data_dirs_reserved_bytes,Number of bytes to reserve on each data directory filesystem for non-Kudu usage. ,reliability,kudu
2723,fs_target_data_dirs_per_tablet,Indicates the target number of data dirs to spread each tablet's data across. ,others,kudu
2724,default_num_replicas,Default number of replicas for tables that do not have the num_replicas set.,reliability,kudu
2725,master_ts_rpc_timeout_ms,Timeout used for the master to TS async rpc calls,reliability,kudu
2726,table_locations_ttl_ms,Maximum time in milliseconds which clients may cache table locations.,reliability,kudu
2727,tablet_creation_timeout_ms,Timeout used by the master when attempting to create tablet replicas during table creation.,reliability,kudu
2728,unresponsive_ts_rpc_timeout_ms,"After this amount of time, the master will stop attempting to contact a tablet server in order to perform operations such as deleting a tablet.",reliability,kudu
2729,tserver_unresponsive_timeout_ms,The period of time that a Master can go without receiving a heartbeat from a tablet server before considering it unresponsive.,reliability,kudu
2730,rpc_acceptor_listen_backlog,Socket backlog parameter used when listening for RPC connections. ,others,kudu
2731,rpc_encrypt_loopback_connections,Whether to encrypt data transfer on RPC connections that stay within a single host. ,security,kudu
2732,rpc_callback_max_cycles,The maximum number of cycles for which an RPC callback should be allowed to run without emitting a warning.,reliability,kudu
2733,remember_clients_ttl_ms,"Maximum amount of time, in milliseconds, the server ""remembers"" a client for the purpose of caching its responses.",performance,kudu
2734,rpc_dump_all_traces,"If true, dump all RPC traces at INFO level",debuggability,kudu
2735,rpc_duration_too_long_ms,Threshold (in milliseconds) above which a RPC is considered too long and its duration and method name are logged at INFO level. ,debuggability,kudu
2736,trusted_subnets,"A trusted subnet whitelist. If set explicitly, all unauthenticated or unencrypted connections are prohibited except the ones from the specified address blocks.",security,kudu
2737,rpc_max_message_size,The maximum size of a message that any RPC that the server will accept. ,reliability,kudu
2738,use_system_auth_to_local,"When enabled, use the system krb5 library to map Kerberos principal names to local (short) usernames.",others,kudu
2739,rpc_advertised_addresses,Comma-separated list of addresses to advertise externally for RPC connections.,environment,kudu
2740,rpc_num_acceptors_per_address,Number of RPC acceptor threads for each bound address,performance,kudu
2741,rpc_authentication,Whether to require RPC connections to authenticate.,security,kudu
2742,rpc_default_keepalive_time_ms,"If an RPC connection from a client is idle for this amount of time, the server will disconnect the client. Setting this to any negative value keeps connections always alive.",reliability,kudu
2743,rpc_encryption,Whether to require RPC connections to be encrypted. ,security,kudu
2744,rpc_negotiation_timeout_ms,Timeout for negotiating an RPC connection.,reliability,kudu
2745,rpc_tls_min_protocol,The minimum protocol version to allow when for securing RPC connections with TLS.,environment,kudu
2746,webserver_enabled,Whether to enable the web server on this daemon.,others,kudu
2747,metrics_log_interval_ms,Interval (in milliseconds) at which the server will dump its metrics to a local log file.,debuggability,kudu
2748,webserver_advertised_addresses,Comma-separated list of addresses to advertise externally for HTTP(S) connections.,environment,kudu
2749,webserver_authentication_domain,Domain used for debug webserver authentication,debuggability,kudu
2750,webserver_enable_doc_root,"If true, webserver may serve static files from the webserver_doc_root",others,kudu
2751,webserver_interface,Interface to start debug webserver on. ,debuggability,kudu
2752,tablet_bloom_block_size,Block size of the bloom filters used for tablet keys,performance,kudu
2753,tablet_history_max_age_sec,Number of seconds to retain tablet history. Reads initiated at a snapshot that is older than this age will be rejected.,reliability,kudu
2754,tablet_transaction_memory_limit_mb,Maximum amount of memory that may be consumed by all in-flight transactions belonging to a particular tablet.,reliability,kudu
2755,scanner_ttl_ms,Number of milliseconds of inactivity allowed for a scannerbefore it may be expired,reliability,kudu
2756,tablet_copy_begin_session_timeout_ms,Tablet server RPC client timeout for BeginTabletCopySession calls.,reliability,kudu
2757,tablet_copy_idle_timeout_sec,"Amount of time without activity before a tablet copy session will expire, in seconds",reliability,kudu
2758,num_tablets_to_copy_simultaneously,Number of threads available to copy tablets from remote servers.,performance,kudu
2759,num_tablets_to_delete_simultaneously,Number of threads available to delete tablets. ,performance,kudu
2760,num_tablets_to_open_simultaneously,Number of threads available to open tablets during startup,performance,kudu
2761,disable_core_dumps,Disable core dumps when this process crashes.,debuggability,kudu
2762,metrics_retirement_age_ms,The minimum number of milliseconds a metric will be kept for after it is no longer active.,reliability,kudu
2763,minidump_path,Directory to write minidump files to. ,environment,kudu
2764,minidump_size_limit_hint_kb,Size limit hint for minidump files in KB.,reliability,kudu
2765,memory_limit_soft_percentage,Percentage of the hard memory limit that this daemon may consume before memory throttling of writes begins. ,performance,kudu
2766,logbuflevel,Buffer log messages logged at this level or lower,debuggability,kudu
2767,logfile_mode,Log file mode/permissions.,security,kudu
2768,tserver_master_addrs,Comma separated addresses of the masters which the tablet server should connect to. ,environment,kudu
2769,log_prefix,Prepend the log prefix to the start of each log line,others,kudu
2770,remember_responses_ttl_ms,"Maximum amount of time, in milliseconds, the server ""remembers"" a response to a specific request for a client. ",reliability,kudu
2771,redact,Comma-separated list that controls redaction context. ,security,kudu
2772,enable_minidumps,Whether to enable minidump generation upon process crash or SIGUSR1. ,reliability,kudu
2773,max_minidumps,Maximum number of minidump files to keep per daemon.,reliability,kudu
2774,tcmalloc_max_free_bytes_percentage,Maximum percentage of the RSS that tcmalloc is allowed to use for reserved but unallocated memory.,reliability,kudu
2775,mapreduce.job.committer.setup.cleanup.needed,"true, if job needs job-setup and job-cleanup. false, otherwise",others,mapreduce
2776,mapreduce.task.io.sort.factor,The number of streams to merge at once while sorting files. This determines the number of open file handles.,performance,mapreduce
2777,mapreduce.task.io.sort.mb,"The total amount of buffer memory to use while sorting files, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",performance,mapreduce
2778,mapreduce.map.sort.spill.percent,"The soft limit in the serialization buffer. Once reached, a thread will begin to spill the contents to disk in the background. Note that collection will not block if this threshold is exceeded while a spill is already in progress, so spills may be larger than this threshold when it is set to less than .5",reliability,mapreduce
2779,mapreduce.jobtracker.address,"The host and port that the MapReduce job tracker runs at. If ""local"", then jobs are run in-process as a single map and reduce task.",environment,mapreduce
2780,mapreduce.local.clientfactory.class.name,This the client factory that is responsible for creating local job runner client,others,mapreduce
2781,mapreduce.jobtracker.system.dir,The directory where MapReduce stores control files.,environment,mapreduce
2782,mapreduce.jobtracker.staging.root.dir,"The root of the staging area for users' job files In practice, this should be the directory where users' home directories are located (usually /user)",environment,mapreduce
2783,mapreduce.cluster.temp.dir,A shared directory for temporary files.,environment,mapreduce
2784,mapreduce.job.maps,"The default number of map tasks per job. Ignored when mapreduce.framework.name is ""local"".",performance,mapreduce
2785,mapreduce.job.reduces,"The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can still be executed in a single wave. Ignored when mapreduce.framework.name is ""local"".",reliability,mapreduce
2786,mapreduce.job.running.map.limit,The maximum number of simultaneous map tasks per job. There is no limit if this value is 0 or negative.,reliability,mapreduce
2787,mapreduce.job.running.reduce.limit,The maximum number of simultaneous reduce tasks per job. There is no limit if this value is 0 or negative.,reliability,mapreduce
2788,mapreduce.job.max.map,Limit on the number of map tasks allowed per job. There is no limit if this value is negative.,reliability,mapreduce
2789,mapreduce.job.reducer.preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers reducer preemption when there is no anticipated headroom. If set to 0 or a negative value, the reducer is preempted as soon as lack of headroom is detected. Default is 0.",reliability,mapreduce
2790,mapreduce.job.reducer.unconditional-preempt.delay.sec,"The threshold (in seconds) after which an unsatisfied mapper request triggers a forced reducer preemption irrespective of the anticipated headroom. By default, it is set to 5 mins. Setting it to 0 leads to immediate reducer preemption. Setting to -1 disables this preemption altogether.",reliability,mapreduce
2791,mapreduce.job.max.split.locations,The max number of block locations to store for each split for locality calculation.,reliability,mapreduce
2792,mapreduce.job.split.metainfo.maxsize,The maximum permissible size of the split metainfo file. The MapReduce ApplicationMaster won't attempt to read submitted split metainfo files bigger than this configured value. No limits if set to -1.,performance,mapreduce
2793,mapreduce.map.maxattempts,"Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it.",reliability,mapreduce
2794,mapreduce.reduce.maxattempts,"Expert: The maximum number of attempts per reduce task. In other words, framework will try to execute a reduce task these many number of times before giving up on it.",reliability,mapreduce
2795,mapreduce.reduce.shuffle.fetch.retry.enabled,Set to enable fetch retry during host restart.,others,mapreduce
2796,mapreduce.reduce.shuffle.fetch.retry.interval-ms,Time of interval that fetcher retry to fetch again when some non-fatal failure happens because of some events like NM restart.,reliability,mapreduce
2797,mapreduce.reduce.shuffle.fetch.retry.timeout-ms,Timeout value for fetcher to retry to fetch again when some non-fatal failure happens because of some events like NM restart.,reliability,mapreduce
2798,mapreduce.reduce.shuffle.retry-delay.max.ms,The maximum number of ms the reducer will delay before retrying to download map data.,reliability,mapreduce
2799,mapreduce.reduce.shuffle.parallelcopies,The default number of parallel transfers run by reduce during the copy(shuffle) phase.,performance,mapreduce
2800,mapreduce.reduce.shuffle.connect.timeout,Expert: The maximum amount of time (in milli seconds) reduce task spends in trying to connect to a remote node for getting map output.,reliability,mapreduce
2801,mapreduce.reduce.shuffle.read.timeout,Expert: The maximum amount of time (in milli seconds) reduce task waits for map output data to be available for reading after obtaining connection.,reliability,mapreduce
2802,mapreduce.shuffle.listen.queue.size,The length of the shuffle server listen queue.,performance,mapreduce
2803,mapreduce.shuffle.connection-keep-alive.enable,set to true to support keep-alive connections.,others,mapreduce
2804,mapreduce.shuffle.connection-keep-alive.timeout,"The number of seconds a shuffle client attempts to retain http connection. Refer ""Keep-Alive: timeout="" header in Http specification",reliability,mapreduce
2805,mapreduce.task.timeout,"The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string. A value of 0 disables the timeout.",reliability,mapreduce
2806,mapreduce.map.memory.mb,The amount of memory to request from the scheduler for each map task.,performance,mapreduce
2807,mapreduce.map.cpu.vcores,The number of virtual cores to request from the scheduler for each map task.,performance,mapreduce
2808,mapreduce.reduce.memory.mb,The amount of memory to request from the scheduler for each reduce task.,performance,mapreduce
2809,mapreduce.reduce.cpu.vcores,The number of virtual cores to request from the scheduler for each reduce task.,performance,mapreduce
2810,mapred.child.java.opts,"Java opts for the task processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.",performance,mapreduce
2811,mapred.child.env,User added environment variables for the task processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit nodemanager's B env variable on Unix. 3) B=%B%;c This is inherit nodemanager's B env variable on Windows.,environment,mapreduce
2812,mapreduce.admin.user.env,"Expert: Additional execution environment entries for map and reduce task processes. This is not an additive property. You must preserve the original value if you want your map and reduce tasks to have access to native libraries (compression, etc). When this value is empty, the command to set execution envrionment will be OS dependent: For linux, use LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native. For windows, use PATH = %PATH%;%HADOOP_COMMON_HOME%\\bin.",environment,mapreduce
2813,yarn.app.mapreduce.am.log.level,"The logging level for the MR ApplicationMaster. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overriden if ""mapreduce.job.log4j-properties-file"" is set.",debuggability,mapreduce
2814,mapreduce.map.log.level,"The logging level for the map task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",debuggability,mapreduce
2815,mapreduce.reduce.log.level,"The logging level for the reduce task. The allowed levels are: OFF, FATAL, ERROR, WARN, INFO, DEBUG, TRACE and ALL. The setting here could be overridden if ""mapreduce.job.log4j-properties-file"" is set.",debuggability,mapreduce
2816,mapreduce.reduce.merge.inmem.threshold,"The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge.",performance,mapreduce
2817,mapreduce.reduce.shuffle.merge.percent,"The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.",performance,mapreduce
2818,mapreduce.reduce.shuffle.input.buffer.percent,The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.,performance,mapreduce
2819,mapreduce.reduce.input.buffer.percent,"The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin.",performance,mapreduce
2820,mapreduce.reduce.shuffle.memory.limit.percent,"Expert: Maximum percentage of the in-memory limit that a single shuffle can consume. Range of valid values is [0.0, 1.0]. If the value is 0.0 map outputs are shuffled directly to disk.",performance,mapreduce
2821,mapreduce.shuffle.ssl.enabled,Whether to use SSL for for the Shuffle HTTP endpoints.,security,mapreduce
2822,mapreduce.shuffle.ssl.file.buffer.size,Buffer size for reading spills from file when using SSL.,performance,mapreduce
2823,mapreduce.shuffle.max.connections,Max allowed connections for the shuffle. Set to 0 (zero) to indicate no limit on the number of connections.,performance,mapreduce
2824,mapreduce.shuffle.max.threads,"Max allowed threads for serving shuffle connections. Set to zero to indicate the default of 2 times the number of available processors (as reported by Runtime.availableProcessors()). Netty is used to serve requests, so a thread is not needed for each connection.",performance,mapreduce
2825,mapreduce.shuffle.transferTo.allowed,"This option can enable/disable using nio transferTo method in the shuffle phase. NIO transferTo does not perform well on windows in the shuffle phase. Thus, with this configuration property it is possible to disable it, in which case custom transfer method will be used. Recommended value is false when running Hadoop on Windows. For Linux, it is recommended to set it to true. If nothing is set then the default value is false for Windows, and true for Linux.",performance,mapreduce
2826,mapreduce.shuffle.transfer.buffer.size,"This property is used only if mapreduce.shuffle.transferTo.allowed is set to false. In that case, this property defines the size of the buffer used in the buffer copy code for the shuffle phase. The size of this buffer determines the size of the IO requests.",performance,mapreduce
2827,mapreduce.reduce.markreset.buffer.percent,The percentage of memory -relative to the maximum heap size- to be used for caching values when using the mark-reset functionality.,performance,mapreduce
2828,mapreduce.map.speculative,"If true, then multiple instances of some map tasks may be executed in parallel.",performance,mapreduce
2829,mapreduce.reduce.speculative,"If true, then multiple instances of some reduce tasks may be executed in parallel.",performance,mapreduce
2830,mapreduce.job.speculative.speculative-cap-running-tasks,The max percent (0-1) of running tasks that can be speculatively re-executed at any time.,reliability,mapreduce
2831,mapreduce.job.speculative.speculative-cap-total-tasks,The max percent (0-1) of all tasks that can be speculatively re-executed at any time.,reliability,mapreduce
2832,mapreduce.job.speculative.minimum-allowed-tasks,The minimum allowed tasks that can be speculatively re-executed at any time.,performance,mapreduce
2833,mapreduce.job.speculative.retry-after-no-speculate,The waiting time(ms) to do next round of speculation if there is no task speculated in this round.,reliability,mapreduce
2834,mapreduce.job.speculative.retry-after-speculate,The waiting time(ms) to do next round of speculation if there are tasks speculated in this round.,reliability,mapreduce
2835,mapreduce.job.map.output.collector.class,"The MapOutputCollector implementation(s) to use. This may be a comma-separated list of class names, in which case the map task will try to initialize each of the collectors in turn. The first to successfully initialize will be used.",environment,mapreduce
2836,mapreduce.job.speculative.slowtaskthreshold,The number of standard deviations by which a task's ave progress-rates must be lower than the average of all running tasks' for the task to be considered too slow.,performance,mapreduce
2837,mapreduce.job.ubertask.enable,"Whether to enable the small-jobs ""ubertask"" optimization, which runs ""sufficiently small"" jobs sequentially within a single JVM. ""Small"" is defined by the following maxmaps, maxreduces, and maxbytes settings. Note that configurations for application masters also affect the ""Small"" definition - yarn.app.mapreduce.am.resource.mb must be larger than both mapreduce.map.memory.mb and mapreduce.reduce.memory.mb, and yarn.app.mapreduce.am.resource.cpu-vcores must be larger than both mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores to enable ubertask. Users may override this value.",performance,mapreduce
2838,mapreduce.job.ubertask.maxmaps,"Threshold for number of maps, beyond which job is considered too big for the ubertasking optimization. Users may override this value, but only downward.",performance,mapreduce
2839,mapreduce.job.ubertask.maxreduces,"Threshold for number of reduces, beyond which job is considered too big for the ubertasking optimization. CURRENTLY THE CODE CANNOT SUPPORT MORE THAN ONE REDUCE and will ignore larger values. (Zero is a valid max, however.) Users may override this value, but only downward.",performance,mapreduce
2840,mapreduce.job.ubertask.maxbytes,"Threshold for number of input bytes, beyond which job is considered too big for the ubertasking optimization. If no value is specified, dfs.block.size is used as a default. Be sure to specify a default value in mapred-site.xml if the underlying filesystem is not HDFS. Users may override this value, but only downward.",performance,mapreduce
2841,mapreduce.job.emit-timeline-data,Specifies if the Application Master should emit timeline data to the timeline server. Individual jobs can override this value.,others,mapreduce
2842,mapreduce.job.sharedcache.mode,"A comma delimited list of resource categories to submit to the shared cache. The valid categories are: jobjar, libjars, files, archives. If ""disabled"" is specified then the job submission code will not use the shared cache.",environment,mapreduce
2843,mapreduce.input.fileinputformat.split.minsize,The minimum size chunk that map input should be split into. Note that some file formats may have minimum split sizes that take priority over this setting.,performance,mapreduce
2844,mapreduce.input.fileinputformat.list-status.num-threads,The number of threads to use to list and fetch block locations for the specified input paths. Note: multiple threads should not be used if a custom non thread-safe path filter is used.,performance,mapreduce
2845,mapreduce.input.lineinputformat.linespermap,"When using NLineInputFormat, the number of lines of input data to include in each split.",others,mapreduce
2846,mapreduce.client.submit.file.replication,The replication level for submitted job files. This should be around the square root of the number of nodes.,reliability,mapreduce
2847,mapreduce.task.files.preserve.failedtasks,"Should the files for failed tasks be kept. This should only be used on jobs that are failing, because the storage is never reclaimed. It also prevents the map outputs from being erased from the reduce directory as they are consumed.",reliability,mapreduce
2848,mapreduce.output.fileoutputformat.compress,Should the job outputs be compressed?,others,mapreduce
2849,mapreduce.output.fileoutputformat.compress.type,"If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.",performance,mapreduce
2850,mapreduce.output.fileoutputformat.compress.codec,"If the job outputs are compressed, how should they be compressed?",performance,mapreduce
2851,mapreduce.map.output.compress,Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.,performance,mapreduce
2852,mapreduce.map.output.compress.codec,"If the map outputs are compressed, how should they be compressed?",performance,mapreduce
2853,map.sort.class,The default sort class for sorting keys.,others,mapreduce
2854,mapreduce.task.userlog.limit.kb,The maximum size of user-logs of each task in KB. 0 disables the cap.,reliability,mapreduce
2855,yarn.app.mapreduce.am.container.log.limit.kb,The maximum size of the MRAppMaster attempt container logs in KB. 0 disables the cap.,reliability,mapreduce
2856,yarn.app.mapreduce.task.container.log.backups,"Number of backup files for task logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for tasks when both mapreduce.task.userlog.limit.kb and yarn.app.mapreduce.task.container.log.backups are greater than zero.",reliability,mapreduce
2857,yarn.app.mapreduce.am.container.log.backups,"Number of backup files for the ApplicationMaster logs when using ContainerRollingLogAppender (CRLA). See org.apache.log4j.RollingFileAppender.maxBackupIndex. By default, ContainerLogAppender (CLA) is used, and container logs are not rolled. CRLA is enabled for the ApplicationMaster when both yarn.app.mapreduce.am.container.log.limit.kb and yarn.app.mapreduce.am.container.log.backups are greater than zero.",reliability,mapreduce
2858,yarn.app.mapreduce.shuffle.log.separate,If enabled ('true') logging generated by the client-side shuffle classes in a reducer will be written in a dedicated log file 'syslog.shuffle' instead of 'syslog'.,debuggability,mapreduce
2859,yarn.app.mapreduce.shuffle.log.limit.kb,Maximum size of the syslog.shuffle file in kilobytes (0 for no limit).,reliability,mapreduce
2860,yarn.app.mapreduce.shuffle.log.backups,If yarn.app.mapreduce.shuffle.log.limit.kb and yarn.app.mapreduce.shuffle.log.backups are greater than zero then a ContainerRollngLogAppender is used instead of ContainerLogAppender for syslog.shuffle. See org.apache.log4j.RollingFileAppender.maxBackupIndex,reliability,mapreduce
2861,mapreduce.job.maxtaskfailures.per.tracker,The number of task-failures on a node manager of a given job after which new tasks of that job aren't assigned to it. It MUST be less than mapreduce.map.maxattempts and mapreduce.reduce.maxattempts otherwise the failed task will never be tried on a different node.,reliability,mapreduce
2862,mapreduce.client.output.filter,"The filter for controlling the output of the task's userlogs sent to the console of the JobClient. The permissible options are: NONE, KILLED, FAILED, SUCCEEDED and ALL.",debuggability,mapreduce
2863,mapreduce.client.completion.pollinterval,The interval (in milliseconds) between which the JobClient polls the MapReduce ApplicationMaster for updates about job status. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,reliability,mapreduce
2864,mapreduce.client.progressmonitor.pollinterval,The interval (in milliseconds) between which the JobClient reports status to the console and checks for job completion. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.,reliability,mapreduce
2865,mapreduce.client.libjars.wildcard,"Whether the libjars cache files should be localized using a wildcarded directory instead of naming each archive independently. Using wildcards reduces the space needed for storing the job information in the case of a highly available resource manager configuration. This propery should only be set to false for specific jobs which are highly sensitive to the details of the archive localization. Having this property set to true will cause the archives to all be localized to the same local cache location. If false, each archive will be localized to its own local cache location. In both cases a symbolic link will be created to every archive from the job's working directory.",performance,mapreduce
2866,mapreduce.task.profile,"To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is ""true"" if task profiling is enabled.",debuggability,mapreduce
2867,mapreduce.task.profile.maps,To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,others,mapreduce
2868,mapreduce.task.profile.reduces,To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted.,others,mapreduce
2869,mapreduce.task.profile.params,"JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.",others,mapreduce
2870,mapreduce.task.profile.map.params,Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params,others,mapreduce
2871,mapreduce.task.profile.reduce.params,Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params,others,mapreduce
2872,mapreduce.task.skip.start.attempts,"The number of Task attempts AFTER which skip mode will be kicked off. When skip mode is kicked off, the tasks reports the range of records which it will process next, to the MR ApplicationMaster. So that on failures, the MR AM knows which ones are possibly the bad records. On further executions, those are skipped.",reliability,mapreduce
2873,mapreduce.map.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS is incremented by MapRunner after invoking the map function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",reliability,mapreduce
2874,mapreduce.reduce.skip.proc-count.auto-incr,"The flag which if set to true, SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS is incremented by framework after invoking the reduce function. This value must be set to false for applications which process the records asynchronously or buffer the input records. For example streaming. In such cases applications should increment this counter on their own.",reliability,mapreduce
2875,mapreduce.job.skip.outdir,"If no value is specified here, the skipped records are written to the output directory at _logs/skip. User can stop writing skipped records by giving the value ""none"".",environment,mapreduce
2876,mapreduce.map.skip.maxrecords,"The number of acceptable skip records surrounding the bad record PER bad record in mapper. The number includes the bad record as well. To turn the feature of detection/skipping of bad records off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever records(depends on application) get skipped are acceptable.",reliability,mapreduce
2877,mapreduce.reduce.skip.maxgroups,"The number of acceptable skip groups surrounding the bad group PER bad group in reducer. The number includes the bad group as well. To turn the feature of detection/skipping of bad groups off, set the value to 0. The framework tries to narrow down the skipped range by retrying until this threshold is met OR all attempts get exhausted for this task. Set the value to Long.MAX_VALUE to indicate that framework need not try to narrow down. Whatever groups(depends on application) get skipped are acceptable.",reliability,mapreduce
2878,mapreduce.ifile.readahead,Configuration key to enable/disable IFile readahead.,others,mapreduce
2879,mapreduce.ifile.readahead.bytes,Configuration key to set the IFile readahead length in bytes.,performance,mapreduce
2880,mapreduce.job.queuename,"Queue to which a job is submitted. This must match one of the queues defined in mapred-queues.xml for the system. Also, the ACL setup for the queue must allow the current user to submit a job to the queue. Before specifying a queue, ensure that the system is configured with the queue, and access is allowed for submitting jobs to the queue.",environment,mapreduce
2881,mapreduce.job.tags,Tags for the job that will be passed to YARN at submission time. Queries to YARN for applications can filter on these tags.,others,mapreduce
2882,mapreduce.cluster.local.dir,The local directory where MapReduce stores intermediate data files. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,environment,mapreduce
2883,mapreduce.cluster.acls.enabled,"Specifies whether ACLs should be checked for authorization of users for doing various queue and job level operations. ACLs are disabled by default. If enabled, access control checks are made by MapReduce ApplicationMaster when requests are made by users for queue operations like submit job to a queue and kill a job in the queue and job operations like viewing the job-details (See mapreduce.job.acl-view-job) or for modifying the job (See mapreduce.job.acl-modify-job) using Map/Reduce APIs, RPCs or via the console and web user interfaces. For enabling this flag, set to true in mapred-site.xml file of all MapReduce clients (MR job submitting nodes).",security,mapreduce
2884,mapreduce.job.acl-modify-job,"Job specific access-control list for 'modifying' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can do modification operations on the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard all the modifications with respect to this job and takes care of all the following operations: o killing this job o killing a task of this job, failing a task of this job o setting the priority of this job Each of these operations are also protected by the per-queue level ACL ""acl-administer-jobs"" configured via mapred-queues.xml. So a caller should have the authorization to satisfy either the queue-level ACL or the job-level ACL. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the modification operations on a job. By default, nobody else besides job-owner, the user who started the cluster, members of supergroup and queue administrators can perform modification operations on a job.",security,mapreduce
2885,mapreduce.job.acl-view-job,"Job specific access-control list for 'viewing' the job. It is only used if authorization is enabled in Map/Reduce by setting the configuration property mapreduce.cluster.acls.enabled to true. This specifies the list of users and/or groups who can view private details about the job. For specifying a list of users and groups the format to use is ""user1,user2 group1,group"". If set to '*', it allows all users/groups to modify this job. If set to ' '(i.e. space), it allows none. This configuration is used to guard some of the job-views and at present only protects APIs that can return possibly sensitive information of the job-owner like o job-level counters o task-level counters o tasks' diagnostic information o task-logs displayed on the HistoryServer's web-UI and o job.xml showed by the HistoryServer's web-UI Every other piece of information of jobs is still accessible by any other user, for e.g., JobStatus, JobProfile, list of jobs in the queue, etc. Irrespective of this ACL configuration, (a) job-owner, (b) the user who started the cluster, (c) members of an admin configured supergroup configured via mapreduce.cluster.permissions.supergroup and (d) queue administrators of the queue to which this job was submitted to configured via acl-administer-jobs for the specific queue in mapred-queues.xml can do all the view operations on a job. By default, nobody else besides job-owner, the user who started the cluster, memebers of supergroup and queue administrators can perform view operations on a job.",security,mapreduce
2886,mapreduce.job.finish-when-all-reducers-done,"Specifies whether the job should complete once all reducers have finished, regardless of whether there are still running mappers.",reliability,mapreduce
2887,mapreduce.job.token.tracking.ids.enabled,"Whether to write tracking ids of tokens to job-conf. When true, the configuration property ""mapreduce.job.token.tracking.ids"" is set to the token-tracking-ids of the job",debuggability,mapreduce
2888,mapreduce.job.token.tracking.ids,"When mapreduce.job.token.tracking.ids.enabled is set to true, this is set by the framework to the token-tracking-ids used by the job.",debuggability,mapreduce
2889,mapreduce.task.merge.progress.records,The number of records to process during merge before sending a progress notification to the MR ApplicationMaster.,performance,mapreduce
2890,mapreduce.task.combine.progress.records,The number of records to process during combine output collection before sending a progress notification.,performance,mapreduce
2891,mapreduce.job.reduce.slowstart.completedmaps,Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.,performance,mapreduce
2892,mapreduce.job.complete.cancel.delegation.tokens,"if false - do not unregister/cancel delegation tokens from renewal, because same tokens may be used by spawned jobs",reliability,mapreduce
2893,mapreduce.shuffle.port,Default port that the ShuffleHandler will run on. ShuffleHandler is a service run at the NodeManager to facilitate transfers of intermediate Map outputs to requesting Reducers.,environment,mapreduce
2894,mapreduce.job.reduce.shuffle.consumer.plugin.class,Name of the class whose instance will be used to send shuffle requests by reducetasks of this job. The class must be an instance of org.apache.hadoop.mapred.ShuffleConsumerPlugin.,environment,mapreduce
2895,mapreduce.job.node-label-expression,"All the containers of the Map Reduce job will be run with this node label expression. If the node-label-expression for job is not set, then it will use queue's default-node-label-expression for all job's containers.",others,mapreduce
2896,mapreduce.job.am.node-label-expression,This is node-label configuration for Map Reduce Application Master container. If not configured it will make use of mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,others,mapreduce
2897,mapreduce.map.node-label-expression,This is node-label configuration for Map task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,others,mapreduce
2898,mapreduce.reduce.node-label-expression,This is node-label configuration for Reduce task containers. If not configured it will use mapreduce.job.node-label-expression and if job's node-label expression is not configured then it will use queue's default-node-label-expression.,others,mapreduce
2899,mapreduce.job.counters.limit,Limit on the number of user counters allowed per job.,reliability,mapreduce
2900,mapreduce.framework.name,"The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.",environment,mapreduce
2901,yarn.app.mapreduce.am.staging-dir,The staging dir used while submitting jobs.,environment,mapreduce
2902,mapreduce.am.max-attempts,"The maximum number of application attempts. It is a application-specific setting. It should not be larger than the global number set by resourcemanager. Otherwise, it will be override. The default number is set to 2, to allow at least one retry for AM.",reliability,mapreduce
2903,mapreduce.job.end-notification.url,"Indicates url which will be called on completion of job to inform end status of job. User can give at most 2 variables with URI : $jobId and $jobStatus. If they are present in URI, then they will be replaced by their respective values.",environment,mapreduce
2904,mapreduce.job.end-notification.retry.attempts,The number of times the submitter of the job wants to retry job end notification if it fails. This is capped by mapreduce.job.end-notification.max.attempts,reliability,mapreduce
2905,mapreduce.job.end-notification.retry.interval,The number of milliseconds the submitter of the job wants to wait before job end notification is retried if it fails. This is capped by mapreduce.job.end-notification.max.retry.interval,reliability,mapreduce
2906,mapreduce.job.end-notification.max.attempts,"The maximum number of times a URL will be read for providing job end notification. Cluster administrators can set this to limit how long after end of a job, the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.",reliability,mapreduce
2907,mapreduce.job.log4j-properties-file,"Used to override the default settings of log4j in container-log4j.properties for NodeManager. Like container-log4j.properties, it requires certain framework appenders properly defined in this overriden file. The file on the path will be added to distributed cache and classpath. If no-scheme is given in the path, it defaults to point to a log4j file on the local FS.",environment,mapreduce
2908,mapreduce.job.end-notification.max.retry.interval,The maximum amount of time (in milliseconds) to wait before retrying job end notification. Cluster administrators can set this to limit how long the Application Master waits before exiting. Must be marked as final to prevent users from overriding this.,reliability,mapreduce
2909,yarn.app.mapreduce.am.env,User added environment variables for the MR App Master processes. Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit tasktracker's B env variable.,environment,mapreduce
2910,yarn.app.mapreduce.am.admin.user.env,Environment variables for the MR App Master processes for admin purposes. These values are set first and can be overridden by the user env (yarn.app.mapreduce.am.env) Example : 1) A=foo This will set the env variable A to foo 2) B=$B:c This is inherit app master's B env variable.,environment,mapreduce
2911,yarn.app.mapreduce.am.command-opts,"Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.",others,mapreduce
2912,yarn.app.mapreduce.am.admin-command-opts,Java opts for the MR App Master processes for admin purposes. It will appears before the opts set by yarn.app.mapreduce.am.command-opts and thus its options can be overridden user. Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.,others,mapreduce
2913,yarn.app.mapreduce.am.job.task.listener.thread-count,The number of threads used to handle RPC calls in the MR AppMaster from remote tasks,performance,mapreduce
2914,yarn.app.mapreduce.am.job.client.port-range,"Range of ports that the MapReduce AM can use when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200",environment,mapreduce
2915,yarn.app.mapreduce.am.webapp.port-range,"Range of ports that the MapReduce AM can use for its webapp when binding. Leave blank if you want all possible ports. For example 50000-50050,50100-50200",environment,mapreduce
2916,yarn.app.mapreduce.am.job.committer.cancel-timeout,The amount of time in milliseconds to wait for the output committer to cancel an operation if the job is killed,reliability,mapreduce
2917,yarn.app.mapreduce.am.job.committer.commit-window,"Defines a time window in milliseconds for output commit operations. If contact with the RM has occurred within this window then commits are allowed, otherwise the AM will not allow output commits until contact with the RM has been re-established.",reliability,mapreduce
2918,mapreduce.fileoutputcommitter.algorithm.version,"The file output committer algorithm version valid algorithm version number: 1 or 2 default to 1, which is the original algorithm In algorithm version 1, 1. commitTask will rename directory $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/_temporary/$appAttemptID/$taskID/ 2. recoverTask will also do a rename $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/_temporary/($appAttemptID + 1)/$taskID/ 3. commitJob will merge every task output file in $joboutput/_temporary/$appAttemptID/$taskID/ to $joboutput/, then it will delete $joboutput/_temporary/ and write $joboutput/_SUCCESS It has a performance regression, which is discussed in MAPREDUCE-4815. If a job generates many files to commit then the commitJob method call at the end of the job can take minutes. the commit is single-threaded and waits until all tasks have completed before commencing. algorithm version 2 will change the behavior of commitTask, recoverTask, and commitJob. 1. commitTask will rename all files in $joboutput/_temporary/$appAttemptID/_temporary/$taskAttemptID/ to $joboutput/ 2. recoverTask actually doesn't require to do anything, but for upgrade from version 1 to version 2 case, it will check if there are any files in $joboutput/_temporary/($appAttemptID - 1)/$taskID/ and rename them to $joboutput/ 3. commitJob can simply delete $joboutput/_temporary and write $joboutput/_SUCCESS This algorithm will reduce the output commit time for large jobs by having the tasks commit directly to the final output directory as they were completing and commitJob had very little to do.",environment,mapreduce
2919,yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,The interval in ms at which the MR AppMaster should send heartbeats to the ResourceManager,reliability,mapreduce
2920,yarn.app.mapreduce.client-am.ipc.max-retries,The number of client retries to the AM - before reconnecting to the RM to fetch Application Status.,reliability,mapreduce
2921,yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,The number of client retries on socket timeouts to the AM - before reconnecting to the RM to fetch Application Status.,reliability,mapreduce
2922,yarn.app.mapreduce.client.max-retries,The number of client retries to the RM/HS before throwing exception. This is a layer above the ipc.,reliability,mapreduce
2923,yarn.app.mapreduce.am.resource.mb,The amount of memory the MR AppMaster needs.,performance,mapreduce
2924,yarn.app.mapreduce.am.resource.cpu-vcores,The number of virtual CPU cores the MR AppMaster needs.,performance,mapreduce
2925,yarn.app.mapreduce.am.hard-kill-timeout-ms,Number of milliseconds to wait before the job client kills the application.,reliability,mapreduce
2926,yarn.app.mapreduce.client.job.max-retries,"The number of retries the client will make for getJob and dependent calls. This is needed for non-HDFS DFS where additional, high level retries are required to avoid spurious failures during the getJob call. 30 is a good value for WASB",reliability,mapreduce
2927,yarn.app.mapreduce.client.job.retry-interval,The delay between getJob retries in ms for retries configured with yarn.app.mapreduce.client.job.max-retries.,reliability,mapreduce
2928,mapreduce.application.classpath,"CLASSPATH for MR applications. A comma-separated list of CLASSPATH entries. If mapreduce.application.framework is set then this must specify the appropriate classpath for that archive, and the name of the archive must be present in the classpath. If mapreduce.app-submission.cross-platform is false, platform-specific environment vairable expansion syntax would be used to construct the default CLASSPATH entries. For Linux: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*, $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*. For Windows: %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*, %HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*. If mapreduce.app-submission.cross-platform is true, platform-agnostic default CLASSPATH for MR applications would be used: {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/*, {{HADOOP_MAPRED_HOME}}/share/hadoop/mapreduce/lib/* Parameter expansion marker will be replaced by NodeManager on container launch based on the underlying OS accordingly.",environment,mapreduce
2929,mapreduce.app-submission.cross-platform,"If enabled, user can submit an application cross-platform i.e. submit an application from a Windows client to a Linux/Unix server or vice versa.",others,mapreduce
2930,mapreduce.application.framework.path,"Path to the MapReduce framework archive. If set, the framework archive will automatically be distributed along with the job, and this path would normally reside in a public location in an HDFS filesystem. As with distributed cache files, this can be a URL with a fragment specifying the alias to use for the archive name. For example, hdfs:/mapred/framework/hadoop-mapreduce-2.1.1.tar.gz#mrframework would alias the localized archive as ""mrframework"". Note that mapreduce.application.classpath must include the appropriate classpath for the specified framework. The base name of the archive, or alias of the archive if an alias is used, must appear in the specified classpath.",environment,mapreduce
2931,mapreduce.job.classloader,Whether to use a separate (isolated) classloader for user classes in the task JVM.,others,mapreduce
2932,mapreduce.job.classloader.system.classes,"Used to override the default definition of the system classes for the job classloader. The system classes are a comma-separated list of patterns that indicate whether to load a class from the system classpath, instead from the user-supplied JARs, when mapreduce.job.classloader is enabled. A positive pattern is defined as: 1. A single class name 'C' that matches 'C' and transitively all nested classes 'C$*' defined in C; 2. A package name ending with a '.' (e.g., ""com.example."") that matches all classes from that package. A negative pattern is defined by a '-' in front of a positive pattern (e.g., ""-com.example.""). A class is considered a system class if and only if it matches one of the positive patterns and none of the negative ones. More formally: A class is a member of the inclusion set I if it matches one of the positive patterns. A class is a member of the exclusion set E if it matches one of the negative patterns. The set of system classes S = I \ E.",others,mapreduce
2933,mapreduce.jvm.system-properties-to-log,Comma-delimited list of system properties to log on mapreduce JVM start,debuggability,mapreduce
2934,mapreduce.jobhistory.address,MapReduce JobHistory Server IPC host:port,environment,mapreduce
2935,mapreduce.jobhistory.webapp.address,MapReduce JobHistory Server Web UI host:port,environment,mapreduce
2936,mapreduce.jobhistory.webapp.https.address,The https address the MapReduce JobHistory Server WebApp is on.,environment,mapreduce
2937,mapreduce.jobhistory.keytab,Location of the kerberos keytab file for the MapReduce JobHistory Server.,environment,mapreduce
2938,mapreduce.jobhistory.principal,Kerberos principal name for the MapReduce JobHistory Server.,security,mapreduce
2939,mapreduce.jobhistory.cleaner.interval-ms,"How often the job history cleaner checks for files to delete, in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms.",performance,mapreduce
2940,mapreduce.jobhistory.max-age-ms,Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week).,performance,mapreduce
2941,mapreduce.jobhistory.client.thread-count,The number of threads to handle client API requests,performance,mapreduce
2942,mapreduce.jobhistory.datestring.cache.size,Size of the date string cache. Effects the number of directories which will be scanned to find a job.,performance,mapreduce
2943,mapreduce.jobhistory.joblist.cache.size,Size of the job list cache,performance,mapreduce
2944,mapreduce.jobhistory.loadedjobs.cache.size,Size of the loaded job cache. This property is ignored if the property mapreduce.jobhistory.loadedtasks.cache.size is set to a positive value.,performance,mapreduce
2945,mapreduce.jobhistory.loadedtasks.cache.size,"Change the job history cache limit to be set in terms of total task count. If the total number of tasks loaded exceeds this value, then the job cache will be shrunk down until it is under this limit (minimum 1 job in cache). If this value is empty or nonpositive then the cache reverts to using the property mapreduce.jobhistory.loadedjobs.cache.size as a job cache size. Two recommendations for the mapreduce.jobhistory.loadedtasks.cache.size property: 1) For every 100k of cache size, set the heap size of the Job History Server to 1.2GB. For example, mapreduce.jobhistory.loadedtasks.cache.size=500000, heap size=6GB. 2) Make sure that the cache size is larger than the number of tasks required for the largest job run on the cluster. It might be a good idea to set the value slightly higher (say, 20%) in order to allow for job size growth.",performance,mapreduce
2946,mapreduce.jobhistory.move.interval-ms,Scan for history files to move from intermediate done dir to done dir at this frequency.,reliability,mapreduce
2947,mapreduce.jobhistory.move.thread-count,The number of threads used to move files.,performance,mapreduce
2948,mapreduce.jobhistory.store.class,The HistoryStorage class to use to cache history data.,performance,mapreduce
2949,mapreduce.jobhistory.minicluster.fixed.ports,Whether to use fixed ports with the minicluster,others,mapreduce
2950,mapreduce.jobhistory.admin.address,The address of the History server admin interface.,environment,mapreduce
2951,mapreduce.jobhistory.admin.acl,ACL of who can be admin of the History server.,security,mapreduce
2952,mapreduce.jobhistory.recovery.enable,Enable the history server to store server state and recover server state upon startup. If enabled then mapreduce.jobhistory.recovery.store.class must be specified.,reliability,mapreduce
2953,mapreduce.jobhistory.recovery.store.class,The HistoryServerStateStoreService class to store history server state for recovery.,reliability,mapreduce
2954,mapreduce.jobhistory.recovery.store.fs.uri,The URI where history server state will be stored if HistoryServerFileSystemStateStoreService is configured as the recovery storage class.,environment,mapreduce
2955,mapreduce.jobhistory.recovery.store.leveldb.path,The URI where history server state will be stored if HistoryServerLeveldbSystemStateStoreService is configured as the recovery storage class.,environment,mapreduce
2956,mapreduce.jobhistory.http.policy,This configures the HTTP endpoint for JobHistoryServer web UI. The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https,security,mapreduce
2957,mapreduce.jobhistory.jobname.limit,Number of characters allowed for job name in Job History Server web page.,others,mapreduce
2958,mapreduce.jobhistory.jhist.format,"File format the AM will use when generating the .jhist file. Valid values are ""json"" for text output and ""binary"" for faster parsing.",others,mapreduce
2959,yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size,The initial size of thread pool to launch containers in the app master.,performance,mapreduce
2960,mapreduce.task.exit.timeout,"The number of milliseconds before a task will be terminated if it stays in finishing state for too long. After a task attempt completes from TaskUmbilicalProtocol's point of view, it will be transitioned to finishing state. That will give a chance for the task to exit by itself.",reliability,mapreduce
2961,mapreduce.task.exit.timeout.check-interval-ms,The interval in milliseconds between which the MR framework checks if task attempts stay in finishing state for too long.,reliability,mapreduce
2962,mapreduce.task.local-fs.write-limit.bytes,"Limit on the byte written to the local file system by each task. This limit only applies to writes that go through the Hadoop filesystem APIs within the task process (i.e.: writes that will update the local filesystem's BYTES_WRITTEN counter). It does not cover other writes such as logging, sideband writes from subprocesses (e.g.: streaming jobs), etc. Negative values disable the limit. default is -1",reliability,mapreduce
2963,mapreduce.jobhistory.webapp.rest-csrf.enabled,Enable the CSRF filter for the job history web app,security,mapreduce
2964,mapreduce.jobhistory.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,security,mapreduce
2965,mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,security,mapreduce
2966,mapreduce.job.cache.limit.max-resources,"The maximum number of resources a map reduce job is allowed to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",reliability,mapreduce
2967,mapreduce.job.cache.limit.max-resources-mb,"The maximum size (in MB) a map reduce job is allowed to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",reliability,mapreduce
2968,mapreduce.job.cache.limit.max-single-resource-mb,"The maximum size (in MB) of a single resource a map reduce job is allow to submit for localization via files, libjars, archives, and jobjar command line arguments and through the distributed cache. If set to 0 the limit is ignored.",performance,mapreduce
2969,mapreduce.jobhistory.webapp.xfs-filter.xframe-options,Value of the xframe-options,others,mapreduce
2970,mapreduce.jobhistory.loadedjob.tasks.max,The maximum number of tasks that a job can have so that the Job History Server will fully parse its associated job history file and load it into memory. A value of -1 (default) will allow all jobs to be loaded.,reliability,mapreduce
2971,mapreduce.job.redacted-properties,The list of job configuration properties whose value will be redacted.,security,mapreduce
2972,mapreduce.job.send-token-conf,"This configuration is a regex expression. The list of configurations that match the regex expression will be sent to RM. RM will use these configurations for renewing tokens. This configuration is added for below scenario: User needs to run distcp jobs across two clusters, but the RM does not have necessary hdfs configurations to connect to the remote hdfs cluster. Hence, user relies on this config to send the configurations to RM and RM uses these configurations to renew tokens. For example the following regex expression indicates the minimum required configs for RM to connect to a remote hdfs cluster: dfs.nameservices|^dfs.namenode.rpc-address.*$|^dfs.ha.namenodes.*$|^dfs.client.failover.proxy.provider.*$|dfs.namenode.kerberos.principal",others,mapreduce
2973,api_paste_config,File name for the paste.deploy config for api service,environment,neutron
2974,wsgi_log_format,"A python format string that is used as the template to generate log  lines. The following values can beformatted into it: client_ip,  date_time, request_line, status_code, body_length, wall_seconds.",debuggability,neutron
2975,tcp_keepidle,Sets the value of TCP_KEEPIDLE in seconds for each server socket. Not supported on OS X.,reliability,neutron
2976,wsgi_default_pool_size,Size of the pool of greenthreads used by wsgi,performance,neutron
2977,max_header_line,Maximum line size of message headers to be accepted. max_header_line  may need to be increased when using large tokens (typically those  generated when keystone is configured to use PKI tokens with big service catalogs).,performance,neutron
2978,wsgi_keep_alive,"If False, closes the client socket connection explicitly.",reliability,neutron
2979,client_socket_timeout,Timeout for client connections' socket operations. If an incoming  connection is idle for this number of seconds it will be closed. A value of '0' means wait forever.,reliability,neutron
2980,rpc_conn_pool_size,Size of RPC connection pool.,performance,neutron
2981,conn_pool_min_size,The pool size limit for connections expiration policy,reliability,neutron
2982,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,neutron
2983,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,performance,neutron
2984,rpc_response_timeout,Seconds to wait for a response from a call.,reliability,neutron
2985,transport_url,"The network address and optional user credentials for connecting to  the messaging backend, in URL format. The expected format is:",environment,neutron
2986,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,others,neutron
2987,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",debuggability,neutron
2988,log_config_append,"The name of a logging configuration file. This file is appended to  any existing logging configuration files. For details about logging  configuration files, see the Python logging module documentation. Note  that when logging configuration files are used then all logging  configuration is set in the configuration file and other logging  configuration options are ignored (for example, log-date-format).",environment,neutron
2989,log_date_format,Defines the format string for %(asctime)s in log records. Default:  the value above . This option is ignored if log_config_append is set.,debuggability,neutron
2990,log_file,"(Optional) Name of log file to send logging output to. If no default  is set, logging will go to stderr as defined by use_stderr. This option  is ignored if log_config_append is set.",debuggability,neutron
2991,log_dir,(Optional) The base directory used for relative log_file  paths. This option is ignored if log_config_append is set.,environment,neutron
2992,watch_log_file,Uses logging handler designed to watch file system. When log file is  moved or removed this handler will open a new log file with specified  path instantaneously. It makes sense only if log_file option is  specified and Linux platform is used. This option is ignored if  log_config_append is set.,debuggability,neutron
2993,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if  log_config_append is set.,debuggability,neutron
2994,use_journal,Enable journald for logging. If running in a systemd environment you  may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log  messages.This option is ignored if log_config_append is set.,debuggability,neutron
2995,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,debuggability,neutron
2996,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,debuggability,neutron
2997,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,debuggability,neutron
2998,use_eventlog,Log output to Windows Event Log.,debuggability,neutron
2999,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,reliability,neutron
3000,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next  rotation.,reliability,neutron
3001,max_logfile_count,Maximum number of rotated log files.,reliability,neutron
3002,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,reliability,neutron
3003,log_rotation_type,Log rotation type.,debuggability,neutron
3004,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,debuggability,neutron
3005,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,debuggability,neutron
3006,logging_debug_format_suffix,Additional data to append to log message when logging level for the  message is DEBUG. Used by oslo_log.formatters.ContextFormatter,debuggability,neutron
3007,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,debuggability,neutron
3008,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in  logging_context_format_string. Used by  oslo_log.formatters.ContextFormatter,debuggability,neutron
3009,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,debuggability,neutron
3010,publish_errors,Enables or disables publication of error events.,debuggability,neutron
3011,instance_format,The format for an instance that is passed with the log message.,debuggability,neutron
3012,instance_uuid_format,The format for an instance UUID that is passed with the log message.,debuggability,neutron
3013,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",reliability,neutron
3014,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,reliability,neutron
3015,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to  rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",debuggability,neutron
3016,fatal_deprecations,Enables or disables fatal status of deprecations.,reliability,neutron
3017,state_path,Where to store Neutron state files. This directory must be writable by the agent.,environment,neutron
3018,bind_host,The host IP to bind to.,environment,neutron
3019,bind_port,The port to bind to,environment,neutron
3020,api_extensions_path,"The path for API extensions. Note that this can be a colon-separated  list of paths. For example: api_extensions_path =  extensions:/path/to/more/exts:/even/more/exts. The __path__ of  neutron.extensions is appended to this, so if your extensions are in  there you don't need to specify them here.",environment,neutron
3021,auth_strategy,The type of authentication to use,security,neutron
3022,core_plugin,The core plugin Neutron will use,environment,neutron
3023,service_plugins,The service plugins Neutron will use,others,neutron
3024,base_mac,"The base MAC address Neutron will use for VIFs. The first 3 octets  will remain unchanged. If the 4th octet is not 00, it will also be used. The others will be randomly generated.",environment,neutron
3025,allow_bulk,Allow the usage of the bulk API,others,neutron
3026,pagination_max_limit,"The maximum number of items returned in a single response, value was 'infinite' or negative integer means no limit",reliability,neutron
3027,default_availability_zones,"Default value of availability zone hints. The availability zone aware schedulers use this when the resources availability_zone_hints is  empty. Multiple availability zones can be specified by a comma separated string. This value can be empty. In this case, even if  availability_zone_hints for a resource is empty, availability zone is  considered for high availability while scheduling the resource.",performance,neutron
3028,max_dns_nameservers,Maximum number of DNS nameservers per subnet,reliability,neutron
3029,max_subnet_host_routes,Maximum number of host routes per subnet,reliability,neutron
3030,ipv6_pd_enabled,Enables IPv6 Prefix Delegation for automatic subnet CIDR allocation.  Set to True to enable IPv6 Prefix Delegation for subnet allocation in a  PD-capable environment. Users making subnet creation requests for IPv6  subnets without providing a CIDR or subnetpool ID will be given a CIDR  via the Prefix Delegation mechanism. Note that enabling PD will override the behavior of the default IPv6 subnetpool.,security,neutron
3031,dhcp_lease_duration,DHCP lease duration (in seconds). Use -1 to tell dnsmasq to use infinite lease times.,reliability,neutron
3032,dns_domain,Domain to use for building the hostnames,environment,neutron
3033,external_dns_driver,Driver for external DNS integration.,environment,neutron
3034,dhcp_agent_notification,Allow sending resource operation notification to DHCP agent,security,neutron
3035,allow_overlapping_ips,Allow overlapping IP support in Neutron. Attention: the following  parameter MUST be set to False if Neutron is being used in conjunction  with Nova security groups.,security,neutron
3036,host,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,neutron
3037,network_link_prefix,"This string is prepended to the normal URL that is returned in links  to the OpenStack Network API. If it is empty (the default), the URLs are returned unchanged.",environment,neutron
3038,notify_nova_on_port_status_changes,Send notification to nova when port status changes,reliability,neutron
3039,notify_nova_on_port_data_changes,Send notification to nova when port data (fixed_ips/floatingip) changes so nova can update its cache.,reliability,neutron
3040,send_events_interval,Number of seconds between sending events to nova if there are any events to send.,reliability,neutron
3041,setproctitle,"Set process name to match child worker role. Available options are:  'off' - retains the previous behavior; 'on' - renames processes to  'neutron-server: role (original string)'; 'brief' - renames the same as  'on', but without the original string, such as 'neutron-server: role'.",others,neutron
3042,ipam_driver,"Neutron IPAM (IP address management) driver to use. By default, the reference implementation of the Neutron IPAM driver is used.",environment,neutron
3043,vlan_transparent,"If True, then allow plugins that support it to create VLAN transparent networks.",others,neutron
3044,filter_validation,"If True, then allow plugins to decide whether to perform validations  on filter parameters. Filter validation is enabled if this config is  turned on and it is supported by all plugins",others,neutron
3045,global_physnet_mtu,"MTU of the underlying physical network. Neutron uses this value to  calculate MTU for all virtual network components. For flat and VLAN  networks, neutron uses this value without modification. For overlay  networks such as VXLAN, neutron automatically subtracts the overlay  protocol overhead from this value. Defaults to 1500, the standard value  for Ethernet.",reliability,neutron
3046,backlog,Number of backlog requests to configure the socket with,performance,neutron
3047,retry_until_window,Number of seconds to keep retrying to listen,reliability,neutron
3048,use_ssl,Enable SSL on the API server,security,neutron
3049,periodic_interval,Seconds between running periodic tasks.,reliability,neutron
3050,api_workers,"Number of separate API worker processes for service. If not  specified, the default is equal to the number of CPUs available for best performance, capped by potential RAM usage.",performance,neutron
3051,rpc_workers,"Number of RPC worker processes for service. If not specified, the default is equal to half the number of API workers.",performance,neutron
3052,rpc_state_report_workers,Number of RPC worker processes dedicated to state reports queue.,performance,neutron
3053,periodic_fuzzy_delay,Range of seconds to randomly delay when starting the periodic task scheduler to reduce stampeding. (Disable by setting to 0),performance,neutron
3054,rpc_response_max_timeout,Maximum seconds to wait for a response from an RPC call.,reliability,neutron
3055,interface_driver,The driver used to manage the virtual interface.,environment,neutron
3056,metadata_proxy_socket,Location for Metadata Proxy UNIX domain socket.,environment,neutron
3057,metadata_proxy_user,User (uid or name) running metadata proxy after its initialization (if empty: agent effective user).,environment,neutron
3058,metadata_proxy_group,Group (gid or name) running metadata proxy after its initialization (if empty: agent effective group).,environment,neutron
3059,agent_down_time,"Seconds to regard the agent is down; should be at least twice report_interval, to be sure the agent is down for good.",reliability,neutron
3060,dhcp_load_type,"Representing the resource type whose load is being reported by the  agent. This can be 'networks', 'subnets' or 'ports'. When specified  (Default is networks), the server will extract particular load sent as  part of its agent configuration object from the agent report state,  which is the number of resources being consumed, at every  report_interval.dhcp_load_type can be used in combination with  network_scheduler_driver =  neutron.scheduler.dhcp_agent_scheduler.WeightScheduler When the  network_scheduler_driver is WeightScheduler, dhcp_load_type can be  configured to represent the choice for the resource being balanced.  Example: dhcp_load_type=networks",others,neutron
3061,enable_new_agents,"Agent starts with admin_state_up=False when enable_new_agents=False.  In the case, user's resources will not be scheduled automatically to the agent until admin changes admin_state_up to True.",reliability,neutron
3062,max_routes,Maximum number of routes per router,reliability,neutron
3063,enable_snat_by_default,Define the default value of enable_snat if not provided in external_gateway_info.,reliability,neutron
3064,network_scheduler_driver,Driver to use for scheduling network to DHCP agent,environment,neutron
3065,network_auto_schedule,Allow auto scheduling networks to DHCP agent.,others,neutron
3066,allow_automatic_dhcp_failover,Automatically remove networks from offline DHCP agents.,reliability,neutron
3067,dhcp_agents_per_network,"Number of DHCP agents scheduled to host a tenant network. If this  number is greater than 1, the scheduler automatically assigns multiple  DHCP agents for a given tenant network, providing high availability for  DHCP service.",performance,neutron
3068,enable_services_on_agents_with_admin_state_down,"Enable services on an agent with admin_state_up False. If this option is False, when admin_state_up of an agent is turned False, services on  it will be disabled. Agents with admin_state_up False are not selected  for automatic scheduling regardless of this option. But manual  scheduling to such agents is available if this option is True.",others,neutron
3069,dvr_base_mac,"The base mac address used for unique DVR instances by Neutron. The  first 3 octets will remain unchanged. If the 4th octet is not 00, it  will also be used. The others will be randomly generated. The  'dvr_base_mac' must be different from 'base_mac' to avoid  mixing them up with MAC's allocated for tenant ports. A 4 octet example  would be dvr_base_mac = fa:16:3f:4f:00:00. The default is 3 octet",environment,neutron
3070,router_distributed,System-wide flag to determine the type of router that tenants can create. Only admin can override.,others,neutron
3071,enable_dvr,"Determine if setup is configured for DVR. If False, DVR API extension will be disabled.",performance,neutron
3072,host_dvr_for_dhcp,"Flag to determine if hosting a DVR local router to the DHCP agent is  desired. If False, any L3 function supported by the DHCP agent instance  will not be possible, for instance: DNS.",performance,neutron
3073,router_scheduler_driver,Driver to use for scheduling router to a default L3 agent,environment,neutron
3074,router_auto_schedule,Allow auto scheduling of routers to L3 agent.,others,neutron
3075,allow_automatic_l3agent_failover,Automatically reschedule routers from offline L3 agents to online L3 agents.,reliability,neutron
3076,l3_ha,Enable HA mode for virtual routers.,reliability,neutron
3077,max_l3_agents_per_router,Maximum number of L3 agents which a HA router will be scheduled on.  If it is set to 0 then the router will be scheduled on every agent.,reliability,neutron
3078,l3_ha_net_cidr,Subnet used for the l3 HA admin network.,environment,neutron
3079,l3_ha_network_type,"The network type to use when creating the HA network for an HA  router. By default or if empty, the first 'tenant_network_types' is  used. This is helpful when the VRRP traffic should use a specific  network which is not the default one.",others,neutron
3080,l3_ha_network_physical_name,The physical network name with which the HA network can be created.,environment,neutron
3081,max_allowed_address_pair,Maximum number of allowed address pairs,reliability,neutron
3082,root_helper,Root helper application. Use 'sudo neutron-rootwrap  /etc/neutron/rootwrap.conf' to use the real root filter facility. Change to 'sudo' to skip the filtering and just run the command directly.,security,neutron
3083,use_helper_for_ns_read,"Use the root helper when listing the namespaces on a system. This may not be required depending on the security configuration. If the root  helper is not required, set this to False for a performance improvement.",security,neutron
3084,root_helper_daemon,Root helper daemon application to use when possible.,security,neutron
3085,report_interval,"Seconds between nodes reporting state to server; should be less than  agent_down_time, best if it is half or less than agent_down_time.",reliability,neutron
3086,log_agent_heartbeats,Log agent heartbeats,reliability,neutron
3087,comment_iptables_rules,Add comments to iptables rules. Set to false to disallow the addition of comments to generated iptables rules that describe each rule's  purpose. System must support the iptables comments module for addition  of comments.,others,neutron
3088,debug_iptables_rules,Duplicate every iptables difference calculation to ensure the format  being generated matches the format of iptables-save. This option should  not be turned on for production systems because it imposes a performance penalty.,performance,neutron
3089,check_child_processes_action,Action to be executed when a child process dies,reliability,neutron
3090,check_child_processes_interval,"Interval between checks of child process liveness (seconds), use 0 to disable",reliability,neutron
3091,kill_scripts_path,"Location of scripts used to kill external processes. Names of scripts here must follow the pattern: '<process-name>-kill' where  <process-name> is name of the process which should be killed using this script. For example, kill script for dnsmasq process should be  named 'dnsmasq-kill'. If path is set to None, then default 'kill'  command will be used to stop processes.",environment,neutron
3092,availability_zone,Availability zone of this node,others,neutron
3093,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests 'origin' header. Format:  '<protocol>://<host>[:<port>]', no trailing slash.  Example: https://horizon.example.com",others,neutron
3094,allow_credentials,Indicate that the actual request can include user credentials,others,neutron
3095,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,others,neutron
3096,max_age,Maximum cache age of CORS preflight requests.,performance,neutron
3097,allow_methods,Indicate which methods can be used during the actual request.,others,neutron
3098,allow_headers,Indicate which header field names may be used during the actual request.,others,neutron
3099,sqlite_synchronous,"If True, SQLite uses synchronous mode.",others,neutron
3100,backend,The back end to use for the database.,environment,neutron
3101,connection,The SQLAlchemy connection string to use to connect to the database.,others,neutron
3102,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,others,neutron
3103,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,neutron
3104,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",others,neutron
3105,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,neutron
3106,min_pool_size,Minimum number of SQL connections to keep open in a pool.,performance,neutron
3107,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,reliability,neutron
3108,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,neutron
3109,retry_interval,Interval between retries of opening a SQL connection.,reliability,neutron
3110,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",reliability,neutron
3111,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,neutron
3112,connection_trace,Add Python stack traces to SQL as comment strings.,debuggability,neutron
3113,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,neutron
3114,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,reliability,neutron
3115,db_retry_interval,Seconds between retries of a database transaction.,reliability,neutron
3116,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",reliability,neutron
3117,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",reliability,neutron
3118,db_max_retries,Maximum retries in case of connection error or deadlock error before  error is raised. Set to -1 to specify an infinite retry count.,reliability,neutron
3119,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',others,neutron
3120,engine,Database engine for which script will be generated when using offline migration.,environment,neutron
3121,www_authenticate_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint.",security,neutron
3122,auth_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint. This option is deprecated in favor of  www_authenticate_uri and will be removed in the S release.",security,neutron
3123,auth_version,API version of the admin Identity API endpoint.,environment,neutron
3124,delay_auth_decision,"Do not handle authorization requests within the middleware, but  delegate the authorization decision to downstream WSGI components.",security,neutron
3125,http_connect_timeout,Request timeout value for communicating with Identity API server.,reliability,neutron
3126,http_request_max_retries,How many times are we trying to reconnect when communicating with Identity API Server.,reliability,neutron
3127,cache,"Request environment key where the Swift cache object is stored. When  auth_token middleware is deployed with a Swift cache, use this option to have the middleware share a caching backend with swift. Otherwise, use  the memcached_servers option instead.",performance,neutron
3128,certfile,Required if identity server requires client certificate,security,neutron
3129,keyfile,Required if identity server requires client certificate,security,neutron
3130,cafile,A PEM encoded Certificate Authority to use when verifying HTTPs connections. Defaults to system CAs.,security,neutron
3131,insecure,Verify HTTPS connections.,security,neutron
3132,region_name,The region in which the identity server can be found.,environment,neutron
3133,signing_dir,Directory used to cache files related to PKI tokens. This option has  been deprecated in the Ocata release and will be removed in the P  release.,security,neutron
3134,memcached_servers,"Optionally specify a list of memcached server(s) to use for caching.  If left undefined, tokens will instead be cached in-process.",performance,neutron
3135,token_cache_time,"In order to prevent excessive effort spent validating tokens, the  middleware caches previously-seen tokens for a configurable duration (in seconds). Set to -1 to disable caching completely.",performance,neutron
3136,memcache_security_strategy,"(Optional) If defined, indicate whether token data should be  authenticated or authenticated and encrypted. If MAC, token data is  authenticated (with HMAC) in the cache. If ENCRYPT, token data is  encrypted and authenticated in the cache. If the value is not one of  these options or empty, auth_token will raise an exception on  initialization.",security,neutron
3137,memcache_secret_key,"(Optional, mandatory if memcache_security_strategy is defined) This string is used for key derivation.",security,neutron
3138,memcache_pool_dead_retry,(Optional) Number of seconds memcached server is considered dead before it is tried again.,reliability,neutron
3139,memcache_pool_maxsize,(Optional) Maximum total number of open connections to every memcached server.,reliability,neutron
3140,memcache_pool_socket_timeout,(Optional) Socket timeout in seconds for communicating with a memcached server.,reliability,neutron
3141,memcache_pool_unused_timeout,(Optional) Number of seconds a connection to memcached is held unused in the pool before it is closed.,reliability,neutron
3142,memcache_pool_conn_get_timeout,(Optional) Number of seconds that an operation will wait to get a memcached client connection from the pool.,reliability,neutron
3143,memcache_use_advanced_pool,(Optional) Use the advanced (eventlet safe) memcached client pool. The advanced pool will only work under python 2.x.,reliability,neutron
3144,include_service_catalog,"(Optional) Indicate whether to set the X-Service-Catalog header. If  False, middleware will not ask for service catalog on token validation  and will not set the X-Service-Catalog header.",others,neutron
3145,enforce_token_bind,Used to control the use and type of token binding. Can be set to:  'disabled' to not check token binding. 'permissive' (default) to  validate binding information if the bind type is of a form known to the  server and ignore it if not. 'strict' like 'permissive' but if the bind  type is unknown the token will be rejected. 'required' any form of token binding is needed to be allowed. Finally the name of a binding method  that must be present in tokens.,security,neutron
3146,hash_algorithms,"Hash algorithms to use for hashing PKI tokens. This may be a single  algorithm or multiple. The algorithms are those supported by Python  standard hashlib.new(). The hashes will be tried in the order given, so  put the preferred one first for performance. The result of the first  hash will be stored in the cache. This will typically be set to multiple values only while migrating from a less secure algorithm to a more  secure one. Once all the old tokens are expired this option should be  set to a single value for better performance.",performance,neutron
3147,service_token_roles,A choice of roles that must be present in a service token. Service  tokens are allowed to request that an expired token can be used and so  this check should tightly control that only actual services should be  sending this token. Roles here are applied as an ANY check so any role  in this list must be present. For backwards compatibility reasons this  currently only affects the allow_expired check.,security,neutron
3148,service_token_roles_required,For backwards compatibility reasons we must let valid service tokens  pass that don't pass the service_token_roles check as valid. Setting  this true will become the default in a future release and should be  enabled if possible.,security,neutron
3149,auth_type,Authentication type to load,security,neutron
3150,auth_section,Config Section from which to load plugin specific options,security,neutron
3151,region_name,Name of nova region to use. Useful if keystone manages more than one region.,environment,neutron
3152,endpoint_type,"Type of the nova endpoint to use.  This endpoint will be looked up in the keystone catalog and should be one of public, internal or admin.",security,neutron
3153,auth_url,Authentication URL,security,neutron
3154,auth_type,Authentication type to load,security,neutron
3155,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,neutron
3156,certfile,PEM encoded client certificate cert file,security,neutron
3157,collect_timing,Collect per-API call timing information.,debuggability,neutron
3158,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,neutron
3159,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,neutron
3160,domain_id,Domain ID to scope to,environment,neutron
3161,domain_name,Domain name to scope to,environment,neutron
3162,insecure,Verify HTTPS connections.,security,neutron
3163,keyfile,PEM encoded client certificate key file,security,neutron
3164,password,User's password,security,neutron
3165,project_domain_id,Domain ID containing project,environment,neutron
3166,project_domain_name,Domain name containing project,environment,neutron
3167,project_id,Project ID to scope to,environment,neutron
3168,project_name,Project name to scope to,environment,neutron
3169,split_loggers,Log requests to multiple loggers.,others,neutron
3170,system_scope,Scope for system operations,environment,neutron
3171,tenant_id,Tenant ID,environment,neutron
3172,tenant_name,Tenant Name,environment,neutron
3173,timeout,Timeout value for http requests,reliability,neutron
3174,trust_id,Trust ID,security,neutron
3175,user_domain_id,User's domain id,security,neutron
3176,user_domain_name,User's domain name,security,neutron
3177,user_id,User id,security,neutron
3178,username,Username,security,neutron
3179,disable_process_locking,Enables or disables inter-process locks.,reliability,neutron
3180,lock_path,"Directory to use for lock files.  For security, the specified  directory should only be writable by the user running the processes that need locking. Defaults to environment variable OSLO_LOCK_PATH. If  external locks are used, a lock path must be set.",security,neutron
3181,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,environment,neutron
3182,idle_timeout,Timeout for inactive connections (in seconds),reliability,neutron
3183,trace,Debug: dump AMQP frames to stdout,debuggability,neutron
3184,ssl,"Attempt to connect via SSL. If no other ssl-related parameters are  given, it will use the system's CA-bundle to verify the server's  certificate.",security,neutron
3185,ssl_ca_file,CA certificate PEM file used to verify the server's certificate,security,neutron
3186,ssl_cert_file,Self-identifying certificate PEM file for client authentication,security,neutron
3187,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),security,neutron
3188,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),security,neutron
3189,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate  matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the  server uses the Server Name Indication TLS extension (rfc6066) to  provide a certificate per virtual host. Set ssl_verify_vhost to True if  the server's SSL certificate uses the virtual host name instead of the  DNS name.",security,neutron
3190,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,security,neutron
3191,sasl_config_dir,Path to directory that contains the SASL configuration,security,neutron
3192,sasl_config_name,Name of configuration file (without .conf suffix),security,neutron
3193,sasl_default_realm,SASL realm to use if no realm present in username,security,neutron
3194,connection_retry_interval,Seconds to pause before attempting to re-connect.,reliability,neutron
3195,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,reliability,neutron
3196,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,reliability,neutron
3197,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,reliability,neutron
3198,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,reliability,neutron
3199,default_reply_timeout,The deadline for an rpc reply message delivery.,reliability,neutron
3200,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,reliability,neutron
3201,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,reliability,neutron
3202,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,reliability,neutron
3203,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy'   - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic'  - use legacy addresses if the message bus does not support routing otherwise use routable addressing,security,neutron
3204,pseudo_vhost,"Enable virtual host support for those message buses that do not  natively support virtual hosting (such as qpidd). When set to true the  virtual host name will be added to all message bus addresses,  effectively creating a private 'subnet' per virtual host. Set to False  if the message bus supports virtual hosting using the 'hostname' field  in the AMQP 1.0 Open performative as the name of the virtual host.",security,neutron
3205,server_request_prefix,address prefix used when sending to a specific server,others,neutron
3206,broadcast_prefix,address prefix used when broadcasting to all servers,others,neutron
3207,group_request_prefix,address prefix when sending to any server in group,others,neutron
3208,rpc_address_prefix,Address prefix for all generated RPC addresses,others,neutron
3209,notify_address_prefix,Address prefix for all generated Notification addresses,others,neutron
3210,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,others,neutron
3211,unicast_address,Appended to the address prefix when sending to a particular  RPC/Notification server. Used by the message bus to identify messages  sent to a single destination.,environment,neutron
3212,anycast_address,Appended to the address prefix when sending to a group of consumers.  Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,environment,neutron
3213,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',environment,neutron
3214,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',environment,neutron
3215,reply_link_credit,Window size for incoming RPC Reply messages.,performance,neutron
3216,rpc_server_credit,Window size for incoming RPC Request messages,performance,neutron
3217,notify_server_credit,Window size for incoming Notification messages,performance,neutron
3218,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify'   - Send Notifications pre-settled,others,neutron
3219,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,performance,neutron
3220,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,reliability,neutron
3221,pool_size,Pool Size for Kafka Consumers,performance,neutron
3222,conn_pool_min_size,The pool size limit for connections expiration policy,reliability,neutron
3223,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,neutron
3224,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,environment,neutron
3225,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,reliability,neutron
3226,producer_batch_size,Size of batch for the producer async send,performance,neutron
3227,enable_auto_commit,Enable asynchronous consumer commits,others,neutron
3228,max_poll_records,The maximum number of records returned in a poll call,performance,neutron
3229,security_protocol,Protocol used to communicate with brokers,security,neutron
3230,sasl_mechanism,Mechanism when security protocol is SASL,security,neutron
3231,ssl_cafile,CA certificate PEM file used to verify the server certificate,security,neutron
3232,driver,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",environment,neutron
3233,transport_url,"A URL representing the messaging driver to use for notifications. If  not set, we fall back to the same configuration used for RPC.",environment,neutron
3234,topics,AMQP topic used for OpenStack notifications.,others,neutron
3235,retry,"The maximum number of attempts to re-send a notification message  which failed to be delivered due to a recoverable error. 0 - No retry,  -1 - indefinite",reliability,neutron
3236,amqp_durable_queues,Use durable queues in AMQP.,others,neutron
3237,amqp_auto_delete,Auto-delete queues in AMQP.,others,neutron
3238,ssl,Connect over SSL.,security,neutron
3239,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",security,neutron
3240,ssl_key_file,SSL key file (valid only if SSL enabled).,security,neutron
3241,ssl_cert_file,SSL cert file (valid only if SSL enabled).,security,neutron
3242,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,security,neutron
3243,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,reliability,neutron
3244,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression  will not be used. This option may not be available in future versions.",performance,neutron
3245,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its  replies. This value should not be longer than rpc_response_timeout.,reliability,neutron
3246,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we  are currently connected to becomes unavailable. Takes effect only if  more than one RabbitMQ node is provided in config.,reliability,neutron
3247,rabbit_login_method,The RabbitMQ login method.,others,neutron
3248,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,reliability,neutron
3249,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,reliability,neutron
3250,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,reliability,neutron
3251,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change  this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when  declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run:  'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",others,neutron
3252,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL  (x-expires). Queues which are unused for the duration of the TTL are  automatically deleted. The parameter affects only reply and fanout  queues.,performance,neutron
3253,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,performance,neutron
3254,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,reliability,neutron
3255,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,reliability,neutron
3256,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,others,neutron
3257,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",others,neutron
3258,policy_file,The file that defines policies.,others,neutron
3259,policy_default_rule,Default rule. Enforced when a requested rule is not found.,reliability,neutron
3260,policy_dirs,"Directories where policy configuration files are stored. They can be  relative to any directory in the search path defined by the config_dir  option, or absolute paths. The file defined by policy_file must exist  for these directories to be searched.  Missing or empty directories are  ignored.",environment,neutron
3261,remote_content_type,Content Type to send and receive data for REST based policy check,others,neutron
3262,remote_ssl_verify_server_crt,server identity verification for REST based policy check,security,neutron
3263,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,environment,neutron
3264,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,environment,neutron
3265,remote_ssl_client_key_file,Absolute path client key file REST based policy check,environment,neutron
3266,user,User that the privsep daemon should run as.,environment,neutron
3267,group,Group that the privsep daemon should run as.,others,neutron
3268,capabilities,List of Linux capabilities retained by the privsep daemon.,others,neutron
3269,thread_pool_size,The number of threads available for privsep to concurrently run processes. Defaults to the number of CPU cores in the system.,performance,neutron
3270,helper_command,"Command to invoke to start the privsep daemon if not using the 'fork' method. If not specified, a default is generated using 'sudo  privsep-helper' and arguments designed to recreate the current  configuration. This command must accept suitable --privsep_context and  --privsep_sock_path arguments.",others,neutron
3271,default_quota,Default number of resource allowed per tenant. A negative value means unlimited.,reliability,neutron
3272,quota_network,Number of networks allowed per tenant. A negative value means unlimited.,performance,neutron
3273,quota_subnet,"Number of subnets allowed per tenant, A negative value means unlimited.",performance,neutron
3274,quota_port,Number of ports allowed per tenant. A negative value means unlimited.,reliability,neutron
3275,quota_driver,Default driver to use for quota checks.,others,neutron
3276,track_quota_usage,Keep in track in the database of current resource quota usage.  Plugins which do not leverage the neutron database should set this flag  to False.,debuggability,neutron
3277,quota_router,Number of routers allowed per tenant. A negative value means unlimited.,reliability,neutron
3278,quota_floatingip,Number of floating IPs allowed per tenant. A negative value means unlimited.,performance,neutron
3279,quota_security_group,Number of security groups allowed per tenant. A negative value means unlimited.,reliability,neutron
3280,quota_security_group_rule,Number of security rules allowed per tenant. A negative value means unlimited.,reliability,neutron
3281,ca_file,CA certificate file to use to verify connecting clients.,security,neutron
3282,cert_file,Certificate file to use when starting the server securely.,security,neutron
3283,key_file,Private key file to use when starting the server securely.,security,neutron
3284,version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",security,neutron
3285,ciphers,Sets the list of available ciphers. value should be a string in the OpenSSL cipher list format.,others,neutron
3286,run_external_periodic_tasks,Some periodic tasks can be run in a separate process. Should we run them here?,others,nova
3287,backdoor_port,"Enable eventlet backdoor.  Acceptable values are 0, <port>, and <start>:<end>, where 0 results in listening on a random tcp port number; <port> results in listening on the specified port  number (and not enabling backdoor if that port is in use); and  <start>:<end> results in listening on the smallest unused  port number within the specified range of port numbers.  The chosen port is displayed in the service's log file.",environment,nova
3288,backdoor_socket,"Enable eventlet backdoor, using the provided path as a unix socket  that can receive connections. This option is mutually exclusive with  'backdoor_port' in that only one should be provided. If both are  provided then the existence of this option overrides the usage of that  option.",reliability,nova
3289,log_options,Enables or disables logging values of all registered options when starting a service (at DEBUG level).,debuggability,nova
3290,graceful_shutdown_timeout,Specify a timeout after which a gracefully shutdown server will exit. Zero value means endless wait.,reliability,nova
3291,rpc_conn_pool_size,Size of RPC connection pool.,performance,nova
3292,conn_pool_min_size,The pool size limit for connections expiration policy,reliability,nova
3293,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,nova
3294,executor_thread_pool_size,Size of executor thread pool when executor is threading or eventlet.,performance,nova
3295,rpc_response_timeout,Seconds to wait for a response from a call.,reliability,nova
3296,transport_url,"The network address and optional user credentials for connecting to  the messaging backend, in URL format. The expected format is:",environment,nova
3297,control_exchange,The default exchange under which topics are scoped. May be overridden by an exchange name specified in the transport_url option.,others,nova
3298,debug,"If set to true, the logging level will be set to DEBUG instead of the default INFO level.",debuggability,nova
3299,log_config_append,"The name of a logging configuration file. This file is appended to  any existing logging configuration files. For details about logging  configuration files, see the Python logging module documentation. Note  that when logging configuration files are used then all logging  configuration is set in the configuration file and other logging  configuration options are ignored (for example, log-date-format).",debuggability,nova
3300,log_date_format,Defines the format string for %(asctime)s in log records. Default:  the value above . This option is ignored if log_config_append is set.,debuggability,nova
3301,log_file,"(Optional) Name of log file to send logging output to. If no default  is set, logging will go to stderr as defined by use_stderr. This option  is ignored if log_config_append is set.",debuggability,nova
3302,log_dir,(Optional) The base directory used for relative log_file  paths. This option is ignored if log_config_append is set.,environment,nova
3303,watch_log_file,Uses logging handler designed to watch file system. When log file is  moved or removed this handler will open a new log file with specified  path instantaneously. It makes sense only if log_file option is  specified and Linux platform is used. This option is ignored if  log_config_append is set.,debuggability,nova
3304,use_syslog,Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later to honor RFC5424. This option is ignored if  log_config_append is set.,debuggability,nova
3305,use_journal,Enable journald for logging. If running in a systemd environment you  may wish to enable journal support. Doing so will use the journal native protocol which includes structured metadata in addition to log  messages.This option is ignored if log_config_append is set.,debuggability,nova
3306,syslog_log_facility,Syslog facility to receive log lines. This option is ignored if log_config_append is set.,debuggability,nova
3307,use_json,Use JSON formatting for logging. This option is ignored if log_config_append is set.,debuggability,nova
3308,use_stderr,Log output to standard error. This option is ignored if log_config_append is set.,debuggability,nova
3309,use_eventlog,Log output to Windows Event Log.,debuggability,nova
3310,log_rotate_interval,The amount of time before the log files are rotated. This option is ignored unless log_rotation_type is setto 'interval'.,reliability,nova
3311,log_rotate_interval_type,Rotation interval type. The time of the last file change (or the time when the service was started) is used when scheduling the next  rotation.,reliability,nova
3312,max_logfile_count,Maximum number of rotated log files.,reliability,nova
3313,max_logfile_size_mb,Log file maximum size in MB. This option is ignored if 'log_rotation_type' is not set to 'size'.,performance,nova
3314,log_rotation_type,Log rotation type.,debuggability,nova
3315,logging_context_format_string,Format string to use for log messages with context. Used by oslo_log.formatters.ContextFormatter,debuggability,nova
3316,logging_default_format_string,Format string to use for log messages when context is undefined. Used by oslo_log.formatters.ContextFormatter,debuggability,nova
3317,logging_debug_format_suffix,Additional data to append to log message when logging level for the  message is DEBUG. Used by oslo_log.formatters.ContextFormatter,debuggability,nova
3318,logging_exception_prefix,Prefix each line of exception output with this format. Used by oslo_log.formatters.ContextFormatter,debuggability,nova
3319,logging_user_identity_format,Defines the format string for %(user_identity)s that is used in  logging_context_format_string. Used by  oslo_log.formatters.ContextFormatter,debuggability,nova
3320,default_log_levels,List of package logging levels in logger=LEVEL pairs. This option is ignored if log_config_append is set.,debuggability,nova
3321,publish_errors,Enables or disables publication of error events.,debuggability,nova
3322,instance_format,The format for an instance that is passed with the log message.,debuggability,nova
3323,instance_uuid_format,The format for an instance UUID that is passed with the log message.,debuggability,nova
3324,rate_limit_interval,"Interval, number of seconds, of log rate limiting.",reliability,nova
3325,rate_limit_burst,Maximum number of logged messages per rate_limit_interval.,reliability,nova
3326,rate_limit_except_level,"Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty string. Logs with level greater or equal to  rate_limit_except_level are not filtered. An empty string means that all levels are filtered.",debuggability,nova
3327,fatal_deprecations,Enables or disables fatal status of deprecations.,others,nova
3328,internal_service_availability_zone,Availability zone for internal services.,others,nova
3329,default_availability_zone,Default availability zone for compute services.,others,nova
3330,default_schedule_zone,Default availability zone for instances.,environment,nova
3331,password_length,Length of generated instance admin passwords.,security,nova
3332,instance_usage_audit_period,Time period to generate instance usages for. It is possible to define optional offset to given period by appending @ character followed by a number defining offset.,security,nova
3333,use_rootwrap_daemon,Start and use a daemon that can run the commands that need to be run with root privileges. This option is usually enabled on nodes that run nova compute processes.,security,nova
3334,rootwrap_config,Path to the rootwrap configuration file.,environment,nova
3335,tempdir,Explicitly specify the temporary working directory.,environment,nova
3336,compute_driver,Defines which driver to use for controlling virtualization.,others,nova
3337,allow_resize_to_same_host,Allow destination machine to match source for resize. Useful when testing in single-host environments. By default it is not allowed to resize to the same host. Setting this option to true will add the same host to the destination options. Also set to true if you allow the ServerGroupAffinityFilter and need to resize.,performance,nova
3338,non_inheritable_image_properties,Image properties that should not be inherited from the instance when taking a snapshot.,performance,nova
3339,max_local_block_devices,Maximum number of devices that will result in a local image being created on the hypervisor node.,reliability,nova
3340,compute_monitors,"A comma-separated list of monitors that can be used for getting compute metrics. You can use the alias/name from the setuptools entry points for nova.compute.monitors.* namespaces. If no namespace is supplied, the 'cpu.' namespace is assumed for backwards-compatibility.",others,nova
3341,default_ephemeral_format,The default format an ephemeral_volume will be formatted with on creation.,others,nova
3342,vif_plugging_is_fatal,Determine if instance should boot or fail on VIF plugging timeout.,reliability,nova
3343,vif_plugging_timeout,Timeout for Neutron VIF plugging event message arrival.,reliability,nova
3344,injected_network_template,Path to '/etc/network/interfaces' template.,environment,nova
3345,preallocate_images,The image preallocation mode to use.,performance,nova
3346,use_cow_images,Enable use of copy-on-write (cow) images.,reliability,nova
3347,force_raw_images,Force conversion of backing images to raw format.,others,nova
3348,virt_mkfs,Name of the mkfs commands for ephemeral device.,environment,nova
3349,resize_fs_using_block_device,Enable resizing of filesystems via a block device.,performance,nova
3350,timeout_nbd,"Amount of time, in seconds, to wait for NBD device start up.",reliability,nova
3351,image_cache_subdirectory_name,Location of cached images.,environment,nova
3352,remove_unused_base_images,Should unused base images be removed?,others,nova
3353,remove_unused_original_minimum_age_seconds,Unused unresized base images younger than this will not be removed.,reliability,nova
3354,pointer_model,Generic property to specify the pointer type.,others,nova
3355,vcpu_pin_set,Defines which physical CPUs (pCPUs) can be used by instance virtual CPUs (vCPUs).,performance,nova
3356,reserved_huge_pages,Number of huge/large memory pages to reserved per NUMA host cell.,performance,nova
3357,reserved_host_disk_mb,"Amount of disk resources in MB to make them always available to host. The disk usage gets reported back to the scheduler from nova-compute running on the compute nodes. To prevent the disk resources from being considered as available, this option can be used to reserve disk space for that host.",performance,nova
3358,reserved_host_memory_mb,"Amount of memory in MB to reserve for the host so that it is always available to host processes. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host memory from being considered as available, this option is used to reserve memory for the host.",performance,nova
3359,reserved_host_cpus,"Number of physical CPUs to reserve for the host. The host resources usage is reported back to the scheduler continuously from nova-compute running on the compute node. To prevent the host CPU from being considered as available, this option is used to reserve random pCPU(s) for the host.",performance,nova
3360,cpu_allocation_ratio,This option helps you specify virtual CPU to physical CPU allocation ratio.,performance,nova
3361,ram_allocation_ratio,This option helps you specify virtual RAM to physical RAM allocation ratio.,performance,nova
3362,disk_allocation_ratio,This option helps you specify virtual disk to physical disk allocation ratio.,performance,nova
3363,initial_cpu_allocation_ratio,This option helps you specify initial virtual CPU to physical CPU allocation ratio.,performance,nova
3364,initial_ram_allocation_ratio,This option helps you specify initial virtual RAM to physical RAM allocation ratio.,performance,nova
3365,initial_disk_allocation_ratio,This option helps you specify initial virtual disk to physical disk allocation ratio.,performance,nova
3366,console_host,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3367,default_access_ip_network_name,"Name of the network to be used to set access IPs for instances. If there are multiple IPs to choose from, an arbitrary one will be chosen.",environment,nova
3368,instances_path,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3369,instance_usage_audit,This option enables periodic compute.instance.exists notifications. Each compute node must be configured to generate system usage data. These notifications are consumed by OpenStack Telemetry service.,security,nova
3370,live_migration_retry_count,Maximum number of 1 second retries in live_migration. It specifies number of retries to iptables when it complains. It happens when an user continuously sends live-migration request to same host leading to concurrent request to iptables.,reliability,nova
3371,resume_guests_state_on_host_boot,This option specifies whether to start guests that were running before the host rebooted. It ensures that all of the instances on a Nova compute node resume their state each time the compute node boots or restarts.,reliability,nova
3372,network_allocate_retries,Number of times to retry network allocation. It is required to attempt network allocation retries if the virtual interface plug fails.,reliability,nova
3373,max_concurrent_builds,"Limits the maximum number of instance builds to run concurrently by nova-compute. Compute service can attempt to build an infinite number of instances, if asked to do so. This limit is enforced to avoid building unlimited instance concurrently on a compute node. This value can be set per compute node.",performance,nova
3374,max_concurrent_live_migrations,Maximum number of live migrations to run concurrently. This limit is enforced to avoid outbound live migrations overwhelming the host/network and causing failures. It is not recommended that you change this unless you are very sure that doing so is safe and stable in your environment.,reliability,nova
3375,block_device_allocate_retries,"Number of times to retry block device allocation on failures. Starting with Liberty, Cinder can use image volume cache. This may help with block device allocation performance. Look at the cinder image_volume_cache_enabled configuration option.",reliability,nova
3376,sync_power_state_pool_size,Number of greenthreads available for use to sync power states.,performance,nova
3377,image_cache_manager_interval,Number of seconds to wait between runs of the image cache manager.,reliability,nova
3378,bandwidth_poll_interval,Interval to pull network bandwidth usage info.,reliability,nova
3379,sync_power_state_interval,Interval to sync power states between the database and the hypervisor.,reliability,nova
3380,heal_instance_info_cache_interval,Interval between instance network information cache updates.,reliability,nova
3381,reclaim_instance_interval,Interval for reclaiming deleted instances.,reliability,nova
3382,volume_usage_poll_interval,Interval for gathering volume usages.,reliability,nova
3383,shelved_poll_interval,Interval for polling shelved instances to offload.,reliability,nova
3384,shelved_offload_time,Time before a shelved instance is eligible for removal from a host.,reliability,nova
3385,instance_delete_interval,Interval for retrying failed instance file deletes.,reliability,nova
3386,block_device_allocate_retries_interval,Interval (in seconds) between block device allocation retries on failures.,reliability,nova
3387,scheduler_instance_sync_interval,Interval between sending the scheduler a list of current instance UUIDs to verify that its view of instances is in sync with nova.,reliability,nova
3388,update_resources_interval,Interval for updating compute resources.,reliability,nova
3389,reboot_timeout,Time interval after which an instance is hard rebooted automatically.,reliability,nova
3390,instance_build_timeout,Maximum time in seconds that an instance can take to build.,reliability,nova
3391,rescue_timeout,Interval to wait before un-rescuing an instance stuck in RESCUE.,reliability,nova
3392,resize_confirm_window,Automatically confirm resizes after N seconds.,reliability,nova
3393,shutdown_timeout,Total time to wait in seconds for an instance to perform a clean shutdown.,reliability,nova
3394,running_deleted_instance_action,The compute service periodically checks for instances that have been deleted in the database but remain running on the compute node. The above option enables action to be taken when such instances are identified.,reliability,nova
3395,running_deleted_instance_poll_interval,"Time interval in seconds to wait between runs for the clean up action. If set to 0, above check will be disabled. If 'running_deleted_instance _action' is set to 'log' or 'reap', a value greater than 0 must be set.",reliability,nova
3396,running_deleted_instance_timeout,Time interval in seconds to wait for the instances that have been marked as deleted in database to be eligible for cleanup.,reliability,nova
3397,maximum_instance_delete_attempts,The number of times to attempt to reap an instance's files.,reliability,nova
3398,osapi_compute_unique_server_name_scope,Sets the scope of the check for unique instance names.,reliability,nova
3399,enable_new_services,Enable new nova-compute services on this host automatically.,others,nova
3400,instance_name_template,Template string to be used to generate instance names.,others,nova
3401,migrate_max_retries,Number of times to retry live-migration before failing.,reliability,nova
3402,config_drive_format,Configuration drive format,others,nova
3403,force_config_drive,Force injection to take place on a config drive,performance,nova
3404,mkisofs_cmd,Name or path of the tool used for ISO image creation,environment,nova
3405,default_flavor,Default flavor to use for the EC2 API only. The Nova API does not support a default flavor.,others,nova
3406,my_ip,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3407,my_block_storage_ip,The IP address which is used to connect to the block storage network.,environment,nova
3408,host,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3409,dhcpbridge_flagfile,"This option is a list of full paths to one or more configuration files for dhcpbridge. In most cases the default path of '/etc/nova/nova-dhcpbridge.conf' should be sufficient, but if you have special needs for configuring dhcpbridge, you can change or add to this list.",environment,nova
3410,networks_path,The location where the network configuration files will be kept. The default is the 'networks' directory off of the location where nova's Python module is installed.,environment,nova
3411,public_interface,This is the name of the network interface for public IP addresses. The default is 'eth0'.,security,nova
3412,dhcpbridge,The location of the binary nova-dhcpbridge. By default it is the binary named 'nova-dhcpbridge' that is installed with all the other nova binaries.,environment,nova
3413,routing_source_ip,The public IP address of the network host.,environment,nova
3414,dhcp_lease_time,"The lifetime of a DHCP lease, in seconds. The default is 86400 (one day).",reliability,nova
3415,dns_server,"Despite the singular form of the name of this option, it is actually a list of zero or more server addresses that dnsmasq will use for DNS nameservers. If this is not empty, dnsmasq will not read /etc/resolv.conf, but will only use the servers specified in this option. If the option use_network_dns_servers is True, the dns1 and dns2 servers from the network will be appended to this list, and will be used as DNS servers, too.",environment,nova
3416,use_network_dns_servers,"When this option is set to True, the dns1 and dns2 servers for the network specified by the user on boot will be used for DNS, as well as any specified in the dns_server option.",others,nova
3417,dmz_cidr,This option is a list of zero or more IP address ranges in your network's DMZ that should be accepted.,security,nova
3418,force_snat_range,"This is a list of zero or more IP ranges that traffic from the routing_source_ip will be SNATted to. If the list is empty, then no SNAT rules are created.",environment,nova
3419,dnsmasq_config_file,"The path to the custom dnsmasq configuration file, if any.",environment,nova
3420,linuxnet_interface_driver,"This is the class used as the ethernet device driver for linuxnet bridge operations. The default value should be all you need for most cases, but if you wish to use a customized class, set this option to the full dot-separated import path for that class.",environment,nova
3421,linuxnet_ovs_integration_bridge,The name of the Open vSwitch bridge that is used with linuxnet when connecting with Open vSwitch.',environment,nova
3422,send_arp_for_ha,"When True, when a device starts up, and upon binding floating IP addresses, arp messages will be sent to ensure that the arp caches on the compute hosts are up-to-date.",reliability,nova
3423,send_arp_for_ha_count,"When arp messages are configured to be sent, they will be sent with the count set to the value of this option. Of course, if this is set to zero, no arp messages will be sent.",performance,nova
3424,use_single_default_gateway,"When set to True, only the firt nic of a VM will get its default gateway from the DHCP server.",security,nova
3425,forward_bridge_interface,"One or more interfaces that bridges can forward traffic to. If any of the items in this list is the special keyword 'all', then all traffic will be forwarded.",environment,nova
3426,metadata_host,This option determines the IP address for the network metadata API server.,environment,nova
3427,metadata_port,This option determines the port used for the metadata API server.,environment,nova
3428,iptables_top_regex,"This expression, if defined, will select any matching iptables rules and place them at the top when applying metadata changes to the rules.",others,nova
3429,iptables_bottom_regex,"This expression, if defined, will select any matching iptables rules and place them at the bottom when applying metadata changes to the rules.",others,nova
3430,iptables_drop_action,"By default, packets that do not pass the firewall are DROPped. In many cases, though, an operator may find it more useful to change this from DROP to REJECT, so that the user issuing those packets may have a better idea as to what's going on, or LOGDROP in order to record the blocked traffic before DROPping.",security,nova
3431,defer_iptables_apply,Defer application of IPTables rules until after init phase.,others,nova
3432,ovs_vsctl_timeout,"This option represents the period of time, in seconds, that the ovs_vsctl calls will wait for a response from the database before timing out. A setting of 0 means that the utility should wait forever for a response.",reliability,nova
3433,fake_network,This option is used mainly in testing to avoid calls to the underlying network utilities.,others,nova
3434,ebtables_exec_attempts,This option determines the number of times to retry ebtables commands before giving up. The minimum number of retries is 1.,reliability,nova
3435,ebtables_retry_interval,"This option determines the time, in seconds, that the system will sleep in between ebtables retries. Note that each successive retry waits a multiple of this value, so for example, if this is set to the default of 1.0 seconds, and ebtables_exec_attempts is 4, after the first failure, the system will sleep for 1 * 1.0 seconds, after the second failure it will sleep 2 * 1.0 seconds, and after the third failure it will sleep 3 * 1.0 seconds.",reliability,nova
3436,use_neutron,Enable neutron as the backend for networking.,others,nova
3437,flat_injected,"This option determines whether the network setup information is injected into the VM before it is booted. While it was originally designed to be used only by nova-network, it is also used by the vmware and xenapi virt drivers to control whether network information is injected into a VM. The libvirt virt driver also uses it when we use config_drive to configure network to control whether network information is injected into a VM.",others,nova
3438,flat_network_bridge,This option determines the bridge used for simple network interfaces when no bridge is specified in the VM creation request.,environment,nova
3439,flat_network_dns,"This is the address of the DNS server for a simple network. If this option is not specified, the default of '8.8.4.4' is used.",environment,nova
3440,flat_interface,"This option is the name of the virtual interface of the VM on which the bridge will be built. While it was originally designed to be used only by nova-network, it is also used by libvirt for the bridge interface name.",environment,nova
3441,vlan_start,"This is the VLAN number used for private networks. Note that the when creating the networks, if the specified number has already been assigned, nova-network will increment this number until it finds an available VLAN.",reliability,nova
3442,vlan_interface,"This option is the name of the virtual interface of the VM on which the VLAN bridge will be built. While it was originally designed to be used only by nova-network, it is also used by libvirt and xenapi for the bridge interface name.",environment,nova
3443,num_networks,"This option represents the number of networks to create if not explicitly specified when the network is created. The only time this is used is if a CIDR is specified, but an explicit network_size is not. In that case, the subnets are created by diving the IP address space of the CIDR by num_networks. The resulting subnet sizes cannot be larger than the configuration option network_size; in that event, they are reduced to network_size, and a warning is logged.",performance,nova
3444,vpn_ip,This option is no longer used since the /os-cloudpipe API was removed in the 16.0.0 Pike release. This is the public IP address for the cloudpipe VPN servers. It defaults to the IP address of the host.,security,nova
3445,vpn_start,This is the port number to use as the first VPN port for private networks.,environment,nova
3446,network_size,This option determines the number of addresses in each private subnet.,performance,nova
3447,fixed_range_v6,This option determines the fixed IPv6 address block when creating a network.,environment,nova
3448,gateway,This is the default IPv4 gateway. It is used only in the testing suite.,security,nova
3449,gateway_v6,This is the default IPv6 gateway. It is used only in the testing suite.,security,nova
3450,cnt_vpn_clients,This option represents the number of IP addresses to reserve at the top of the address range for VPN clients. It also will be ignored if the configuration option for network_manager is not set to the default of 'nova.network.manager.VlanManager'.,reliability,nova
3451,fixed_ip_disassociate_timeout,"This is the number of seconds to wait before disassociating a deallocated fixed IP address. This is only used with the nova-network service, and has no effect when using neutron for networking.",reliability,nova
3452,create_unique_mac_address_attempts,This option determines how many times nova-network will attempt to create a unique MAC address before giving up and raising a VirtualInterfaceMacAddressException error.,reliability,nova
3453,teardown_unused_network_gateway,"Determines whether unused gateway devices, both VLAN and bridge, are deleted if the network is in nova-network VLAN mode and is multi-hosted.",others,nova
3454,force_dhcp_release,"When this option is True, a call is made to release the DHCP for the instance when that instance is terminated.",reliability,nova
3455,update_dns_entries,"When this option is True, whenever a DNS entry must be updated, a fanout cast message is sent to all network hosts to update their DNS entries in multi-host mode.",reliability,nova
3456,dns_update_periodic_interval,"This option determines the time, in seconds, to wait between refreshing DNS entries for the network.",reliability,nova
3457,dhcp_domain,This option allows you to specify the domain for the DHCP server.,others,nova
3458,l3_lib,This option allows you to specify the L3 management library to be used.,others,nova
3459,share_dhcp_address,THIS VALUE SHOULD BE SET WHEN CREATING THE NETWORK.,environment,nova
3460,ldap_dns_url,URL for LDAP server which will store DNS entries,environment,nova
3461,ldap_dns_user,Bind user for LDAP server,security,nova
3462,ldap_dns_password,Bind user's password for LDAP server,security,nova
3463,ldap_dns_soa_hostmaster,Hostmaster for LDAP DNS driver Statement of Authority,security,nova
3464,ldap_dns_servers,DNS Servers for LDAP DNS driver,reliability,nova
3465,ldap_dns_base_dn,Base distinguished name for the LDAP search query,environment,nova
3466,ldap_dns_soa_refresh,Refresh interval (in seconds) for LDAP DNS driver Start of Authority,reliability,nova
3467,ldap_dns_soa_retry,Retry interval (in seconds) for LDAP DNS driver Start of Authority,reliability,nova
3468,ldap_dns_soa_expiry,Expiry interval (in seconds) for LDAP DNS driver Start of Authority,reliability,nova
3469,ldap_dns_soa_minimum,Minimum interval (in seconds) for LDAP DNS driver Start of Authority,reliability,nova
3470,multi_host,Default value for multi_host in networks.,environment,nova
3471,network_driver,Driver to use for network creation.,environment,nova
3472,firewall_driver,Firewall driver to use with nova-network service.,security,nova
3473,allow_same_net_traffic,Determine whether to allow network traffic from same network.,performance,nova
3474,default_floating_pool,Default pool for floating IPs.,reliability,nova
3475,auto_assign_floating_ip,Autoassigning floating IP to VM,others,nova
3476,floating_ip_dns_manager,Full class name for the DNS Manager for floating IPs.,security,nova
3477,instance_dns_manager,Full class name for the DNS Manager for instance IPs.,security,nova
3478,instance_dns_domain,"If specified, Nova checks if the availability_zone of every instance matches what the database says the availability_zone should be for the specified dns_domain.",security,nova
3479,use_ipv6,Assign IPv6 and IPv4 addresses when creating instances.,others,nova
3480,ipv6_backend,Abstracts out IPv6 address generation to pluggable backends.,others,nova
3481,enable_network_quota,This option is used to enable or disable quota checking for tenant networks.,others,nova
3482,quota_networks,This option controls the number of private networks that can be created per project (or per tenant).,others,nova
3483,network_manager,Full class name for the Manager for network,others,nova
3484,record,"Filename that will be used for storing websocket frames received and sent by a proxy service (like VNC, spice, serial) running on this host. If this is not set, no recording will be done.",others,nova
3485,daemon,Run as a background process.,others,nova
3486,ssl_only,Disallow non-encrypted connections.,security,nova
3487,source_is_ipv6,Set to True if source host is addressed with IPv6.,others,nova
3488,cert,Path to SSL certificate file.,security,nova
3489,key,SSL key file (if separate from cert).,security,nova
3490,web,Path to directory with content which will be served by a web server.,environment,nova
3491,pybasedir,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3492,bindir,The directory where the Nova binaries are installed.,environment,nova
3493,state_path,The top-level directory for maintaining Nova's state.,environment,nova
3494,long_rpc_timeout,"This option allows setting an alternate timeout value for RPC calls that have the potential to take a long time. If set, RPC calls to other services will use this value for the timeout (in seconds) instead of the global rpc_response_timeout value.",reliability,nova
3495,report_interval,Number of seconds indicating how frequently the state of services on a given hypervisor is reported. Nova needs to know this to determine the overall health of the deployment.,reliability,nova
3496,service_down_time,Maximum time in seconds since last check-in for up service,reliability,nova
3497,periodic_enable,Enable periodic tasks.,reliability,nova
3498,periodic_fuzzy_delay,Number of seconds to randomly delay when starting the periodic task scheduler to reduce stampeding.,reliability,nova
3499,enabled_apis,List of APIs to be enabled by default.,others,nova
3500,enabled_ssl_apis,List of APIs with enabled SSL.,security,nova
3501,osapi_compute_listen,IP address on which the OpenStack API will listen.,environment,nova
3502,osapi_compute_listen_port,Port on which the OpenStack API will listen.,environment,nova
3503,osapi_compute_workers,Number of workers for OpenStack API service. The default will be the number of CPUs available.,performance,nova
3504,metadata_listen,IP address on which the metadata API will listen.,environment,nova
3505,metadata_listen_port,Port on which the metadata API will listen.,environment,nova
3506,metadata_workers,Number of workers for metadata service. If not specified the number of available CPUs will be used.,performance,nova
3507,servicegroup_driver,This option specifies the driver to be used for the servicegroup service.,environment,nova
3508,auth_strategy,Determine the strategy to use for authentication.,security,nova
3509,use_forwarded_for,"When True, the 'X-Forwarded-For' header is treated as the canonical remote address. When False (the default), the 'remote_address' header is used.",others,nova
3510,config_drive_skip_versions,"When gathering the existing metadata for a config drive, the EC2-style metadata is returned for all versions that don't appear in this option. As of the Liberty release, the available versions are:",environment,nova
3511,vendordata_providers,A list of vendordata providers.,environment,nova
3512,vendordata_dynamic_targets,A list of targets for the dynamic vendordata provider. These targets are of the form <name>@<url>.,environment,nova
3513,vendordata_dynamic_ssl_certfile,Path to an optional certificate file or CA bundle to verify dynamic vendordata REST services ssl certificates against.,security,nova
3514,vendordata_dynamic_connect_timeout,Maximum wait time for an external REST service to connect.,reliability,nova
3515,vendordata_dynamic_read_timeout,Maximum wait time for an external REST service to return data once connected.,reliability,nova
3516,vendordata_dynamic_failure_fatal,Should failures to fetch dynamic vendordata be fatal to instance boot?,reliability,nova
3517,metadata_cache_expiration,"This option is the time (in seconds) to cache metadata. When set to 0, metadata caching is disabled entirely; this is generally not recommended for performance reasons. Increasing this setting should improve response times of the metadata API when under heavy load. Higher values may increase memory usage, and result in longer times for host metadata changes to take effect.",reliability,nova
3518,local_metadata_per_cell,"Indicates that the nova-metadata API service has been deployed per-cell, so that we can have better performance and data isolation in a multi-cell deployment. Users should consider the use of this configuration depending on how neutron is setup. If you have networks that span cells, you might need to run nova-metadata API service globally. If your networks are segmented along cell boundaries, then you can run nova-metadata API service per cell. When running nova-metadata API service per cell, you should also configure each Neutron metadata-agent to point to the corresponding nova-metadata API service.",performance,nova
3519,vendordata_jsonfile_path,"Cloud providers may store custom data in vendor data file that will then be available to the instances via the metadata service, and to the rendering of config-drive. The default class for this, JsonFileVendorData, loads this information from a JSON file, whose path is configured by this option. If there is no path set by this option, the class returns an empty dictionary.",environment,nova
3520,max_limit,"As a query can potentially return many thousands of items, you can limit the maximum number of items in a single response by setting this option.",performance,nova
3521,compute_link_prefix,"This string is prepended to the normal URL that is returned in links to the OpenStack Compute API. If it is empty (the default), the URLs are returned unchanged.",environment,nova
3522,glance_link_prefix,"This string is prepended to the normal URL that is returned in links to Glance resources. If it is empty (the default), the URLs are returned unchanged.",environment,nova
3523,instance_list_per_project_cells,"When enabled, this will cause the API to only query cell databases in which the tenant has mapped instances. This requires an additional (fast) query in the API database before each list, but also (potentially) limits the number of cell databases that must be queried to provide the result. If you have a small number of cells, or tenants are likely to have instances in all cells, then this should be False. If you have many cells, especially if you confine tenants to a small subset of those cells, this should be True.",performance,nova
3524,instance_list_cells_batch_strategy,"This controls the method by which the API queries cell databases in smaller batches during large instance list operations. If batching is performed, a large instance list operation will request some fraction of the overall API limit from each cell database initially, and will re-request that same batch size as records are consumed (returned) from each cell as necessary. Larger batches mean less chattiness between the API and the database, but potentially more wasted effort processing the results from the database which will not be returned to the user. Any strategy will yield a batch size of at least 100 records, to avoid a user causing many tiny database queries in their request.",performance,nova
3525,instance_list_cells_batch_fixed_size,"This controls the batch size of instances requested from each cell database if instance_list_cells_batch_strategy` is set to fixed. This integral value will define the limit issued to each cell every time a batch of instances is requested, regardless of the number of cells in the system or any other factors. Per the general logic called out in the documentation for instance_list_cells_batch_strategy, the minimum value for this is 100 records per batch.",performance,nova
3526,list_records_by_skipping_down_cells,"When set to False, this will cause the API to return a 500 error if there is an infrastructure failure like non-responsive cells. If you want the API to skip the down cells and return the results from the up cells set this option to True.",debuggability,nova
3527,use_neutron_default_nets,"When True, the TenantNetworkController will query the Neutron API to get the default networks to use.",others,nova
3528,neutron_default_tenant_id,Tenant ID for getting the default network from Neutron API (also referred in some places as the 'project ID') to use.,environment,nova
3529,enable_instance_password,"Enables returning of the instance password by the relevant server API calls such as create, rebuild, evacuate, or rescue. If the hypervisor does not support password injection, then the password returned will not be correct, so if your hypervisor does not support password injection, set this to False.",security,nova
3530,connection,The SQLAlchemy connection string to use to connect to the database. Do not set this for the nova-compute service.,others,nova
3531,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',environment,nova
3532,sqlite_synchronous,"If True, SQLite uses synchronous mode.",others,nova
3533,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,environment,nova
3534,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,nova
3535,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,nova
3536,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,reliability,nova
3537,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,nova
3538,retry_interval,Interval between retries of opening a SQL connection.,reliability,nova
3539,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",reliability,nova
3540,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,nova
3541,connection_trace,Add Python stack traces to SQL as comment strings.,debuggability,nova
3542,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,nova
3543,barbican_endpoint,"Use this endpoint to connect to Barbican, for example: 'http://localhost:9311/'",environment,nova
3544,barbican_api_version,"Version of the Barbican API, for example: 'v1'",environment,nova
3545,auth_endpoint,Use this endpoint to connect to Keystone,environment,nova
3546,retry_delay,Number of seconds to wait before retrying poll for key creation completion,reliability,nova
3547,number_of_retries,Number of times to retry poll for key creation completion,reliability,nova
3548,verify_ssl,"Specifies if insecure TLS (https) requests. If False, the server's certificate will not be validated",security,nova
3549,barbican_endpoint_type,"Specifies the type of endpoint.  Allowed values are: public, private, and admin",security,nova
3550,config_prefix,Prefix for building the configuration dictionary for the cache  region. This should not need to be changed unless there is another  dogpile.cache region with the same configuration name.,environment,nova
3551,expiration_time,"Default TTL, in seconds, for any cached item in the dogpile.cache  region. This applies to any cached method that doesn't have an explicit  cache expiration time defined for it.",reliability,nova
3552,backend,"Cache backend module. For eventlet-based or environments with  hundreds of threaded servers, Memcache with pooling  (oslo_cache.memcache_pool) is recommended. For environments with less  than 100 threaded servers, Memcached (dogpile.cache.memcached) or Redis  (dogpile.cache.redis) is recommended. Test environments with a single  instance of the server can use the dogpile.cache.memory backend.",performance,nova
3553,backend_argument,Arguments supplied to the backend module. Specify this option once  per argument to be passed to the dogpile.cache backend. Example format:  '<argname>:<value>'.,others,nova
3554,proxies,Proxy classes to import that will affect the way the dogpile.cache  backend functions. See the dogpile.cache documentation on  changing-backend-behavior.,reliability,nova
3555,enabled,Global toggle for caching.,performance,nova
3556,debug_cache_backend,"Extra debugging from the cache backend (cache keys,  get/set/delete/etc calls). This is only really useful if you need to see the specific cache-backend get/set/delete calls with the keys/values.   Typically this should be left set to false.",debuggability,nova
3557,memcache_servers,Memcache servers in the format of 'host:port'. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,environment,nova
3558,memcache_dead_retry,Number of seconds memcached server is considered dead before it is  tried again. (dogpile.cache.memcache and oslo_cache.memcache_pool  backends only).,reliability,nova
3559,memcache_socket_timeout,Timeout in seconds for every call to a server. (dogpile.cache.memcache and oslo_cache.memcache_pool backends only).,reliability,nova
3560,memcache_pool_maxsize,Max total number of open connections to every memcached server. (oslo_cache.memcache_pool backend only).,reliability,nova
3561,memcache_pool_unused_timeout,Number of seconds a connection to memcached is held unused in the  pool before it is closed. (oslo_cache.memcache_pool backend only).,reliability,nova
3562,memcache_pool_connection_get_timeout,Number of seconds that an operation will wait to get a memcache client connection.,reliability,nova
3563,enable,Enable cell v1 functionality.,others,nova
3564,name,Name of the current cell.,others,nova
3565,capabilities,Cell capabilities.,performance,nova
3566,call_timeout,Call timeout.,reliability,nova
3567,reserve_percent,Reserve percentage,reliability,nova
3568,cell_type,Type of cell.,others,nova
3569,mute_child_interval,Mute child interval.,reliability,nova
3570,bandwidth_update_interval,Bandwidth update interval.,reliability,nova
3571,instance_update_sync_database_limit,Instance update sync database limit.,reliability,nova
3572,mute_weight_multiplier,Mute weight multiplier.,others,nova
3573,ram_weight_multiplier,Ram weight multiplier.,others,nova
3574,offset_weight_multiplier,Offset weight multiplier,others,nova
3575,instance_updated_at_threshold,Instance updated at threshold,reliability,nova
3576,instance_update_num_instances,Instance update num instances,others,nova
3577,max_hop_count,Maximum hop count,reliability,nova
3578,scheduler,Cells scheduler.,performance,nova
3579,rpc_driver_queue_base,RPC driver queue base.,environment,nova
3580,scheduler_filter_classes,Scheduler filter classes.,others,nova
3581,scheduler_weight_classes,Scheduler weight classes.,others,nova
3582,scheduler_retries,Scheduler retries.,reliability,nova
3583,scheduler_retry_delay,Scheduler retry delay.,reliability,nova
3584,db_check_interval,DB check interval.,reliability,nova
3585,cells_config,Optional cells configuration.,others,nova
3586,catalog_info,Info to match when looking for cinder in the service catalog.,others,nova
3587,endpoint_template,If this option is set then it will override service catalog lookup with this template for cinder endpoint,others,nova
3588,os_region_name,Region name of this node. This is used when picking the URL in the service catalog.,others,nova
3589,http_retries,Number of times cinderclient should retry on any failed http call. 0 means connection is attempted only once. Setting it to any positive integer means that on failure connection is retried that many times e.g. setting it to 3 means total attempts to connect will be 4.,reliability,nova
3590,cross_az_attach,Allow attach between instance and volume in different availability zones.,others,nova
3591,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
3592,certfile,PEM encoded client certificate cert file,security,nova
3593,keyfile,PEM encoded client certificate key file,security,nova
3594,insecure,Verify HTTPS connections.,security,nova
3595,timeout,Timeout value for http requests,reliability,nova
3596,collect_timing,Collect per-API call timing information.,debuggability,nova
3597,split_loggers,Log requests to multiple loggers.,debuggability,nova
3598,auth_type,Authentication type to load,security,nova
3599,auth_section,Config Section from which to load plugin specific options,security,nova
3600,auth_url,Authentication URL,security,nova
3601,system_scope,Scope for system operations,reliability,nova
3602,domain_id,Domain ID to scope to,environment,nova
3603,domain_name,Domain name to scope to,environment,nova
3604,project_id,Project ID to scope to,environment,nova
3605,project_name,Project name to scope to,environment,nova
3606,project_domain_id,Domain ID containing project,environment,nova
3607,project_domain_name,Domain name containing project,environment,nova
3608,trust_id,Trust ID,security,nova
3609,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
3610,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
3611,user_id,User ID,environment,nova
3612,username,Username,environment,nova
3613,user_domain_id,User's domain id,environment,nova
3614,user_domain_name,User's domain name,environment,nova
3615,password,User's password,security,nova
3616,tenant_id,Tenant ID,environment,nova
3617,tenant_name,Tenant Name,environment,nova
3618,consecutive_build_service_disable_threshold,Enables reporting of build failures to the scheduler.,debuggability,nova
3619,shutdown_retry_interval,Time to wait in seconds before resending an ACPI shutdown signal to instances.,reliability,nova
3620,resource_provider_association_refresh,"Interval for updating nova-compute-side cache of the compute node resource provider's inventories, aggregates, and traits.",reliability,nova
3621,cpu_shared_set,Defines which physical CPUs (pCPUs) will be used for best-effort guest vCPU resources.,performance,nova
3622,live_migration_wait_for_vif_plug,Determine if the source compute host should wait for a network-vif-plugged event from the (neutron) networking service before starting the actual transfer of the guest to the destination compute host.,reliability,nova
3623,max_concurrent_disk_ops,"Number of concurrent disk-IO-intensive operations (glance image downloads, image format conversions, etc.) that we will do in parallel.  If this is set too high then response time suffers. The default value of 0 means no limit.",performance,nova
3624,max_disk_devices_to_attach,"Maximum number of disk devices allowed to attach to a single server. Note that the number of disks supported by an server depends on the bus used. For example, the ide disk bus is limited to 4 attached devices. The configured maximum is enforced during server create, rebuild, evacuate, unshelve, live migrate, and attach volume.",reliability,nova
3625,workers,Number of workers for OpenStack Conductor service. The default will be the number of CPUs available.,performance,nova
3626,allowed_origins,Adds list of allowed origins to the console websocket proxy to allow connections from other origin hostnames. Websocket proxy matches the host header with the origin header to prevent cross-site requests. This list specifies if any there are values other than host are allowed in the origin header.,security,nova
3627,token_ttl,The lifetime of a console auth token (in seconds).,security,nova
3628,allowed_origin,"Indicate whether this resource may be shared with the domain received in the requests 'origin' header. Format:  '<protocol>://<host>[:<port>]', no trailing slash.  Example: https://horizon.example.com",others,nova
3629,allow_credentials,Indicate that the actual request can include user credentials,others,nova
3630,expose_headers,Indicate which headers are safe to expose to the API. Defaults to HTTP Simple Headers.,security,nova
3631,max_age,Maximum cache age of CORS preflight requests.,performance,nova
3632,allow_methods,Indicate which methods can be used during the actual request.,others,nova
3633,allow_headers,Indicate which header field names may be used during the actual request.,others,nova
3634,sqlite_synchronous,"If True, SQLite uses synchronous mode.",others,nova
3635,backend,The back end to use for the database.,environment,nova
3636,connection,The SQLAlchemy connection string to use to connect to the database.,others,nova
3637,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,environment,nova
3638,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,nova
3639,mysql_enable_ndb,"If True, transparently enables support for handling MySQL Cluster (NDB).",others,nova
3640,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,nova
3641,min_pool_size,Minimum number of SQL connections to keep open in a pool.,performance,nova
3642,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,reliability,nova
3643,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,nova
3644,retry_interval,Interval between retries of opening a SQL connection.,reliability,nova
3645,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",performance,nova
3646,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,nova
3647,connection_trace,Add Python stack traces to SQL as comment strings.,debuggability,nova
3648,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,nova
3649,use_db_reconnect,Enable the experimental use of database reconnect on connection lost.,reliability,nova
3650,db_retry_interval,Seconds between retries of a database transaction.,reliability,nova
3651,db_inc_retry_interval,"If True, increases the interval between retries of a database operation up to db_max_retry_interval.",reliability,nova
3652,db_max_retry_interval,"If db_inc_retry_interval is set, the maximum seconds between retries of a database operation.",reliability,nova
3653,db_max_retries,Maximum retries in case of connection error or deadlock error before  error is raised. Set to -1 to specify an infinite retry count.,reliability,nova
3654,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',environment,nova
3655,use_tpool,Enable the experimental use of thread pooling for all DB API calls,others,nova
3656,enabled_vgpu_types,The vGPU types enabled in the compute node.,others,nova
3657,enabled,Enables/disables LVM ephemeral storage encryption.,security,nova
3658,cipher,Cipher-mode string to be used.,security,nova
3659,key_size,Encryption key length in bits.,security,nova
3660,host_subset_size,Size of subset of best hosts selected by scheduler.,performance,nova
3661,max_io_ops_per_host,The number of instances that can be actively performing IO on a host.,reliability,nova
3662,max_instances_per_host,Maximum number of instances that be active on a host.,reliability,nova
3663,track_instance_changes,Enable querying of individual hosts for instance information.,others,nova
3664,available_filters,Filters that the scheduler can use.,others,nova
3665,enabled_filters,Filters that the scheduler will use.,others,nova
3666,weight_classes,Weighers that the scheduler will use.,others,nova
3667,ram_weight_multiplier,RAM weight multipler ratio.,performance,nova
3668,cpu_weight_multiplier,CPU weight multiplier ratio.,performance,nova
3669,disk_weight_multiplier,Disk weight multipler ratio.,performance,nova
3670,io_ops_weight_multiplier,IO operations weight multipler ratio.,performance,nova
3671,pci_weight_multiplier,PCI device affinity weight multiplier.,performance,nova
3672,soft_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-affinity.,performance,nova
3673,soft_anti_affinity_weight_multiplier,Multiplier used for weighing hosts for group soft-anti-affinity.,performance,nova
3674,build_failure_weight_multiplier,Multiplier used for weighing hosts that have had recent build failures.,reliability,nova
3675,shuffle_best_same_weighed_hosts,Enable spreading the instances between hosts with the same best weight.,performance,nova
3676,image_properties_default_architecture,The default architecture to be used when using the image properties filter.,others,nova
3677,isolated_images,List of UUIDs for images that can only be run on certain hosts.,others,nova
3678,isolated_hosts,List of hosts that can only run certain images.,others,nova
3679,restrict_isolated_hosts_to_isolated_images,Prevent non-isolated images from being built on isolated hosts.,reliability,nova
3680,aggregate_image_properties_isolation_namespace,Image property namespace for use in the host aggregate.,environment,nova
3681,aggregate_image_properties_isolation_separator,Separator character(s) for image property namespace and name.,others,nova
3682,api_servers,List of glance api servers endpoints available to nova.,others,nova
3683,num_retries,Enable glance operation retries.,reliability,nova
3684,allowed_direct_url_schemes,List of url schemes that can be directly accessed.,security,nova
3685,verify_glance_signatures,Enable image signature verification.,security,nova
3686,enable_certificate_validation,Enable certificate validation for image signature verification.,security,nova
3687,default_trusted_certificate_ids,List of certificate IDs for certificates that should be trusted.,security,nova
3688,debug,Enable or disable debug logging with glanceclient.,debuggability,nova
3689,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
3690,certfile,PEM encoded client certificate cert file,security,nova
3691,keyfile,PEM encoded client certificate key file,security,nova
3692,insecure,Verify HTTPS connections.,security,nova
3693,timeout,Timeout value for http requests,reliability,nova
3694,collect_timing,Collect per-API call timing information.,debuggability,nova
3695,split_loggers,Log requests to multiple loggers.,debuggability,nova
3696,service_type,The default service_type for endpoint URL discovery.,environment,nova
3697,service_name,The default service_name for endpoint URL discovery.,environment,nova
3698,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",security,nova
3699,region_name,The default region_name for endpoint URL discovery.,environment,nova
3700,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",others,nova
3701,debug,Enable/disables guestfs logging.,debuggability,nova
3702,path,The path to respond to healtcheck requests on.,environment,nova
3703,detailed,Show more detailed information as part of the response. Security  note: Enabling this option may expose sensitive details about the  service being monitored. Be sure to verify that it will not violate your security policies.,security,nova
3704,backends,Additional backends that can perform health checks and report that information back as part of a request.,reliability,nova
3705,disable_by_file_path,Check the presence of a file to determine if an application is running on a port. Used by DisableByFileHealthcheck plugin.,reliability,nova
3706,disable_by_file_paths,Check the presence of a file based on a port to determine if an  application is running on a port. Expects a 'port:path' list of strings. Used by DisableByFilesPortsHealthcheck plugin.,reliability,nova
3707,dynamic_memory_ratio,Dynamic memory ratio,performance,nova
3708,enable_instance_metrics_collection,Enable instance metrics collection,others,nova
3709,instances_path_share,Instances path share,others,nova
3710,limit_cpu_features,Limit CPU features,performance,nova
3711,mounted_disk_query_retry_count,Mounted disk query retry count,reliability,nova
3712,mounted_disk_query_retry_interval,Mounted disk query retry interval,reliability,nova
3713,power_state_check_timeframe,Power state check timeframe,reliability,nova
3714,power_state_event_polling_interval,Power state event polling interval,reliability,nova
3715,qemu_img_cmd,qemu-img command,others,nova
3716,vswitch_name,External virtual switch name,environment,nova
3717,wait_soft_reboot_seconds,Wait soft reboot seconds,reliability,nova
3718,config_drive_cdrom,Configuration drive cdrom,others,nova
3719,config_drive_inject_password,Configuration drive inject password,security,nova
3720,volume_attach_retry_count,Volume attach retry count,reliability,nova
3721,volume_attach_retry_interval,Volume attach retry interval,reliability,nova
3722,enable_remotefx,Enable RemoteFX feature,others,nova
3723,use_multipath_io,Use multipath connections when attaching iSCSI or FC disks.,others,nova
3724,iscsi_initiator_list,List of iSCSI initiators that will be used for estabilishing iSCSI sessions.,environment,nova
3725,api_endpoint,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3726,api_max_retries,"The number of times to retry when a request conflicts. If set to 0, only try once, no retries.",reliability,nova
3727,api_retry_interval,The number of seconds to wait before retrying the request.,reliability,nova
3728,serial_console_state_timeout,Timeout (seconds) to wait for node serial console state changed. Set to 0 to disable timeout.,reliability,nova
3729,partition_key,"Case-insensitive key to limit the set of nodes that may be managed by this service to the set of nodes in Ironic which have a matching  conductor_group property. If unset, all available nodes will be eligible to be managed by this service. Note that setting this to the empty  string ("""") will match the default conductor group, and is different than leaving the option unset.",security,nova
3730,peer_list,"List of hostnames for all nova-compute services (including this host) with this partition_key config value. Nodes matching the partition_key  value will be distributed between all services specified here. If  partition_key is unset, this option is ignored.",environment,nova
3731,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
3732,certfile,PEM encoded client certificate cert file,security,nova
3733,keyfile,PEM encoded client certificate key file,security,nova
3734,insecure,Verify HTTPS connections.,security,nova
3735,timeout,Timeout value for http requests,reliability,nova
3736,collect_timing,Collect per-API call timing information.,debuggability,nova
3737,split_loggers,Log requests to multiple loggers.,debuggability,nova
3738,auth_type,Authentication type to load,security,nova
3739,auth_section,Config Section from which to load plugin specific options,security,nova
3740,auth_url,Authentication URL,security,nova
3741,system_scope,Scope for system operations,reliability,nova
3742,domain_id,Domain ID to scope to,environment,nova
3743,domain_name,Domain name to scope to,environment,nova
3744,project_id,Project ID to scope to,environment,nova
3745,project_name,Project name to scope to,environment,nova
3746,project_domain_id,Domain ID containing project,environment,nova
3747,project_domain_name,Domain name containing project,environment,nova
3748,trust_id,Trust ID,security,nova
3749,user_id,User ID,environment,nova
3750,username,Username,environment,nova
3751,user_domain_id,User's domain id,environment,nova
3752,user_domain_name,User's domain name,environment,nova
3753,password,User's password,security,nova
3754,service_type,The default service_type for endpoint URL discovery.,environment,nova
3755,service_name,The default service_name for endpoint URL discovery.,environment,nova
3756,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",security,nova
3757,region_name,The default region_name for endpoint URL discovery.,environment,nova
3758,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",others,nova
3759,fixed_key,"Fixed key returned by key manager, specified in hex.",others,nova
3760,backend,Specify the key manager implementation. Options are 'barbican' and  'vault'. Default is  'barbican'. Will support the  values earlier set  using [key_manager]/api_class for some time.,others,nova
3761,auth_type,"The type of authentication credential to create. Possible values are  'token', 'password', 'keystone_token', and 'keystone_password'. Required if no context is passed to the credential factory.",security,nova
3762,token,Token for authentication. Required for 'token' and 'keystone_token' auth_type if no context is passed to the credential factory.,security,nova
3763,username,Username for authentication. Required for 'password' auth_type. Optional for the 'keystone_password' auth_type.,environment,nova
3764,password,Password for authentication. Required for 'password' and 'keystone_password' auth_type.,security,nova
3765,auth_url,Use this endpoint to connect to Keystone.,security,nova
3766,user_id,User ID for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3767,user_domain_id,User's domain ID for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3768,user_domain_name,User's domain name for authentication. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3769,trust_id,Trust ID for trust scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,security,nova
3770,domain_id,Domain ID for domain scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3771,domain_name,Domain name for domain scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3772,project_id,Project ID for project scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3773,project_name,Project name for project scoping. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3774,project_domain_id,Project's domain ID for project. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3775,project_domain_name,Project's domain name for project. Optional for 'keystone_token' and 'keystone_password' auth_type.,environment,nova
3776,reauthenticate,Allow fetching a new token if the current one is going to expire.  Optional for 'keystone_token' and 'keystone_password' auth_type.,security,nova
3777,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
3778,certfile,PEM encoded client certificate cert file,security,nova
3779,keyfile,PEM encoded client certificate key file,security,nova
3780,insecure,Verify HTTPS connections.,security,nova
3781,timeout,Timeout value for http requests,reliability,nova
3782,collect_timing,Collect per-API call timing information.,debuggability,nova
3783,split_loggers,Log requests to multiple loggers.,debuggability,nova
3784,service_type,The default service_type for endpoint URL discovery.,environment,nova
3785,service_name,The default service_name for endpoint URL discovery.,environment,nova
3786,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",security,nova
3787,region_name,The default region_name for endpoint URL discovery.,environment,nova
3788,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",others,nova
3789,www_authenticate_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint.",security,nova
3790,auth_uri,"Complete 'public' Identity API endpoint. This endpoint should not be  an 'admin' endpoint, as it should be accessible by all end users.  Unauthenticated clients are redirected to this endpoint to authenticate. Although this endpoint should ideally be unversioned, client support in the wild varies. If you're using a versioned v2 endpoint here, then  this should not be the same endpoint the service user utilizes  for validating tokens, because normal end users may not be able to reach that endpoint. This option is deprecated in favor of  www_authenticate_uri and will be removed in the S release.",security,nova
3791,auth_version,API version of the admin Identity API endpoint.,security,nova
3792,delay_auth_decision,"Do not handle authorization requests within the middleware, but  delegate the authorization decision to downstream WSGI components.",security,nova
3793,http_connect_timeout,Request timeout value for communicating with Identity API server.,reliability,nova
3794,http_request_max_retries,How many times are we trying to reconnect when communicating with Identity API Server.,reliability,nova
3795,cache,"Request environment key where the Swift cache object is stored. When  auth_token middleware is deployed with a Swift cache, use this option to have the middleware share a caching backend with swift. Otherwise, use  the memcached_servers option instead.",performance,nova
3796,certfile,Required if identity server requires client certificate,security,nova
3797,keyfile,Required if identity server requires client certificate,security,nova
3798,cafile,A PEM encoded Certificate Authority to use when verifying HTTPs connections. Defaults to system CAs.,security,nova
3799,insecure,Verify HTTPS connections.,security,nova
3800,region_name,The region in which the identity server can be found.,environment,nova
3801,signing_dir,Directory used to cache files related to PKI tokens. This option has  been deprecated in the Ocata release and will be removed in the P  release.,environment,nova
3802,memcached_servers,"Optionally specify a list of memcached server(s) to use for caching.  If left undefined, tokens will instead be cached in-process.",performance,nova
3803,token_cache_time,"In order to prevent excessive effort spent validating tokens, the  middleware caches previously-seen tokens for a configurable duration (in seconds). Set to -1 to disable caching completely.",performance,nova
3804,memcache_security_strategy,"(Optional) If defined, indicate whether token data should be  authenticated or authenticated and encrypted. If MAC, token data is  authenticated (with HMAC) in the cache. If ENCRYPT, token data is  encrypted and authenticated in the cache. If the value is not one of  these options or empty, auth_token will raise an exception on  initialization.",security,nova
3805,memcache_secret_key,"(Optional, mandatory if memcache_security_strategy is defined) This string is used for key derivation.",security,nova
3806,memcache_pool_dead_retry,(Optional) Number of seconds memcached server is considered dead before it is tried again.,reliability,nova
3807,memcache_pool_maxsize,(Optional) Maximum total number of open connections to every memcached server.,reliability,nova
3808,memcache_pool_socket_timeout,(Optional) Socket timeout in seconds for communicating with a memcached server.,reliability,nova
3809,memcache_pool_unused_timeout,(Optional) Number of seconds a connection to memcached is held unused in the pool before it is closed.,reliability,nova
3810,memcache_pool_conn_get_timeout,(Optional) Number of seconds that an operation will wait to get a memcached client connection from the pool.,reliability,nova
3811,memcache_use_advanced_pool,(Optional) Use the advanced (eventlet safe) memcached client pool. The advanced pool will only work under python 2.x.,reliability,nova
3812,include_service_catalog,"(Optional) Indicate whether to set the X-Service-Catalog header. If  False, middleware will not ask for service catalog on token validation  and will not set the X-Service-Catalog header.",others,nova
3813,enforce_token_bind,Used to control the use and type of token binding. Can be set to:  'disabled' to not check token binding. 'permissive' (default) to  validate binding information if the bind type is of a form known to the  server and ignore it if not. 'strict' like 'permissive' but if the bind  type is unknown the token will be rejected. 'required' any form of token binding is needed to be allowed. Finally the name of a binding method  that must be present in tokens.,security,nova
3814,hash_algorithms,"Hash algorithms to use for hashing PKI tokens. This may be a single  algorithm or multiple. The algorithms are those supported by Python  standard hashlib.new(). The hashes will be tried in the order given, so  put the preferred one first for performance. The result of the first  hash will be stored in the cache. This will typically be set to multiple values only while migrating from a less secure algorithm to a more  secure one. Once all the old tokens are expired this option should be  set to a single value for better performance.",security,nova
3815,service_token_roles,A choice of roles that must be present in a service token. Service  tokens are allowed to request that an expired token can be used and so  this check should tightly control that only actual services should be  sending this token. Roles here are applied as an ANY check so any role  in this list must be present. For backwards compatibility reasons this  currently only affects the allow_expired check.,security,nova
3816,service_token_roles_required,For backwards compatibility reasons we must let valid service tokens  pass that don't pass the service_token_roles check as valid. Setting  this true will become the default in a future release and should be  enabled if possible.,reliability,nova
3817,auth_type,Authentication type to load,security,nova
3818,auth_section,Config Section from which to load plugin specific options,security,nova
3819,rescue_image_id,The ID of the image to boot from to rescue data from a corrupted instance.,environment,nova
3820,rescue_kernel_id,The ID of the kernel (AKI) image to use with the rescue image.,environment,nova
3821,rescue_ramdisk_id,The ID of the RAM disk (ARI) image to use with the rescue image.,environment,nova
3822,virt_type,Describes the virtualization type (or so called domain type) libvirt should use.,others,nova
3823,connection_uri,Overrides the default libvirt URI of the chosen virtualization type.,environment,nova
3824,inject_password,Allow the injection of an admin password for instance only at create and rebuild process.,security,nova
3825,inject_key,Allow the injection of an SSH key at boot time.,security,nova
3826,inject_partition,Determines the way how the file system is chosen to inject data into it.,others,nova
3827,use_usb_tablet,Enable a mouse cursor within a graphical VNC or SPICE sessions.,others,nova
3828,live_migration_scheme,URI scheme used for live migration.,environment,nova
3829,live_migration_inbound_addr,Target used for live migration traffic.,environment,nova
3830,live_migration_uri,Live migration target URI to use.,environment,nova
3831,live_migration_tunnelled,Enable tunnelled migration.,others,nova
3832,live_migration_bandwidth,Maximum bandwidth(in MiB/s) to be used during migration.,performance,nova
3833,live_migration_downtime,"Maximum permitted downtime, in milliseconds, for live migration switchover.",reliability,nova
3834,live_migration_downtime_steps,Number of incremental steps to reach max downtime value.,reliability,nova
3835,live_migration_downtime_delay,"Time to wait, in seconds, between each step increase of the migration downtime.",reliability,nova
3836,live_migration_completion_timeout,"Time to wait, in seconds, for migration to successfully complete transferring data before aborting the operation.",reliability,nova
3837,live_migration_timeout_action,"This option will be used to determine what action will be taken against a VM after live_migration_completion_timeout expires. By default, the live migrate operation will be aborted after completion timeout. If it is set to force_complete, the compute service will either pause the VM or trigger post-copy depending on if post copy is enabled and available (live_migration_permit_post_copy is set to True).",reliability,nova
3838,live_migration_permit_post_copy,"This option allows nova to switch an on-going live migration to post-copy mode, i.e., switch the active VM to the one on the destination node before the migration is complete, therefore ensuring an upper bound on the memory that needs to be transferred. Post-copy requires libvirt>=1.3.3 and QEMU>=2.5.0.",reliability,nova
3839,live_migration_permit_auto_converge,This option allows nova to start live migration with auto converge on.,others,nova
3840,snapshot_image_format,Determine the snapshot image format when sending to the image service.,others,nova
3841,live_migration_with_native_tls,Use QEMU-native TLS encryption when live migrating.,security,nova
3842,disk_prefix,Override the default disk prefix for the devices attached to an instance.,environment,nova
3843,wait_soft_reboot_seconds,Number of seconds to wait for instance to shut down after soft reboot request is made. We fall back to hard reboot if instance does not  shutdown within this window.,reliability,nova
3844,cpu_mode,Is used to set the CPU mode an instance should have.,others,nova
3845,cpu_model,Set the name of the libvirt CPU model the instance should use.,others,nova
3846,cpu_model_extra_flags,"This allows specifying granular CPU feature flags when configuring CPU models.  For example, to explicitly specify the pcid (Process-Context ID, an Intel processor feature -- which is now required to address the guest performance degradation as a result of applying the 'Meltdown' CVE fixes to certain Intel CPU models) flag to the 'IvyBridge' virtual CPU model:",others,nova
3847,snapshots_directory,Location where libvirt driver will store snapshots before uploading them to image service,environment,nova
3848,xen_hvmloader_path,Location where the Xen hvmloader is kept,environment,nova
3849,disk_cachemodes,Specific cache modes to use for different disk types.,performance,nova
3850,rng_dev_path,"The path to an RNG (Random Number Generator) device that will be used as the source of entropy on the host.  Since libvirt 1.3.4, any path (that returns random numbers when read) is accepted.  The recommended source of entropy is /dev/urandom -- it is non-blocking, therefore relatively fast; and avoids the limitations of /dev/random, which is a legacy interface.  For more details (and comparision between different RNG sources), refer to the 'Usage' section in the Linux kernel API documentation for [u]random: http://man7.org/linux/man-pages/man4/urandom.4.html and http://man7.org/linux/man-pages/man7/random.7.html.",environment,nova
3851,hw_machine_type,"For qemu or KVM guests, set this option to specify a default machine  type per host architecture. You can find a list of supported machine  types in your environment by checking the output of the 'virsh  capabilities' command. The format of the value for this config option is host-arch=machine-type. For example:  x86_64=machinetype1,armv7l=machinetype2",others,nova
3852,sysinfo_serial,"The data source used to the populate the host 'serial' UUID exposed to guest in the virtual BIOS. All choices except unique will change the serial when migrating the instance to another host. Changing the choice of this option will also affect existing instances on this host once they are stopped and started again. It is recommended to use the default choice (unique) since that will not change when an instance is migrated. However, if you have a need for per-host serials in addition to per-instance serial numbers, then consider restricting flavors via host aggregates.",others,nova
3853,mem_stats_period_seconds,A number of seconds to memory usage statistics period. Zero or negative value mean to disable memory usage statistics.,debuggability,nova
3854,uid_maps,List of uid targets and ranges.Syntax is guest-uid:host-uid:count. Maximum of 5 allowed.,others,nova
3855,gid_maps,List of guid targets and ranges.Syntax is guest-gid:host-gid:count. Maximum of 5 allowed.,others,nova
3856,realtime_scheduler_priority,In a realtime host context vCPUs for guest will run in that  scheduling priority. Priority depends on the host kernel (usually 1-99),performance,nova
3857,enabled_perf_events,"This will allow you to specify a list of events to monitor low-level performance of guests, and collect related statsitics via the libvirt driver, which in turn uses the Linux kernel's perf infrastructure. With this config attribute set, Nova will generate libvirt guest XML to monitor the specified events.  For more information, refer to the 'Performance monitoring events' section here: https://libvirt.org/formatdomain.html#elementsPerf.  And here: https://libvirt.org/html/libvirt-libvirt-domain.html -- look for VIR_PERF_PARAM_*",performance,nova
3858,num_pcie_ports,The number of PCIe ports an instance will get.,performance,nova
3859,file_backed_memory,Available capacity in MiB for file-backed memory.,performance,nova
3860,images_type,VM Images format.,others,nova
3861,images_volume_group,"LVM Volume Group that is used for VM images, when you specify images_type=lvm",environment,nova
3862,sparse_logical_volumes,Create sparse logical volumes (with virtualsize) if this flag is set to True.,others,nova
3863,images_rbd_pool,The RADOS pool in which rbd volumes are stored,performance,nova
3864,images_rbd_ceph_conf,Path to the ceph configuration file to use,security,nova
3865,hw_disk_discard,Discard option for nova managed disks.,others,nova
3866,image_info_filename_pattern,Allows image information files to be stored in non-standard locations,others,nova
3867,remove_unused_resized_minimum_age_seconds,Unused resized base images younger than this will not be removed,reliability,nova
3868,checksum_base_images,Write a checksum for files in _base to disk,reliability,nova
3869,checksum_interval_seconds,How frequently to checksum base images,reliability,nova
3870,volume_clear,Method used to wipe ephemeral disks when they are deleted. Only takes effect if LVM is set as backing storage.,reliability,nova
3871,volume_clear_size,"Size of area in MiB, counting from the beginning of the allocated volume, that will be cleared using method set in volume_clear option.",performance,nova
3872,snapshot_compression,Enable snapshot compression for qcow2 images.,performance,nova
3873,use_virtio_for_bridges,Use virtio for bridge interfaces with KVM/QEMU,others,nova
3874,volume_use_multipath,Use multipath connection of the iSCSI or FC volume,others,nova
3875,num_volume_scan_tries,Number of times to scan given storage protocol to find volume.,reliability,nova
3876,num_aoe_discover_tries,Number of times to rediscover AoE target to find volume.,reliability,nova
3877,iscsi_iface,The iSCSI transport iface to use to connect to target in case offload support is desired.,environment,nova
3878,num_iser_scan_tries,Number of times to scan iSER target to find volume.,reliability,nova
3879,iser_use_multipath,Use multipath connection of the iSER volume.,others,nova
3880,rbd_user,The RADOS client name for accessing rbd(RADOS Block Devices) volumes.,environment,nova
3881,rbd_secret_uuid,The libvirt UUID of the secret for the rbd_user volumes.,environment,nova
3882,nfs_mount_point_base,Directory where the NFS volume is mounted on the compute node. The default is 'mnt' directory of the location where nova's Python module is installed.,environment,nova
3883,nfs_mount_options,Mount options passed to the NFS client. See section of the nfs man page for details.,others,nova
3884,quobyte_mount_point_base,Directory where the Quobyte volume is mounted on the compute node.,environment,nova
3885,quobyte_client_cfg,Path to a Quobyte Client configuration file.,environment,nova
3886,smbfs_mount_point_base,Directory where the SMBFS shares are mounted on the compute node.,environment,nova
3887,smbfs_mount_options,Mount options passed to the SMBFS client.,others,nova
3888,remote_filesystem_transport,libvirt's transport method for remote file operations.,environment,nova
3889,vzstorage_mount_point_base,Directory where the Virtuozzo Storage clusters are mounted on the compute node.,environment,nova
3890,vzstorage_mount_user,Mount owner user name.,environment,nova
3891,vzstorage_mount_group,Mount owner group name.,environment,nova
3892,vzstorage_mount_perms,Mount access mode.,security,nova
3893,vzstorage_log_path,Path to vzstorage client log.,debuggability,nova
3894,vzstorage_cache_path,Path to the SSD cache file.,performance,nova
3895,vzstorage_mount_opts,Extra mount options for pstorage-mount,others,nova
3896,rx_queue_size,Configure virtio rx queue size.,performance,nova
3897,tx_queue_size,Configure virtio tx queue size.,performance,nova
3898,num_nvme_discover_tries,Number of times to rediscover NVMe target to find volume,reliability,nova
3899,weight_multiplier,"When using metrics to weight the suitability of a host, you can use this option to change how the calculated weight influences the weight assigned to a host as follows:",others,nova
3900,weight_setting,"This setting specifies the metrics to be weighed and the relative ratios for each metric. This should be a single string value, consisting of a series of one or more 'name=ratio' pairs, separated by commas, where 'name' is the name of the metric to be weighed, and 'ratio' is the relative weight for that metric.",others,nova
3901,required,"This setting determines how any unavailable metrics are treated. If this option is set to True, any hosts for which a metric is unavailable will raise an exception, so it is recommended to also use the MetricFilter to filter out those hosts before weighing.",others,nova
3902,weight_of_unavailable,"When any of the following conditions are met, this value will be used in place of any actual metric value:",others,nova
3903,mksproxy_base_url,Location of MKS web console proxy,environment,nova
3904,enabled,Enables graphical console access for virtual machines.,others,nova
3905,url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
3906,ovs_bridge,Default name for the Open vSwitch integration bridge.,environment,nova
3907,default_floating_pool,Default name for the floating IP pool.,environment,nova
3908,extension_sync_interval,Integer value representing the number of seconds to wait before querying Neutron for extensions.  After this number of seconds the next time Nova needs to create a resource in Neutron it will requery Neutron for the extensions that it has loaded.  Setting value to 0 will refresh the extensions with no wait.,reliability,nova
3909,physnets,List of physnets present on this host.,environment,nova
3910,http_retries,Number of times neutronclient should retry on any failed http call.,reliability,nova
3911,service_metadata_proxy,"When set to True, this option indicates that Neutron will be used to proxy metadata requests and resolve instance ids. Otherwise, the instance ID must be passed to the metadata request in the 'X-Instance-ID' header.",security,nova
3912,metadata_proxy_shared_secret,"This option holds the shared secret string used to validate proxy requests to Neutron metadata requests. In order to be used, the 'X-Metadata-Provider-Signature' header must be supplied in the request.",security,nova
3913,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
3914,certfile,PEM encoded client certificate cert file,security,nova
3915,keyfile,PEM encoded client certificate key file,security,nova
3916,insecure,Verify HTTPS connections.,security,nova
3917,timeout,Timeout value for http requests,reliability,nova
3918,collect_timing,Collect per-API call timing information.,debuggability,nova
3919,split_loggers,Log requests to multiple loggers.,debuggability,nova
3920,auth_type,Authentication type to load,security,nova
3921,auth_section,Config Section from which to load plugin specific options,security,nova
3922,auth_url,Authentication URL,security,nova
3923,system_scope,Scope for system operations,reliability,nova
3924,domain_id,Domain ID to scope to,environment,nova
3925,domain_name,Domain name to scope to,environment,nova
3926,project_id,Project ID to scope to,environment,nova
3927,project_name,Project name to scope to,environment,nova
3928,project_domain_id,Domain ID containing project,environment,nova
3929,project_domain_name,Domain name containing project,environment,nova
3930,trust_id,Trust ID,security,nova
3931,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
3932,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
3933,user_id,User ID,environment,nova
3934,username,Username,environment,nova
3935,user_domain_id,User's domain id,environment,nova
3936,user_domain_name,User's domain name,environment,nova
3937,password,User's password,security,nova
3938,tenant_id,Tenant ID,environment,nova
3939,tenant_name,Tenant Name,environment,nova
3940,service_type,The default service_type for endpoint URL discovery.,environment,nova
3941,service_name,The default service_name for endpoint URL discovery.,environment,nova
3942,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",security,nova
3943,region_name,The default region_name for endpoint URL discovery.,environment,nova
3944,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",others,nova
3945,notify_on_state_change,"If set, send compute.instance.update notifications on instance state changes.",others,nova
3946,default_level,Default notification level for outgoing notifications.,debuggability,nova
3947,notification_format,Specifies which notification format shall be used by nova.,others,nova
3948,versioned_notifications_topics,Specifies the topics for the versioned notifications issued by nova.,others,nova
3949,bdms_in_notifications,"If enabled, include block device information in the versioned notification payload. Sending block device information is disabled by default as providing that information can incur some overhead on the system since the information may need to be loaded from the database.",debuggability,nova
3950,project_id_regex,"This option is a string representing a regular expression (regex) that matches the project_id as contained in URLs. If not set, it will match normal UUIDs created by keystone.",others,nova
3951,disable_process_locking,Enables or disables inter-process locks.,security,nova
3952,lock_path,"Directory to use for lock files.  For security, the specified  directory should only be writable by the user running the processes that need locking. Defaults to environment variable OSLO_LOCK_PATH. If  external locks are used, a lock path must be set.",security,nova
3953,container_name,Name for the AMQP container. must be globally unique. Defaults to a generated UUID,environment,nova
3954,idle_timeout,Timeout for inactive connections (in seconds),reliability,nova
3955,trace,Debug: dump AMQP frames to stdout,debuggability,nova
3956,ssl,"Attempt to connect via SSL. If no other ssl-related parameters are  given, it will use the system's CA-bundle to verify the server's  certificate.",security,nova
3957,ssl_ca_file,CA certificate PEM file used to verify the server's certificate,security,nova
3958,ssl_cert_file,Self-identifying certificate PEM file for client authentication,security,nova
3959,ssl_key_file,Private key PEM file used to sign ssl_cert_file certificate (optional),security,nova
3960,ssl_key_password,Password for decrypting ssl_key_file (if encrypted),security,nova
3961,ssl_verify_vhost,"By default SSL checks that the name in the server's certificate  matches the hostname in the transport_url. In some configurations it may be preferable to use the virtual hostname instead, for example if the  server uses the Server Name Indication TLS extension (rfc6066) to  provide a certificate per virtual host. Set ssl_verify_vhost to True if  the server's SSL certificate uses the virtual host name instead of the  DNS name.",security,nova
3962,sasl_mechanisms,Space separated list of acceptable SASL mechanisms,security,nova
3963,sasl_config_dir,Path to directory that contains the SASL configuration,security,nova
3964,sasl_config_name,Name of configuration file (without .conf suffix),environment,nova
3965,sasl_default_realm,SASL realm to use if no realm present in username,security,nova
3966,connection_retry_interval,Seconds to pause before attempting to re-connect.,reliability,nova
3967,connection_retry_backoff,Increase the connection_retry_interval by this many seconds after each unsuccessful failover attempt.,reliability,nova
3968,connection_retry_interval_max,Maximum limit for connection_retry_interval + connection_retry_backoff,reliability,nova
3969,link_retry_delay,Time to pause between re-connecting an AMQP 1.0 link that failed due to a recoverable error.,reliability,nova
3970,default_reply_retry,The maximum number of attempts to re-send a reply message which failed due to a recoverable error.,reliability,nova
3971,default_reply_timeout,The deadline for an rpc reply message delivery.,reliability,nova
3972,default_send_timeout,The deadline for an rpc cast or call message delivery. Only used when caller does not provide a timeout expiry.,reliability,nova
3973,default_notify_timeout,The deadline for a sent notification message delivery. Only used when caller does not provide a timeout expiry.,reliability,nova
3974,default_sender_link_timeout,The duration to schedule a purge of idle sender links. Detach link after expiry.,reliability,nova
3975,addressing_mode,Indicates the addressing mode used by the driver. Permitted values: 'legacy'   - use legacy non-routable addressing 'routable' - use routable addresses 'dynamic'  - use legacy addresses if the message bus does not support routing otherwise use routable addressing,security,nova
3976,pseudo_vhost,"Enable virtual host support for those message buses that do not  natively support virtual hosting (such as qpidd). When set to true the  virtual host name will be added to all message bus addresses,  effectively creating a private 'subnet' per virtual host. Set to False  if the message bus supports virtual hosting using the 'hostname' field  in the AMQP 1.0 Open performative as the name of the virtual host.",security,nova
3977,server_request_prefix,address prefix used when sending to a specific server,environment,nova
3978,broadcast_prefix,address prefix used when broadcasting to all servers,environment,nova
3979,group_request_prefix,address prefix when sending to any server in group,environment,nova
3980,rpc_address_prefix,Address prefix for all generated RPC addresses,environment,nova
3981,notify_address_prefix,Address prefix for all generated Notification addresses,environment,nova
3982,multicast_address,Appended to the address prefix when sending a fanout message. Used by the message bus to identify fanout messages.,environment,nova
3983,unicast_address,Appended to the address prefix when sending to a particular  RPC/Notification server. Used by the message bus to identify messages  sent to a single destination.,environment,nova
3984,anycast_address,Appended to the address prefix when sending to a group of consumers.  Used by the message bus to identify messages that should be delivered in a round-robin fashion across consumers.,environment,nova
3985,default_notification_exchange,Exchange name used in notification addresses. Exchange name resolution precedence: Target.exchange if set else default_notification_exchange if set else control_exchange if set else 'notify',environment,nova
3986,default_rpc_exchange,Exchange name used in RPC addresses. Exchange name resolution precedence: Target.exchange if set else default_rpc_exchange if set else control_exchange if set else 'rpc',environment,nova
3987,reply_link_credit,Window size for incoming RPC Reply messages.,performance,nova
3988,rpc_server_credit,Window size for incoming RPC Request messages,performance,nova
3989,notify_server_credit,Window size for incoming Notification messages,performance,nova
3990,pre_settled,Send messages of this type pre-settled. Pre-settled messages will not receive acknowledgement from the peer. Note well: pre-settled messages may be silently discarded if the delivery fails. Permitted values: 'rpc-call' - send RPC Calls pre-settled 'rpc-reply'- send RPC Replies pre-settled 'rpc-cast' - Send RPC Casts pre-settled 'notify'   - Send Notifications pre-settled,others,nova
3991,kafka_max_fetch_bytes,Max fetch bytes of Kafka consumer,reliability,nova
3992,kafka_consumer_timeout,Default timeout(s) for Kafka consumers,reliability,nova
3993,pool_size,Pool Size for Kafka Consumers,performance,nova
3994,conn_pool_min_size,The pool size limit for connections expiration policy,reliability,nova
3995,conn_pool_ttl,The time-to-live in sec of idle connections in the pool,reliability,nova
3996,consumer_group,Group id for Kafka consumer. Consumers in one group will coordinate message consumption,environment,nova
3997,producer_batch_timeout,Upper bound on the delay for KafkaProducer batching in seconds,reliability,nova
3998,producer_batch_size,Size of batch for the producer async send,performance,nova
3999,enable_auto_commit,Enable asynchronous consumer commits,reliability,nova
4000,max_poll_records,The maximum number of records returned in a poll call,reliability,nova
4001,security_protocol,Protocol used to communicate with brokers,security,nova
4002,sasl_mechanism,Mechanism when security protocol is SASL,security,nova
4003,ssl_cafile,CA certificate PEM file used to verify the server certificate,security,nova
4004,driver,"The Drivers(s) to handle sending notifications. Possible values are messaging, messagingv2, routing, log, test, noop",environment,nova
4005,transport_url,"A URL representing the messaging driver to use for notifications. If  not set, we fall back to the same configuration used for RPC.",environment,nova
4006,topics,AMQP topic used for OpenStack notifications.,debuggability,nova
4007,retry,"The maximum number of attempts to re-send a notification message  which failed to be delivered due to a recoverable error. 0 - No retry,  -1 - indefinite",reliability,nova
4008,amqp_durable_queues,Use durable queues in AMQP.,others,nova
4009,amqp_auto_delete,Auto-delete queues in AMQP.,performance,nova
4010,ssl,Connect over SSL.,security,nova
4011,ssl_version,"SSL version to use (valid only if SSL enabled). Valid values are  TLSv1 and SSLv23. SSLv2, SSLv3, TLSv1_1, and TLSv1_2 may be available on some distributions.",security,nova
4012,ssl_key_file,SSL key file (valid only if SSL enabled).,security,nova
4013,ssl_cert_file,SSL cert file (valid only if SSL enabled).,security,nova
4014,ssl_ca_file,SSL certification authority file (valid only if SSL enabled).,security,nova
4015,kombu_reconnect_delay,How long to wait before reconnecting in response to an AMQP consumer cancel notification.,reliability,nova
4016,kombu_compression,"EXPERIMENTAL: Possible values are: gzip, bz2. If not set compression  will not be used. This option may not be available in future versions.",performance,nova
4017,kombu_missing_consumer_retry_timeout,How long to wait a missing client before abandoning to send it its  replies. This value should not be longer than rpc_response_timeout.,reliability,nova
4018,kombu_failover_strategy,Determines how the next RabbitMQ node is chosen in case the one we  are currently connected to becomes unavailable. Takes effect only if  more than one RabbitMQ node is provided in config.,reliability,nova
4019,rabbit_login_method,The RabbitMQ login method.,others,nova
4020,rabbit_retry_interval,How frequently to retry connecting with RabbitMQ.,reliability,nova
4021,rabbit_retry_backoff,How long to backoff for between retries when connecting to RabbitMQ.,reliability,nova
4022,rabbit_interval_max,Maximum interval of RabbitMQ connection retries. Default is 30 seconds.,reliability,nova
4023,rabbit_ha_queues,"Try to use HA queues in RabbitMQ (x-ha-policy: all). If you change  this option, you must wipe the RabbitMQ database. In RabbitMQ 3.0, queue mirroring is no longer controlled by the x-ha-policy argument when  declaring a queue. If you just want to make sure that all queues (except those with auto-generated names) are mirrored across all nodes, run:  'rabbitmqctl set_policy HA '^(?!amq.).*' '{'ha-mode': 'all'}' '",others,nova
4024,rabbit_transient_queues_ttl,Positive integer representing duration in seconds for queue TTL  (x-expires). Queues which are unused for the duration of the TTL are  automatically deleted. The parameter affects only reply and fanout  queues.,reliability,nova
4025,rabbit_qos_prefetch_count,Specifies the number of messages to prefetch. Setting to zero allows unlimited messages.,reliability,nova
4026,heartbeat_timeout_threshold,Number of seconds after which the Rabbit broker is considered down if heartbeat's keep-alive fails (0 disable the heartbeat). EXPERIMENTAL,reliability,nova
4027,heartbeat_rate,How often times during the heartbeat_timeout_threshold we check the heartbeat.,reliability,nova
4028,max_request_body_size,"The maximum body size for each  request, in bytes.",performance,nova
4029,secure_proxy_ssl_header,"The HTTP Header that will be used to determine what the original  request protocol scheme was, even if it was hidden by a SSL termination  proxy.",security,nova
4030,enable_proxy_headers_parsing,Whether the application is behind a proxy or not. This determines if the middleware should parse the headers or not.,others,nova
4031,enforce_scope,"This option controls whether or not to enforce scope when evaluating policies. If True, the scope of the token used in the request is compared to the scope_types of the policy being enforced. If the scopes do not match, an InvalidScope exception will be raised. If False, a message will be logged informing operators that policies are being invoked with mismatching scope.",others,nova
4032,policy_file,The file that defines policies.,security,nova
4033,policy_default_rule,Default rule. Enforced when a requested rule is not found.,security,nova
4034,policy_dirs,"Directories where policy configuration files are stored. They can be  relative to any directory in the search path defined by the config_dir  option, or absolute paths. The file defined by policy_file must exist  for these directories to be searched.  Missing or empty directories are  ignored.",environment,nova
4035,remote_content_type,Content Type to send and receive data for REST based policy check,others,nova
4036,remote_ssl_verify_server_crt,server identity verification for REST based policy check,security,nova
4037,remote_ssl_ca_crt_file,Absolute path to ca cert file for REST based policy check,security,nova
4038,remote_ssl_client_crt_file,Absolute path to client cert for REST based policy check,security,nova
4039,remote_ssl_client_key_file,Absolute path client key file REST based policy check,environment,nova
4040,alias,An alias for a PCI passthrough device requirement.,others,nova
4041,passthrough_whitelist,White list of PCI devices available to VMs.,security,nova
4042,randomize_allocation_candidates,"If True, when limiting allocation candidate results, the results will be a random sampling of the full result set. If False, allocation candidates are returned in a deterministic but undefined order. That is, all things being equal, two requests for allocation candidates will return the same results in the same order; but no guarantees are made as to how that order is determined.",others,nova
4043,policy_file,The file that defines placement policies. This can be an absolute path or relative to the configuration file.,reliability,nova
4044,incomplete_consumer_project_id,"Early API microversions (<1.8) allowed creating allocations and not specifying a project or user identifier for the consumer. In cleaning up the data modeling, we no longer allow missing project and user information. If an older client makes an allocation, we'll use this in place of the information it doesn't provide.",reliability,nova
4045,incomplete_consumer_user_id,"Early API microversions (<1.8) allowed creating allocations and not specifying a project or user identifier for the consumer. In cleaning up the data modeling, we no longer allow missing project and user information. If an older client makes an allocation, we'll use this in place of the information it doesn't provide.",environment,nova
4046,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
4047,certfile,PEM encoded client certificate cert file,security,nova
4048,keyfile,PEM encoded client certificate key file,security,nova
4049,insecure,Verify HTTPS connections.,security,nova
4050,timeout,Timeout value for http requests,reliability,nova
4051,collect_timing,Collect per-API call timing information.,debuggability,nova
4052,split_loggers,Log requests to multiple loggers.,debuggability,nova
4053,auth_type,Authentication type to load,security,nova
4054,auth_section,Config Section from which to load plugin specific options,security,nova
4055,auth_url,Authentication URL,security,nova
4056,system_scope,Scope for system operations,reliability,nova
4057,domain_id,Domain ID to scope to,environment,nova
4058,domain_name,Domain name to scope to,environment,nova
4059,project_id,Project ID to scope to,environment,nova
4060,project_name,Project name to scope to,environment,nova
4061,project_domain_id,Domain ID containing project,environment,nova
4062,project_domain_name,Domain name containing project,environment,nova
4063,trust_id,Trust ID,security,nova
4064,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4065,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4066,user_id,User ID,environment,nova
4067,username,Username,environment,nova
4068,user_domain_id,User's domain id,environment,nova
4069,user_domain_name,User's domain name,environment,nova
4070,password,User's password,security,nova
4071,tenant_id,Tenant ID,environment,nova
4072,tenant_name,Tenant Name,environment,nova
4073,service_type,The default service_type for endpoint URL discovery.,environment,nova
4074,service_name,The default service_name for endpoint URL discovery.,environment,nova
4075,valid_interfaces,"List of interfaces, in order of preference, for endpoint URL.",security,nova
4076,region_name,The default region_name for endpoint URL discovery.,environment,nova
4077,endpoint_override,"Always use this endpoint URL for requests for this client. NOTE: The  unversioned endpoint should be specified here; to request a particular  API version, use the version, min-version, and/or max-version options.",others,nova
4078,connection,The SQLAlchemy connection string to use to connect to the database.,others,nova
4079,connection_parameters,Optional URL parameters to append onto the connection URL at connect time; specify as param1=value1&param2=value2&',environment,nova
4080,sqlite_synchronous,"If True, SQLite uses synchronous mode.",others,nova
4081,slave_connection,The SQLAlchemy connection string to use to connect to the slave database.,environment,nova
4082,mysql_sql_mode,"The SQL mode to be used for MySQL sessions. This option, including  the default, overrides any server-set SQL mode. To use whatever SQL mode is set by the server configuration, set this to no value. Example:  mysql_sql_mode=",others,nova
4083,connection_recycle_time,Connections which have been present in the connection pool longer  than this number of seconds will be replaced with a new one the next  time they are checked out from the pool.,reliability,nova
4084,max_pool_size,Maximum number of SQL connections to keep open in a pool. Setting a value of 0 indicates no limit.,reliability,nova
4085,max_retries,Maximum number of database connection retries during startup. Set to -1 to specify an infinite retry count.,reliability,nova
4086,retry_interval,Interval between retries of opening a SQL connection.,reliability,nova
4087,max_overflow,"If set, use this value for max_overflow with SQLAlchemy.",reliability,nova
4088,connection_debug,"Verbosity of SQL debugging information: 0=None, 100=Everything.",debuggability,nova
4089,connection_trace,Add Python stack traces to SQL as comment strings.,debuggability,nova
4090,pool_timeout,"If set, use this value for pool_timeout with SQLAlchemy.",reliability,nova
4091,proc_units_factor,"Factor used to calculate the amount of physical processor compute power given to each vCPU. E.g. A value of 1.0 means a whole physical processor, whereas 0.05 means 1/20th of a physical processor.",performance,nova
4092,disk_driver,The disk driver to use for PowerVM disks. PowerVM provides support for localdisk and PowerVM Shared Storage Pool disk drivers.,environment,nova
4093,volume_group_name,"Volume Group to use for block device operations. If disk_driver is localdisk, then this attribute must be specified. It is strongly recommended NOT to use rootvg since that is used by the management partition and filling it will cause failures.",reliability,nova
4094,user,User that the privsep daemon should run as.,environment,nova
4095,group,Group that the privsep daemon should run as.,environment,nova
4096,capabilities,List of Linux capabilities retained by the privsep daemon.,others,nova
4097,thread_pool_size,The number of threads available for privsep to concurrently run processes. Defaults to the number of CPU cores in the system.,performance,nova
4098,helper_command,"Command to invoke to start the privsep daemon if not using the 'fork' method. If not specified, a default is generated using 'sudo  privsep-helper' and arguments designed to recreate the current  configuration. This command must accept suitable --privsep_context and  --privsep_sock_path arguments.",others,nova
4099,enabled,Enable the profiling for all services on this node.,performance,nova
4100,trace_sqlalchemy,Enable SQL requests profiling in services.,performance,nova
4101,hmac_keys,Secret key(s) to use for encrypting context data for performance profiling.,security,nova
4102,connection_string,Connection string for a notifier backend.,others,nova
4103,es_doc_type,Document type for notification indexing in elasticsearch.,others,nova
4104,es_scroll_time,"This parameter is a time value parameter (for example: es_scroll_time=2m), indicating for how long the nodes that participate in the search will maintain relevant resources in order to continue and support it.",reliability,nova
4105,es_scroll_size,Elasticsearch splits large requests in batches. This parameter defines maximum size of each batch (for example: es_scroll_size=10000).,performance,nova
4106,socket_timeout,Redissentinel provides a timeout option on the connections. This parameter defines that timeout (for example: socket_timeout=0.1).,reliability,nova
4107,sentinel_service_name,Redissentinel uses a service name to identify a master redis service. This parameter defines the name (for example: sentinal_service_name=mymaster).,environment,nova
4108,filter_error_trace,Enable filter traces that contain error/exception to a separated place.,debuggability,nova
4109,instances,The number of instances allowed per project.,reliability,nova
4110,cores,The number of instance cores or vCPUs allowed per project.,performance,nova
4111,ram,The number of megabytes of instance RAM allowed per project.,performance,nova
4112,floating_ips,The number of floating IPs allowed per project.,others,nova
4113,fixed_ips,The number of fixed IPs allowed per project.,reliability,nova
4114,metadata_items,The number of metadata items allowed per instance.,reliability,nova
4115,injected_files,The number of injected files allowed.,reliability,nova
4116,injected_file_content_bytes,The number of bytes allowed per injected file.,reliability,nova
4117,injected_file_path_length,The maximum allowed injected file path length.,reliability,nova
4118,security_groups,The number of security groups per project.,security,nova
4119,security_group_rules,The number of security rules per security group.,security,nova
4120,key_pairs,The maximum number of key pairs allowed per user.,security,nova
4121,server_groups,The maxiumum number of server groups per project.,reliability,nova
4122,server_group_members,The maximum number of servers per server group.,reliability,nova
4123,driver,Provides abstraction for quota checks. Users can configure a specific driver to use for quota checks.,environment,nova
4124,recheck_quota,Recheck quota after resource creation to prevent allowing quota to be exceeded.,reliability,nova
4125,enabled,Enable Remote Desktop Protocol (RDP) related features.,others,nova
4126,html5_proxy_base_url,The URL an end user would use to connect to the RDP HTML5 console proxy. The console proxy service is called with this token-embedded URL and establishes the connection to the proper instance.,environment,nova
4127,host,Debug host (IP or name) to connect to. This command line parameter is used when you want to connect to a nova service via a debugger running on a different host.,debuggability,nova
4128,port,Debug port to connect to. This command line parameter allows you to specify the port you want to use to connect to a nova service via a debugger running on different host.,debuggability,nova
4129,driver,"The class of the driver used by the scheduler. This should be chosen from one of the entrypoints under the namespace 'nova.scheduler.driver' of file 'setup.cfg'. If nothing is specified in this option, the 'filter_scheduler' is used.",environment,nova
4130,periodic_task_interval,Periodic task interval.,reliability,nova
4131,max_attempts,"This is the maximum number of attempts that will be made for a given instance build/move operation. It limits the number of alternate hosts returned by the scheduler. When that list of hosts is exhausted, a MaxRetriesExceeded exception is raised and the instance is set to an error state.",reliability,nova
4132,discover_hosts_in_cells_interval,Periodic task interval.,reliability,nova
4133,max_placement_results,This setting determines the maximum limit on results received from the placement service during a scheduling operation. It effectively limits the number of hosts that may be considered for scheduling requests that match a large number of candidates.,reliability,nova
4134,workers,"Number of workers for the nova-scheduler service. The default will be the number of CPUs available if using the 'filter_scheduler' scheduler driver, otherwise the default will be 1.",performance,nova
4135,limit_tenants_to_placement_aggregate,"This setting causes the scheduler to look up a host aggregate with the metadata key of filter_tenant_id set to the project of an incoming request, and request results from placement be limited to that aggregate. Multiple tenants may be added to a single aggregate by appending a serial number to the key, such as filter_tenant_id:123.",others,nova
4136,placement_aggregate_required_for_tenants,"This setting, when limit_tenants_to_placement_aggregate=True, will control whether or not a tenant with no aggregate affinity will be allowed to schedule to any available node. If aggregates are used to limit some tenants but not all, then this should be False. If all tenants should be confined via aggregate, then this should be True to prevent them from receiving unrestricted scheduling to any available node.",others,nova
4137,query_placement_for_availability_zone,"This setting causes the scheduler to look up a host aggregate with the metadata key of availability_zone set to the value provided by an incoming request, and request results from placement be limited to that aggregate.",others,nova
4138,enabled,Enable the serial console feature.,others,nova
4139,port_range,A range of TCP ports a guest can use for its backend.,environment,nova
4140,base_url,The URL an end user would use to connect to the nova-serialproxy service.,environment,nova
4141,proxyclient_address,The IP address to which proxy clients (like nova-serialproxy) should connect to get the serial console of an instance.,environment,nova
4142,serialproxy_host,The IP address which is used by the nova-serialproxy service to listen for incoming requests.,environment,nova
4143,serialproxy_port,The port number which is used by the nova-serialproxy service to listen for incoming requests.,environment,nova
4144,send_service_user_token,"When True, if sending a user token to a REST API, also send a service token.",others,nova
4145,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
4146,certfile,PEM encoded client certificate cert file,security,nova
4147,keyfile,PEM encoded client certificate key file,security,nova
4148,insecure,Verify HTTPS connections.,security,nova
4149,timeout,Timeout value for http requests,reliability,nova
4150,collect_timing,Collect per-API call timing information.,debuggability,nova
4151,split_loggers,Log requests to multiple loggers.,debuggability,nova
4152,auth_type,Authentication type to load,security,nova
4153,auth_section,Config Section from which to load plugin specific options,security,nova
4154,auth_url,Authentication URL,security,nova
4155,system_scope,Scope for system operations,reliability,nova
4156,domain_id,Domain ID to scope to,environment,nova
4157,domain_name,Domain name to scope to,environment,nova
4158,project_id,Project ID to scope to,environment,nova
4159,project_name,Project name to scope to,environment,nova
4160,project_domain_id,Domain ID containing project,environment,nova
4161,project_domain_name,Domain name containing project,environment,nova
4162,trust_id,Trust ID,security,nova
4163,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4164,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4165,user_id,User ID,environment,nova
4166,username,Username,environment,nova
4167,user_domain_id,User's domain id,environment,nova
4168,user_domain_name,User's domain name,environment,nova
4169,password,User's password,security,nova
4170,tenant_id,Tenant ID,environment,nova
4171,tenant_name,Tenant Name,environment,nova
4172,enabled,Enable SPICE related features.,others,nova
4173,agent_enabled,Enable the SPICE guest agent support on the instances.,others,nova
4174,html5proxy_base_url,Location of the SPICE HTML5 console proxy.,environment,nova
4175,server_listen,The  address where the SPICE server running on the instances should listen.,environment,nova
4176,server_proxyclient_address,The address used by nova-spicehtml5proxy client to connect to instance console.,environment,nova
4177,keymap,A keyboard layout which is supported by the underlying hypervisor on this node.,others,nova
4178,html5proxy_host,IP address or a hostname on which the nova-spicehtml5proxy service listens for incoming requests.,environment,nova
4179,html5proxy_port,Port on which the nova-spicehtml5proxy service listens for incoming requests.,environment,nova
4180,compute,Compute RPC API version cap.,environment,nova
4181,cells,Cells RPC API version cap.,environment,nova
4182,intercell,Intercell RPC API version cap.,environment,nova
4183,cert,Cert RPC API version cap.,environment,nova
4184,scheduler,Scheduler RPC API version cap.,environment,nova
4185,conductor,Conductor RPC API version cap.,environment,nova
4186,console,Console RPC API version cap.,environment,nova
4187,consoleauth,Consoleauth RPC API version cap.,environment,nova
4188,network,Network RPC API version cap.,environment,nova
4189,baseapi,Base API RPC API version cap.,environment,nova
4190,root_token_id,root token for vault,security,nova
4191,approle_role_id,AppRole role_id for authentication with vault,security,nova
4192,approle_secret_id,AppRole secret_id for authentication with vault,security,nova
4193,kv_mountpoint,"Mountpoint of KV store in Vault to use, for example: secret",others,nova
4194,vault_url,"Use this endpoint to connect to Vault, for example: 'http://127.0.0.1:8200'",environment,nova
4195,ssl_ca_crt_file,Absolute path to ca cert file,security,nova
4196,use_ssl,SSL Enabled/Disabled,security,nova
4197,cafile,PEM encoded Certificate Authority to use when verifying HTTPs connections.,security,nova
4198,certfile,PEM encoded client certificate cert file,security,nova
4199,keyfile,PEM encoded client certificate key file,security,nova
4200,insecure,Verify HTTPS connections.,security,nova
4201,timeout,Timeout value for http requests,reliability,nova
4202,collect_timing,Collect per-API call timing information.,debuggability,nova
4203,split_loggers,Log requests to multiple loggers.,debuggability,nova
4204,auth_type,Authentication type to load,security,nova
4205,auth_section,Config Section from which to load plugin specific options,security,nova
4206,auth_url,Authentication URL,security,nova
4207,system_scope,Scope for system operations,reliability,nova
4208,domain_id,Domain ID to scope to,environment,nova
4209,domain_name,Domain name to scope to,environment,nova
4210,project_id,Project ID to scope to,environment,nova
4211,project_name,Project name to scope to,environment,nova
4212,project_domain_id,Domain ID containing project,environment,nova
4213,project_domain_name,Domain name containing project,environment,nova
4214,trust_id,Trust ID,security,nova
4215,default_domain_id,Optional domain ID to use with v3 and v2 parameters. It will be used  for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4216,default_domain_name,Optional domain name to use with v3 API and v2 parameters. It will be used for both the user and project domain in v3 and ignored in v2  authentication.,environment,nova
4217,user_id,User ID,environment,nova
4218,username,Username,environment,nova
4219,user_domain_id,User's domain id,environment,nova
4220,user_domain_name,User's domain name,environment,nova
4221,password,User's password,security,nova
4222,tenant_id,Tenant ID,environment,nova
4223,tenant_name,Tenant Name,environment,nova
4224,vlan_interface,This option specifies the physical ethernet adapter name for VLAN networking.,others,nova
4225,integration_bridge,This option should be configured only when using the NSX-MH Neutron plugin. This is the name of the integration bridge on the ESXi server or host. This should not be set for any other Neutron plugin. Hence the default value is not set.,others,nova
4226,console_delay_seconds,Set this value if affected by an increased network latency causing repeated characters when typing in a remote console.,reliability,nova
4227,serial_port_service_uri,Identifies the remote system where the serial port traffic will be sent.,environment,nova
4228,serial_port_proxy_uri,Identifies a proxy service that provides network access to the serial_port_service_uri.,environment,nova
4229,serial_log_dir,Specifies the directory where the Virtual Serial Port Concentrator is storing console log files. It should match the 'serial_log_dir' config value of VSPC.,environment,nova
4230,host_ip,Hostname or IP address for connection to VMware vCenter host.,environment,nova
4231,host_port,Port for connection to VMware vCenter host.,environment,nova
4232,host_username,Username for connection to VMware vCenter host.,environment,nova
4233,host_password,Password for connection to VMware vCenter host.,security,nova
4234,ca_file,Specifies the CA bundle file to be used in verifying the vCenter server certificate.,security,nova
4235,insecure,"If true, the vCenter server certificate is not verified. If false, then the default CA truststore is used for verification.",security,nova
4236,cluster_name,Name of a VMware Cluster ComputeResource.,environment,nova
4237,datastore_regex,Regular expression pattern to match the name of datastore.,others,nova
4238,task_poll_interval,Time interval in seconds to poll remote tasks invoked on VMware VC server.,reliability,nova
4239,api_retry_count,"Number of times VMware vCenter server API must be retried on connection failures, e.g. socket error, etc.",reliability,nova
4240,vnc_port,This option specifies VNC starting port.,environment,nova
4241,vnc_port_total,Total number of VNC ports.,others,nova
4242,vnc_keymap,Keymap for VNC.,others,nova
4243,use_linked_clone,This option enables/disables the use of linked clone.,others,nova
4244,connection_pool_size,This option sets the http connection pool size,performance,nova
4245,pbm_enabled,This option enables or disables storage policy based placement of instances.,others,nova
4246,pbm_wsdl_location,This option specifies the PBM service WSDL file location URL.,environment,nova
4247,pbm_default_policy,This option specifies the default policy to be used.,performance,nova
4248,maximum_objects,This option specifies the limit on the maximum number of objects to return in a single result.,reliability,nova
4249,cache_prefix,This option adds a prefix to the folder where cached images are stored,others,nova
4250,enabled,Enable VNC related features.,others,nova
4251,keymap,Keymap for VNC.,environment,nova
4252,server_listen,The IP address or hostname on which an instance should listen to for incoming VNC connection requests on this node.,environment,nova
4253,server_proxyclient_address,"Private, internal IP address or hostname of VNC console proxy.",environment,nova
4254,novncproxy_base_url,Public address of noVNC VNC console proxy.,environment,nova
4255,xvpvncproxy_host,IP address or hostname that the XVP VNC console proxy should bind to.,environment,nova
4256,xvpvncproxy_port,Port that the XVP VNC console proxy should bind to.,environment,nova
4257,xvpvncproxy_base_url,Public URL address of XVP VNC console proxy.,environment,nova
4258,novncproxy_host,IP address that the noVNC console proxy should bind to.,environment,nova
4259,novncproxy_port,Port that the noVNC console proxy should bind to.,environment,nova
4260,auth_schemes,The authentication schemes to use with the compute node.,security,nova
4261,vencrypt_client_key,The path to the client certificate PEM file (for x509),security,nova
4262,vencrypt_client_cert,The path to the client key file (for x509),security,nova
4263,vencrypt_ca_certs,The path to the CA certificate PEM file,security,nova
4264,disable_rootwrap,Use sudo instead of rootwrap.,security,nova
4265,disable_libvirt_livesnapshot,Disable live snapshots when using the libvirt driver.,others,nova
4266,handle_virt_lifecycle_events,Enable handling of events emitted from compute drivers.,others,nova
4267,disable_group_policy_check_upcall,Disable the server group policy check upcall in compute.,others,nova
4268,enable_consoleauth,Enable the consoleauth service to avoid resetting unexpired consoles.,security,nova
4269,enable_numa_live_migration,Enable live migration of instances with NUMA topologies.,others,nova
4270,ensure_libvirt_rbd_instance_dir_cleanup,Ensure the instance directory is removed during clean up when using rbd.,security,nova
4271,api_paste_config,This option represents a file name for the paste.deploy config for nova-api.,environment,nova
4272,wsgi_log_format,"It represents a python format string that is used as the template to generate log lines. The following values can be formatted into it: client_ip, date_time, request_line, status_code, body_length, wall_seconds.",others,nova
4273,secure_proxy_ssl_header,"This option specifies the HTTP header used to determine the protocol scheme for the original request, even if it was removed by a SSL terminating proxy.",security,nova
4274,ssl_ca_file,This option allows setting path to the CA certificate file that should be used to verify connecting clients.,security,nova
4275,ssl_cert_file,This option allows setting path to the SSL certificate of API server.,security,nova
4276,ssl_key_file,This option specifies the path to the file where SSL private key of API server is stored when SSL is in effect.,security,nova
4277,tcp_keepidle,This option sets the value of TCP_KEEPIDLE in seconds for each server socket. It specifies the duration of time to keep connection active. TCP generates a KEEPALIVE transmission for an application that requests to keep connection active. Not supported on OS X.,reliability,nova
4278,default_pool_size,This option specifies the size of the pool of greenthreads used by wsgi. It is possible to limit the number of concurrent connections using this option.,performance,nova
4279,max_header_line,This option specifies the maximum line size of message headers to be accepted. max_header_line may need to be increased when using large tokens (typically those generated by the Keystone v3 API with big service catalogs).,reliability,nova
4280,keep_alive,"This option allows using the same TCP connection to send and receive multiple HTTP requests/responses, as opposed to opening a new one for every single request/response pair. HTTP keep-alive indicates HTTP connection reuse.",reliability,nova
4281,client_socket_timeout,This option specifies the timeout for client connections' socket operations. If an incoming connection is idle for this number of seconds it will be closed. It indicates timeout on individual read/writes on the socket connection. To wait forever set to 0.,reliability,nova
4282,agent_timeout,Number of seconds to wait for agent's reply to a request.,reliability,nova
4283,agent_version_timeout,Number of seconds to wait for agent't reply to version request.,reliability,nova
4284,agent_resetnetwork_timeout,Number of seconds to wait for agent's reply to resetnetwork request.,reliability,nova
4285,agent_path,Path to locate guest agent on the server.,environment,nova
4286,disable_agent,Disables the use of XenAPI agent.,others,nova
4287,use_agent_default,Whether or not to use the agent by default when its usage is enabled but not indicated by the image.,others,nova
4288,login_timeout,Timeout in seconds for XenAPI login.,reliability,nova
4289,connection_concurrent,Maximum number of concurrent XenAPI connections.,reliability,nova
4290,cache_images,Cache glance images locally.,performance,nova
4291,image_compression_level,Compression level for images.,performance,nova
4292,default_os_type,Default OS type used when uploading an image to glance,environment,nova
4293,block_device_creation_timeout,Time in secs to wait for a block device to be created,reliability,nova
4294,max_kernel_ramdisk_size,Maximum size in bytes of kernel or ramdisk images.,reliability,nova
4295,sr_matching_filter,Filter for finding the SR to be used to install guest instances on.,others,nova
4296,sparse_copy,Whether to use sparse_copy for copying data on a resize down. (False will use standard dd). This speeds up resizes down considerably since large runs of zeros won't have to be rsynced.,performance,nova
4297,num_vbd_unplug_retries,"Maximum number of retries to unplug VBD. If set to 0, should try once, no retries.",reliability,nova
4298,ipxe_network_name,Name of network to use for booting iPXE ISOs.,environment,nova
4299,ipxe_boot_menu_url,URL to the iPXE boot menu.,environment,nova
4300,ipxe_mkisofs_cmd,Name and optionally path of the tool used for ISO image creation.,environment,nova
4301,connection_url,URL for connection to XenServer/Xen Cloud Platform. A special value of unix://local can be used to connect to the local unix socket.,environment,nova
4302,connection_username,Username for connection to XenServer/Xen Cloud Platform,environment,nova
4303,connection_password,Password for connection to XenServer/Xen Cloud Platform,security,nova
4304,vhd_coalesce_poll_interval,The interval used for polling of coalescing vhds.,reliability,nova
4305,check_host,Ensure compute service is running on host XenAPI connects to. This option must be set to false if the 'independent_compute' option is set to true.,reliability,nova
4306,vhd_coalesce_max_attempts,Max number of times to poll for VHD to coalesce.,reliability,nova
4307,sr_base_path,Base path to the storage repository on the XenServer host.,environment,nova
4308,target_host,The iSCSI Target Host.,environment,nova
4309,target_port,The iSCSI Target Port.,environment,nova
4310,independent_compute,"Used to prevent attempts to attach VBDs locally, so Nova can be run in a VM on a different host.",others,nova
4311,running_timeout,Wait time for instances to go to running state.,reliability,nova
4312,image_upload_handler,Dom0 plugin driver used to handle image uploads.,environment,nova
4313,image_handler,The plugin used to handle image uploads and downloads.,others,nova
4314,introduce_vdi_retry_wait,Number of seconds to wait for SR to settle if the VDI does not exist when first introduced.,reliability,nova
4315,ovs_integration_bridge,The name of the integration Bridge that is used with xenapi when connecting with Open vSwitch.,environment,nova
4316,use_join_force,"When adding new host to a pool, this will append a --force flag to the command, forcing hosts to join a pool, even if they have different CPUs.",others,nova
4317,console_public_hostname,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
4318,console_xvp_conf_template,XVP conf template,environment,nova
4319,console_xvp_conf,Generated XVP conf file,environment,nova
4320,console_xvp_pid,XVP master process pid file,environment,nova
4321,console_xvp_log,XVP log file,debuggability,nova
4322,console_xvp_multiplex_port,Port for XVP to multiplex VNC connections on,environment,nova
4323,cloud_connector_url,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
4324,ca_file,CA certificate file to be verified in httpd server with TLS enabled,security,nova
4325,image_tmp_path,"This option has a sample default set, which means that its actual default value may vary from the one documented above.",environment,nova
4326,reachable_timeout,Timeout (seconds) to wait for an instance to start.,reliability,nova
4327,data_directory,Specifies the directory to use for data storage.,environment,postgresql
4328,config_file,Specifies the main server configuration file,environment,postgresql
4329,hba_file,Specifies the configuration file for host-based authentication,security,postgresql
4330,ident_file,Specifies the configuration file for user name mapping,environment,postgresql
4331,external_pid_file,Specifies the name of an additional process-ID (PID) file that the server should create for use by server administration programs.,environment,postgresql
4332,listen_addresses ,Specifies the TCP/IP address(es) on which the server is to listen for connections from client applications. ,environment,postgresql
4333,port,The TCP port the server listens on,environment,postgresql
4334,max_connections,Determines the maximum number of concurrent connections to the database server.,performance,postgresql
4335,superuser_reserved_connections,Determines the number of connection slots that are reserved for connections,environment,postgresql
4336,unix_socket_directories,Specifies the directory of the Unix-domain socket(s) on which the server is to listen,environment,postgresql
4337,unix_socket_group,Sets the owning group of the Unix-domain socket(s),security,postgresql
4338,unix_socket_permissions,Sets the access permissions of the Unix-domain socket(s). ,security,postgresql
4339,bonjour,Enables advertising the server's existence via Bonjour. ,others,postgresql
4340,bonjour_name,Specifies the Bonjour service name.,others,postgresql
4341,tcp_keepalives_idle,Specifies the number of seconds of inactivity after which TCP should send a keepalive message to the client.,reliability,postgresql
4342,tcp_keepalives_interval,Specifies the number of seconds after which a TCP keepalive message should be retransmitted the client should be retransmitted.,reliability,postgresql
4343,tcp_keepalives_count,Specifies the number of TCP keepalives that can be lost before the server's connection to the client is considered dead. ,reliability,postgresql
4344,authentication_timeout,"Maximum time to complete client authentication, in seconds.",reliability,postgresql
4345,ssl,Enables SSL connections. ,security,postgresql
4346,ssl_ca_file,Specifies the name of the file containing the SSL server certificate authority,security,postgresql
4347,ssl_cert_file,Specifies the name of the file containing the SSL server certificate. ,security,postgresql
4348,ssl_crl_file,Specifies the name of the file containing the SSL server certificate revocation list ,security,postgresql
4349,ssl_key_file,Specifies the name of the file containing the SSL server private key. ,security,postgresql
4350,ssl_ciphers,Specifies a list of SSL cipher suites that are allowed to be used on secure connections.,security,postgresql
4351,ssl_prefer_server_ciphers,"Specifies whether to use the server's SSL cipher preferences, rather than the client's. The default is true",security,postgresql
4352,ssl_ecdh_curve,Specifies the name of the curve to use in ECDH key exchange.,security,postgresql
4353,password_encryption,this parameter determines whether the password is to be encrypted.,security,postgresql
4354,krb_server_keyfile,Sets the location of the Kerberos server key file. ,security,postgresql
4355,krb_caseins_users,Sets whether GSSAPI user names should be treated case-insensitively. ,security,postgresql
4356,db_user_namespace,This parameter enables per-database user names.,others,postgresql
4357,shared_buffers,Sets the amount of memory the database server uses for shared memory buffers. ,performance,postgresql
4358,huge_pages,Enables/disables the use of huge memory pages.,performance,postgresql
4359,temp_buffers,Sets the maximum number of temporary buffers used by each database session.,performance,postgresql
4360,max_prepared_transactions,Sets the maximum number of transactions that can be in the prepared state simultaneously,performance,postgresql
4361,work_mem,Specifies the amount of memory to be used by internal sort operations and hash tables before writing,performance,postgresql
4362,maintenance_work_mem,Specifies the maximum amount of memory to be used by maintenance operations,performance,postgresql
4363,autovacuum_work_mem,Specifies the maximum amount of memory to be used by each autovacuum worker process.,performance,postgresql
4364,max_stack_depth,Specifies the maximum safe depth of the server's execution stack. ,reliability,postgresql
4365,dynamic_shared_memory_type,Specifies the dynamic shared memory implementation that the server should use. ,performance,postgresql
4366,temp_file_limit,Specifies the maximum amount of disk space that a process can use for temporary files,performance,postgresql
4367,max_files_per_process,Sets the maximum number of simultaneously open files allowed to each server subprocess.,performance,postgresql
4368,vacuum_cost_delay,"The length of time, in milliseconds, that the process will sleep when the cost limit has been exceeded.",reliability,postgresql
4369,vacuum_cost_page_hit,The estimated cost for vacuuming a buffer found in the shared buffer cache.,performance,postgresql
4370,vacuum_cost_page_miss,The estimated cost for vacuuming a buffer that has to be read from disk.,performance,postgresql
4371,vacuum_cost_page_dirty,The estimated cost charged when vacuum modifies a block that was previously clean.,performance,postgresql
4372,vacuum_cost_limit,The accumulated cost that will cause the vacuuming process to sleep. ,reliability,postgresql
4373,bgwriter_delay,Specifies the delay between activity rounds for the background writer.,reliability,postgresql
4374,bgwriter_lru_maxpages,"In each round, no more than this many buffers will be written by the background writer. Setting this to zero disables background writing.",performance,postgresql
4375,bgwriter_lru_multiplier,The number of dirty buffers  needed by server processes during recent rounds.,performance,postgresql
4376,bgwriter_flush_after,limit the amount of dirty data in the kernel's page cache,reliability,postgresql
4377,effective_io_concurrency,Sets the number of concurrent disk I/O operations that PostgreSQL expects can be executed simultaneously.,performance,postgresql
4378,max_worker_processes,Sets the maximum number of background processes that the system can support.,performance,postgresql
4379,max_parallel_workers_per_gather,Sets the maximum number of workers that can be started by a single Gather node. ,performance,postgresql
4380,backend_flush_after,"Whenever more than backend_flush_after bytes have been written by a single backend, attempt to force the OS to issue these writes to the underlying storage.",reliability,postgresql
4381,old_snapshot_threshold,Sets the minimum time that a snapshot can be used without risk ,reliability,postgresql
4382,fsync,"While turning off fsync is often a performance benefit, this can result in unrecoverable data corruption in the event of a power failure or system crash.",performance,postgresql
4383,synchronous_commit,Specifies whether transaction commit will wait for WAL records to be written to disk before the command returns a success indication to the client.,reliability,postgresql
4384,wal_sync_method,Method used for forcing WAL updates out to disk.,others,postgresql
4385,full_page_writes,"When this parameter is on, the PostgreSQL server writes the entire content of each disk page to WAL during the first modification of that page after a checkpoint. ",others,postgresql
4386,wal_log_hints,"When this parameter is on, the PostgreSQL server writes the entire content",debuggability,postgresql
4387,wal_compression,"When this parameter is on, the PostgreSQL server compresses a full page image ",performance,postgresql
4388,wal_buffers,The amount of shared memory used for WAL data that has not yet been written to disk.,performance,postgresql
4389,wal_writer_delay,Specifies how often the WAL writer flushes WAL.,reliability,postgresql
4390,wal_writer_flush_after,Specifies how often the WAL writer flushes WAL.,reliability,postgresql
4391,commit_delay ,This can improve  group commit throughput,performance,postgresql
4392,commit_siblings,A larger value makes it more probable,performance,postgresql
4393,checkpoint_timeout,"Maximum time between automatic WAL checkpoints, in seconds.",reliability,postgresql
4394,checkpoint_completion_target,Specifies the target of checkpoint completion,reliability,postgresql
4395,checkpoint_flush_after ,"Whenever more than checkpoint_flush_after bytes have been written while performing a checkpoint, attempt to force the OS to issue these writes to the underlying storage.",reliability,postgresql
4396,checkpoint_warning,Write a message to the server log if checkpoints caused by the filling of checkpoint segment files happen closer together than this many seconds ,debuggability,postgresql
4397,max_wal_size,Maximum size to let the WAL grow to between automatic WAL checkpoints.,reliability,postgresql
4398,min_wal_size,"As long as WAL disk usage stays below this setting, old WAL files are always recycled for future use at a checkpoint, rather than removed. ",others,postgresql
4399,archive_mode,"When archive_mode is enabled, completed WAL segments are sent to archive storage by setting archive_command.",others,postgresql
4400,archive_command,The local shell command to execute to archive a completed WAL file segment.,environment,postgresql
4401,archive_timeout," To limit how old unarchived data can be, you can set archive_timeout to force the server to switch to a new WAL segment file periodically. ",reliability,postgresql
4402,max_wal_senders,Specifies the maximum number of concurrent connections from standby servers or streaming base backup clients ,reliability,postgresql
4403,max_replication_slots ,Specifies the maximum number of replication slots  that the server can support.,reliability,postgresql
4404,wal_keep_segments,"Specifies the minimum number of past log file segments kept in the pg_xlog directory, in case a standby server needs to fetch them for streaming replication.",reliability,postgresql
4405,wal_sender_timeout,Terminate replication connections that are inactive longer than the specified number of milliseconds.,reliability,postgresql
4406,track_commit_timestamp,Record commit time of transactions. ,others,postgresql
4407,synchronous_standby_names,Specifies a list of standby servers that can support synchronous replication,environment,postgresql
4408,vacuum_defer_cleanup_age,Specifies the number of transactions by which VACUUM and HOT updates will defer cleanup of dead row versions.This allows more time for queries on the standby to complete without incurring conflicts due to early cleanup of rows. ,reliability,postgresql
4409,hot_standby,Specifies whether or not you can connect and run queries during recovery,reliability,postgresql
4410,max_standby_archive_delay,this parameter determines how long the standby server should wait before ,reliability,postgresql
4411,max_standby_streaming_delay ,"When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries",reliability,postgresql
4412,wal_receiver_status_interval ,Specifies the minimum frequency for the WAL receiver process ,reliability,postgresql
4413,hot_standby_feedback ,"Specifies whether or not a hot standby will send feedback to the primary or upstream standby about queries currently executing on the standby. This parameter can be used to eliminate query cancels caused by leanup records, but can cause database bloat on the primary for some workloads. ",reliability,postgresql
4414,wal_receiver_timeout,Terminate replication connections that are inactive longer than the specified number of milliseconds.,reliability,postgresql
4415,wal_retrieve_retry_interval,Specify how long the standby server should wait when WAL data is not available,reliability,postgresql
4416,enable_bitmapscan,Enables or disables the query planner's use of bitmap-scan plan types.,others,postgresql
4417,enable_hashagg ,Enables or disables the query planner's use of hashed aggregation plan types.,others,postgresql
4418,enable_hashjoin,Enables or disables the query planner's use of hash-join plan types.,others,postgresql
4419,enable_indexscan ,Enables or disables the query planner's use of index-scan plan types.,others,postgresql
4420,enable_indexonlyscan,Enables or disables the query planner's use of index-only-scan plan types,others,postgresql
4421,enable_material,Enables or disables the query planner's use of materialization.,others,postgresql
4422,enable_mergejoin ,Enables or disables the query planner's use of merge-join plan types. ,others,postgresql
4423,enable_nestloop,Enables or disables the query planner's use of nested-loop join plans.,others,postgresql
4424,enable_seqscan ,Enables or disables the query planner's use of sequential scan plan types. ,others,postgresql
4425,enable_sort,Enables or disables the query planner's use of explicit sort steps. ,others,postgresql
4426,enable_tidscan,Enables or disables the query planner's use of TID scan plan types,others,postgresql
4427,seq_page_cost,Sets the planner's estimate of the cost of a disk page fetch that is part of a series of sequential fetches.,performance,postgresql
4428,random_page_cost,Sets the planner's estimate of the cost of a non-sequentially-fetched disk page,performance,postgresql
4429,cpu_tuple_cost ,Sets the planner's estimate of the cost of processing each row during a query. ,performance,postgresql
4430,cpu_index_tuple_cost,Sets the planner's estimate of the cost of processing each index entry,performance,postgresql
4431,cpu_operator_cost,estimate of the cost of processing each operator or function,performance,postgresql
4432,parallel_setup_cost,estimate of the cost of launching parallel worker processes,performance,postgresql
4433,parallel_tuple_cost,estimate of the cost of transferring one tuple,performance,postgresql
4434,min_parallel_relation_size ,Sets the minimum size of relations to be considered for parallel scan,performance,postgresql
4435,effective_cache_size ,assumption about the effective size of the disk cache,performance,postgresql
4436,geqo ,Enables or disables genetic query optimization.,performance,postgresql
4437,geqo_threshold,Use genetic query optimization to plan queries with at least this many FROM items involved.,performance,postgresql
4438,geqo_effort,"Controls the trade-off between planning time and query plan quality in GEQO. This variable must be an integer in the range from 1 to 10. The default value is five. Larger values increase the time spent doing query planning, but also increase the likelihood that an efficient query plan will be chosen.",performance,postgresql
4439,geqo_pool_size ,"Controls the pool size used by GEQO, that is the number of individuals in the genetic population.",performance,postgresql
4440,geqo_seed,Controls the initial value of the random number generator used by GEQO to select random paths through the join order search space.,others,postgresql
4441,log_destination,Set this parameter to a list of desired log destinations separated by commas.,debuggability,postgresql
4442,logging_collector,This parameter enables the logging collector,debuggability,postgresql
4443,log_directory,this parameter determines the directory in which log files will be created,debuggability,postgresql
4444,log_filename,sets the file names of the created log files,debuggability,postgresql
4445,log_file_mode,sets the permissions for log files,security,postgresql
4446,log_rotation_age,determines the maximum lifetime of an individual log file. ,reliability,postgresql
4447,log_rotation_size,determines the maximum size of an individual log file,debuggability,postgresql
4448,log_truncate_on_rotation,"When logging_collector is enabled, this parameter will cause PostgreSQL to truncate",others,postgresql
4449,syslog_facility,determines the syslog facility to be used,debuggability,postgresql
4450,syslog_ident,determines the program name used to identify PostgreSQL messages in syslog logs,debuggability,postgresql
4451,syslog_sequence_numbers,"When logging to syslog and this is on (the default), then each message will be prefixed by an increasing sequence number ",debuggability,postgresql
4452,syslog_split_messages ,"When logging to syslog is enabled, this parameter determines how messages are delivered to syslog.",debuggability,postgresql
4453,event_source,determines the program name used to identify PostgreSQL messages in the log,debuggability,postgresql
4454,client_min_messages ,Controls which message levels are sent to the client. ,others,postgresql
4455,log_min_messages,Controls which message levels are written to the server log.,debuggability,postgresql
4456,log_min_error_statement,Controls which SQL statements that cause an error condition are recorded in the server log.,debuggability,postgresql
4457,log_min_duration_statement,Causes the duration of each completed statement to be logged if the statement ran for at least thespecified number of milliseconds. ,debuggability,postgresql
4458,debug_print_plan,These parameters enable various debugging output to be emitted. ,debuggability,postgresql
4459,debug_pretty_print ,"When set, debug_pretty_print indents the messages produced ",debuggability,postgresql
4460,log_checkpoints,Causes checkpoints and restartpoints to be logged in the server log.,debuggability,postgresql
4461,log_connections,Causes each attempted connection to the server to be logged,debuggability,postgresql
4462,log_disconnections,Causes session terminations to be logged,debuggability,postgresql
4463,log_duration,Causes the duration of every completed statement to be logged. ,debuggability,postgresql
4464,log_error_verbosity,Controls the amount of detail written in the server log for each message that is logged.,debuggability,postgresql
4465,log_hostname, Turning this parameter on causes logging of the host name as well.,debuggability,postgresql
4466,log_line_prefix ,This is a printf-style string that is output at the beginning of each log line. ,debuggability,postgresql
4467,log_lock_waits,Controls whether a log message is produced,debuggability,postgresql
4468,log_statement,Controls which SQL statements are logged. ,debuggability,postgresql
4469,log_replication_commands,Causes each replication command to be logged in the server log. ,debuggability,postgresql
4470,log_temp_files ,Controls logging of temporary file names and sizes. ,debuggability,postgresql
4471,log_timezone ,Sets the time zone used for timestamps written in the server log.,debuggability,postgresql
4472,cluster_name,Sets the cluster name that appears in the process title for all server processes in this cluster,others,postgresql
4473,update_process_title,Enables updating of the process title every time a new SQL command is received by the server.,others,postgresql
4474,track_activities,Enables the collection of information on the currently executing command of each session,debuggability,postgresql
4475,track_activity_query_size,Specifies the number of bytes reserved to track the currently executing command for each active session,debuggability,postgresql
4476,track_counts ,Enables collection of statistics on database activity. ,debuggability,postgresql
4477,track_io_timing,Enables timing of database I/O calls.,debuggability,postgresql
4478,track_functions,Enables tracking of function call counts and time used.,debuggability,postgresql
4479,stats_temp_directory,Sets the directory to store temporary statistics data in,environment,postgresql
4480,autovacuum,Controls whether the server should run the autovacuum launcher daemon.,reliability,postgresql
4481,log_autovacuum_min_duration,Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds.,debuggability,postgresql
4482,autovacuum_max_workers ,Specifies the maximum number of autovacuum processes ,reliability,postgresql
4483,autovacuum_naptime ,Specifies the minimum delay between autovacuum runs on any given database. ,reliability,postgresql
4484,autovacuum_vacuum_threshold ,Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table.,others,postgresql
4485,autovacuum_analyze_threshold,"Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. ",others,postgresql
4486,autovacuum_vacuum_scale_factor ,Specifies a fraction of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. ,others,postgresql
4487,autovacuum_analyze_scale_factor ,Specifies a fraction of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. ,others,postgresql
4488,autovacuum_freeze_max_age,Specifies the maximum age (in transactions) that a table's pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table.,reliability,postgresql
4489,autovacuum_multixact_freeze_max_age ,Specifies the maximum age (in multixacts) that a table's pg_class.relminmxid field can attain before a VACUUM operation is forced to prevent multixact ID wraparound within the table. ,reliability,postgresql
4490,autovacuum_vacuum_cost_delay,Specifies the cost delay value that will be used in automatic VACUUM operations.,reliability,postgresql
4491,autovacuum_vacuum_cost_limit,Specifies the cost limit value that will be used in automatic VACUUM operations.,performance,postgresql
4492,search_path,This variable specifies the order in which schemas are searched,others,postgresql
4493,row_security ,This variable controls whether to raise an error in lieu of applying a row security policy.,security,postgresql
4494,default_tablespace,This variable specifies the default tablespace in which to create objects,others,postgresql
4495,temp_tablespaces,This variable specifies tablespaces in which to create temporary objects (temp tables and indexes on temp tables) when a CREATE command does not explicitly specify a tablespace. ,environment,postgresql
4496,check_function_bodies,"When set to off, it disables validation of the function body string",security,postgresql
4497,default_transaction_isolation ,This parameter controls the default isolation level of each new transaction.,security,postgresql
4498,default_transaction_read_only,This parameter controls the default read-only status of each new transaction. ,security,postgresql
4499,default_transaction_deferrable,This parameter controls the default deferrable status of each new transaction. ,security,postgresql
4500,session_replication_role,Controls firing of replication-related triggers and rules for the current session.,reliability,postgresql
4501,statement_timeout ,Abort any statement that takes more than the specified number of milliseconds.,reliability,postgresql
4502,lock_timeout,Abort any statement that waits longer than the specified number of milliseconds ,reliability,postgresql
4503,idle_in_transaction_session_timeout,Terminate any session with an open transaction that has been idle for longer than the specified duration in milliseconds.,reliability,postgresql
4504,vacuum_freeze_table_age ,VACUUM performs an aggressive scan if the table's pg_class.relfrozenxid field has reached the age specified by this setting. ,reliability,postgresql
4505,vacuum_freeze_min_age,Specifies the cutoff age (in transactions) that VACUUM should use to decide whether to freeze row versions while scanning a table. ,reliability,postgresql
4506,vacuum_multixact_freeze_table_age,VACUUM performs an aggressive scan if the table's pg_class.relminmxid field has reached the age specified by this setting. ,reliability,postgresql
4507,vacuum_multixact_freeze_min_age,Specifies the cutoff age (in multixacts) that VACUUM should use to decide whether to replace multixact IDs with a newer transaction ID or multixact ID while scanning a table.,reliability,postgresql
4508,bytea_output ,Sets the output format for values of type bytea. ,others,postgresql
4509,xmlbinary,Sets how binary values are to be encoded in XML.,others,postgresql
4510,xmloption,Sets whether DOCUMENT or CONTENT is implicit when converting between XML and character string values. ,others,postgresql
4511,gin_pending_list_limit ,Sets the maximum size of the GIN pending list which is used when fastupdate is enabled.,reliability,postgresql
4512,DateStyle,"Sets the display format for date and time values, as well as the rules for interpreting ambiguous date input values",others,postgresql
4513,IntervalStyle,Sets the display format for interval values,others,postgresql
4514,TimeZone,Sets the time zone for displaying and interpreting time stamps.,others,postgresql
4515,timezone_abbreviations ,Sets the collection of time zone abbreviations that will be accepted by the server for datetime input.,others,postgresql
4516,extra_float_digits,This parameter adjusts the number of digits displayed for floating-point values,others,postgresql
4517,client_encoding,Sets the client-side encoding (character set),environment,postgresql
4518,lc_messages,Sets the language in which messages are displayed.,others,postgresql
4519,lc_monetary ,Sets the locale to use for formatting monetary amounts,others,postgresql
4520,lc_numeric,Sets the locale to use for formatting numbers,others,postgresql
4521,lc_time,Sets the locale to use for formatting dates and times,others,postgresql
4522,default_text_search_config ,Selects the text search configuration that is used by those variants of the text search functions that do not have an explicit argument specifying the configuration. ,others,postgresql
4523,local_preload_libraries,This variable specifies one or more shared libraries that are to be preloaded at connection start. ,performance,postgresql
4524,session_preload_libraries,This variable specifies one or more shared libraries that are to be preloaded at connection start.,performance,postgresql
4525,shared_preload_libraries,This variable specifies one or more shared libraries to be preloaded at server start.,environment,postgresql
4526,dynamic_library_path ,the system will search this path for the required file,environment,postgresql
4527,gin_fuzzy_search_limit,Soft upper limit of the size of the set returned by GIN index scans.,reliability,postgresql
4528,deadlock_timeout,"This is the amount of time, in milliseconds, to wait on a lock before checking to see if there is a deadlock condition. ",reliability,postgresql
4529,max_locks_per_transaction,This parameter controls the average number of object locks allocated for each transaction; individual transactions can lock more objects as long as the locks of all transactions fit in the lock table. ,reliability,postgresql
4530,max_pred_locks_per_transaction,This parameter controls the average number of object locks allocated for each transaction; individual transactions can lock more objects as long as the locks of all transactions fit in the lock table. ,reliability,postgresql
4531,exit_on_error,"If true, any error will terminate the current session.",reliability,postgresql
4532,restart_after_crash ,"When set to true, which is the default, PostgreSQL will automatically reinitialize after a backend crash.",reliability,postgresql
4533,allow_system_table_mods ,Allows modification of the structure of system tables.,security,postgresql
4534,ignore_system_indexes,Ignore system indexes when reading system tables (but still update the indexes when modifying the tables). This is useful when recovering from damaged system indexes.,reliability,postgresql
4535,post_auth_delay ,"If nonzero, a delay of this many seconds occurs when a new server process is started, after it conducts the authentication procedure. This is intended to give developers an opportunity to attach to the server process with a debugger. ",debuggability,postgresql
4536,pre_auth_delay ,"If nonzero, a delay of this many seconds occurs just after a new server process is forked, before it conducts the authentication procedure.",security,postgresql
4537,trace_notify,Generates a great amount of debugging output for the LISTEN and NOTIFY commands.,debuggability,postgresql
4538,trace_recovery_messages,Enables logging of recovery-related debugging output that otherwise would not be logged. ,debuggability,postgresql
4539,trace_sort,"If on, emit information about resource usage during sort operations.",debuggability,postgresql
4540,trace_locks,"If on, emit information about lock usage.",debuggability,postgresql
4541,trace_lwlocks,"If on, emit information about lightweight lock usage.",debuggability,postgresql
4542,trace_userlocks ,"If on, emit information about user lock usage. Output is the same as for trace_locks, only for advisory locks.",debuggability,postgresql
4543,trace_lock_oidmin,"If set, do not trace locks for tables below this OID. (use to avoid output on system tables)",debuggability,postgresql
4544,trace_lock_table,Unconditionally trace locks on this table,debuggability,postgresql
4545,debug_deadlocks,"If set, dumps information about all current locks when a deadlock timeout occurs.",debuggability,postgresql
4546,log_btree_build_stats,"If set, logs system resource usage statistics (memory and CPU) on various B-tree operations.",debuggability,postgresql
4547,wal_debug,"If on, emit WAL-related debugging output.",debuggability,postgresql
4548,ignore_checksum_failure,"Detection of a checksum failure during a read normally causes PostgreSQL to report an error, aborting the current transaction. v However, it may allow you to get past the error and retrieve undamaged tuples that might still be present in the table if the block header is still sane. ",reliability,postgresql
4549,zero_damaged_pages,"Setting zero_damaged_pages to on causes the system to instead report a warning, zero out the damaged page in memory, and continue processing. ",debuggability,postgresql
4550,daemonize,By default Redis does not run as a daemon. Use 'yes' if you need it.,others,redis
4551,pidfile,"When running daemonized, Redis writes a pid file in /var/run/redis.pid by default. You can specify a custom pid file location here.",environment,redis
4552,port,"Accept connections on the specified port, default is 6379. If port 0 is specified Redis will not listen on a TCP socket.",environment,redis
4553,bind,"By default Redis listens for connections from all the network interfaces available on the server. It is possible to listen to just one or multiple interfaces using the ""bind"" configuration directive, followed by one or more IP addresses.",environment,redis
4554,unixsocket,"Specify the path for the unix socket that will be used to listen for incoming connections. There is no default, so Redis will not listen on a unix socket when not specified.",environment,redis
4555,timeout,Close the connection after a client is idle for N seconds (0 to disable),reliability,redis
4556,loglevel,Specify the server verbosity level.,debuggability,redis
4557,logfile,Specify the log file name. Also the empty string can be used to force Redis to log on the standard output.,debuggability,redis
4558,syslog-enabled,"To enable logging to the system logger, just set 'syslog-enabled' to yes, and optionally update the other syslog parameters to suit your needs.",debuggability,redis
4559,syslog-ident,Specify the syslog identity.,environment,redis
4560,syslog-facility,Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.,environment,redis
4561,databases,Set the number of databases.,others,redis
4562,save,Save the DB on disk,others,redis
4563,rdbcompression,Compress string objects using LZF when dump .rdb databases,others,redis
4564,rdbchecksum,"This makes the format more resistant to corruption but there is a performance hit to pay (around 10%) when saving and loading RDB files, so you can disable it for maximum performances.",performance,redis
4565,dbfilename,The filename where to dump the DB,environment,redis
4566,dir,The working directory.,environment,redis
4567,slaveof,Master-Slave replication. Use slaveof to make a Redis instance a copy of another Redis server.,reliability,redis
4568,masterauth,"If the master is password protected (using the ""requirepass"" configuration directive below) it is possible to tell the slave to authenticate before starting the replication synchronization process, otherwise the master will refuse the slave request.",security,redis
4569,slave-read-only,read only slaves are not designed to be exposed to untrusted clients on the internet. It's just a protection layer against misuse of the instance.,security,redis
4570,repl-ping-slave-period,Slaves send PINGs to server in a predefined interval.,reliability,redis
4571,repl-timeout,The following option sets the replication timeout.,reliability,redis
4572,repl-disable-tcp-nodelay,Disable TCP_NODELAY on the slave socket after SYNC,reliability,redis
4573,repl-backlog-size,Set the replication backlog size.,reliability,redis
4574,repl-backlog-ttl,"The following option configures the amount of seconds that need to elapse, starting from the time the last slave disconnected, for the backlog buffer to be freed.",reliability,redis
4575,slave-priority,The slave priority is an integer number published by Redis in the INFO output. It is used by Redis Sentinel in order to select a slave to promote into a master if the master is no longer working correctly.,reliability,redis
4576,maxclients,Set the max number of connected clients at the same time.,performance,redis
4577,maxmemory,Don't use more memory than the specified amount of bytes.,reliability,redis
4578,maxmemory-policy,how Redis will select what to remove when maxmemory is reached.,performance,redis
4579,maxmemory-samples,"LRU and minimal TTL algorithms are not precise algorithms but approximated algorithms (in order to save memory), so you can select as well the sample size to check.",performance,redis
4580,appendfilename,The name of the append only file,others,redis
4581,appendfsync,The fsync() call tells the Operating System to actually write data on disk instead to wait for more data in the output buffer.,performance,redis
4582,no-appendfsync-on-rewrite,"If you have latency problems turn this to ""yes"". Otherwise leave it as ""no"" that is the safest pick from the point of view of durability.",security,redis
4583,auto-aof-rewrite-percentage,Redis is able to automatically rewrite the log file implicitly calling BGREWRITEAOF when the AOF log size grows by the specified percentage.,debuggability,redis
4584,auto-aof-rewrite-min-size,"you need to specify a minimal size for the AOF file to be rewritten, this is useful to avoid rewriting the AOF file even if the percentage increase is reached but it is still pretty small.",reliability,redis
4585,lua-time-limit,If the maximum execution time is reached Redis will log that a script is still in execution after the maximum allowed time and will start to reply to queries with an error.,debuggability,redis
4586,slowlog-log-slower-than,"the execution time, in microseconds, to exceed in order for the command to get logged",debuggability,redis
4587,slowlog-max-len,the length of the slow log,debuggability,redis
4588,hash-max-ziplist-entries,"Hashes are encoded using a memory efficient data structure when they have a small number of entries, and the biggest entry does not exceed a given threshold.",reliability,redis
4589,set-max-intset-entries,The following configuration setting sets the limit in the size of the set in order to use this special memory saving encoding.,reliability,redis
4590,activerehashing,Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in order to help rehashing the main Redis hash table,performance,redis
4591,hz,"By default ""hz"" is set to 10. Raising the value will use more CPU when Redis is idle, but at the same time will make Redis more responsive when there are many keys expiring at the same time, and timeouts may be handled with more precision.",performance,redis
4592,aof-rewrite-incremental-fsync,"When a child rewrites the AOF file, if the following option is enabled the file will be fsync-ed every 32 MB of data generated.",reliability,redis
4593,spark.app.name,The name of your application. This will appear in the UI and in log data.,others,spark
4594,spark.driver.cores,"Number of cores to use for the driver process, only in cluster mode.",performance,spark
4595,spark.driver.maxResultSize,"Limit of total size of serialized results of all partitions for each Spark action (e.g.  collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total  size is above this limit. Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory and memory overhead of objects in JVM). Setting a proper limit can protect the driver from out-of-memory errors.",reliability,spark
4596,spark.driver.memory,"Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g). Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-memory command line option or in your default properties file.",performance,spark
4597,spark.driver.memoryOverhead,"The amount of off-heap memory to be allocated per driver in cluster mode, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings,  other native overheads, etc. This tends to grow with the container size (typically 6-10%).  This option is currently supported on YARN and Kubernetes.",performance,spark
4598,spark.executor.memory,"Amount of memory to use per executor process, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g).",performance,spark
4599,spark.executor.pyspark.memory,"The amount of memory to be allocated to PySpark in each executor, in MiB unless otherwise specified.  If set, PySpark memory for an executor will be limited to this amount. If not set, Spark will not limit Python's memory use and it is up to the application to avoid exceeding the overhead memory space shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory is added to executor resource requests. NOTE: Python memory usage may not be limited on platforms that do not support resource limiting, such as Windows.",performance,spark
4600,spark.executor.memoryOverhead,"The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified. This is memory that accounts for things like VM overheads, interned strings, other native  overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.",performance,spark
4601,spark.extraListeners,"A comma-separated list of classes that implement SparkListener; when initializing SparkContext, instances of these classes will be created and registered with Spark's listener bus.  If a class has a single-argument constructor that accepts a SparkConf, that constructor will be called; otherwise, a zero-argument constructor will be called. If no valid constructor can be found, the SparkContext creation will fail with an exception.",environment,spark
4602,spark.local.dir,"Directory to use for ""scratch"" space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. NOTE: In Spark 1.0 and later this will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.",environment,spark
4603,spark.logConf,Logs the effective SparkConf as INFO when a SparkContext is started.,debuggability,spark
4604,spark.master,The cluster manager to connect to. See the list of allowed master URL's.,environment,spark
4605,spark.submit.deployMode,"The deploy mode of Spark driver program, either ""client"" or ""cluster"", Which means to launch driver program locally (""client"") or remotely (""cluster"") on one of the nodes inside the cluster.",others,spark
4606,spark.log.callerContext,"Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS. Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise, and typically can have up to 50 characters.",debuggability,spark
4607,spark.driver.supervise,"If true, restarts the driver automatically if it fails with a non-zero exit status. Only has effect in Spark standalone mode or Mesos cluster deploy mode.",reliability,spark
4608,spark.driver.extraClassPath,"Extra classpath entries to prepend to the classpath of the driver. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-class-path command line option or in your default properties file.",environment,spark
4609,spark.driver.extraJavaOptions,"A string of extra JVM options to pass to the driver. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file.",others,spark
4610,spark.driver.extraLibraryPath,"Set a special library path to use when launching the driver JVM. Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-library-path command line option or in your default properties file.",environment,spark
4611,spark.driver.userClassPathFirst,(Experimental) Whether to give user-added jars precedence over Spark's own jars when loading classes in the driver. This feature can be used to mitigate conflicts between Spark's dependencies and user dependencies. It is currently an experimental feature. This is used in cluster mode only.,environment,spark
4612,spark.executor.extraClassPath,Extra classpath entries to prepend to the classpath of executors. This exists primarily for backwards-compatibility with older versions of Spark. Users typically should not need to set this option.,environment,spark
4613,spark.executor.extraJavaOptions,"A string of extra JVM options to pass to executors. For instance, GC settings or other logging. Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory. The following symbols, if present will be interpolated:  will be replaced by application ID and  will be replaced by executor ID. For example, to enable verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of: -verbose:gc -Xloggc:/tmp/-.gc",others,spark
4614,spark.executor.extraLibraryPath,Set a special library path to use when launching executor JVM's.,environment,spark
4615,spark.executor.logs.rolling.maxRetainedFiles,Sets the number of latest rolling log files that are going to be retained by the system. Older log files will be deleted. Disabled by default.,reliability,spark
4616,spark.executor.logs.rolling.enableCompression,"Enable executor log compression. If it is enabled, the rolled executor logs will be compressed. Disabled by default.",performance,spark
4617,spark.executor.logs.rolling.maxSize,Set the max size of the file in bytes by which the executor logs will be rolled over. Rolling is disabled by default. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.,reliability,spark
4618,spark.executor.logs.rolling.strategy,"Set the strategy of rolling of executor logs. By default it is disabled. It can be set to ""time"" (time-based rolling) or ""size"" (size-based rolling). For ""time"", use spark.executor.logs.rolling.time.interval to set the rolling interval. For ""size"", use spark.executor.logs.rolling.maxSize to set the maximum file size for rolling.",reliability,spark
4619,spark.executor.logs.rolling.time.interval,"Set the time interval by which the executor logs will be rolled over. Rolling is disabled by default. Valid values are daily, hourly, minutely or any interval in seconds. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.",reliability,spark
4620,spark.executor.userClassPathFirst,"(Experimental) Same functionality as spark.driver.userClassPathFirst, but applied to executor instances.",environment,spark
4621,spark.executorEnv.[EnvironmentVariableName],Add the environment variable specified by EnvironmentVariableName to the Executor process. The user can specify multiple of these to set multiple environment variables.,environment,spark
4622,spark.redaction.regex,"Regex to decide which Spark configuration properties and environment variables in driver and executor environments contain sensitive information. When this regex matches a property key or value, the value is redacted from the environment UI and various logs like YARN and event logs.",environment,spark
4623,spark.python.profile,"Enable profiling in Python worker, the profile result will show up by sc.show_profiles(), or it will be displayed before the driver exits. It also can be dumped into disk by sc.dump_profiles(path). If some of the profile results had been displayed manually, they will not be displayed automatically before driver exiting. By default the pyspark.profiler.BasicProfiler will be used, but this can be overridden by passing a profiler class in as a parameter to the SparkContext constructor.",performance,spark
4624,spark.python.profile.dump,"The directory which is used to dump the profile result before driver exiting. The results will be dumped as separated file for each RDD. They can be loaded by pstats.Stats(). If this is specified, the profile result will not be displayed automatically.",environment,spark
4625,spark.python.worker.memory,"Amount of memory to use per python worker process during aggregation, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g). If the memory used during aggregation goes above this amount, it will spill the data into disks.",performance,spark
4626,spark.python.worker.reuse,"Reuse Python worker or not. If yes, it will use a fixed number of Python workers, does not need to fork() a Python process for every task. It will be very useful if there is large broadcast, then the broadcast will not be needed to transferred from JVM to Python worker for every task.",performance,spark
4627,spark.files,Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.,environment,spark
4628,spark.submit.pyFiles,"Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.",environment,spark
4629,spark.jars,Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.,environment,spark
4630,spark.jars.packages,"Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories. For more details, see Advanced Dependency Management.",environment,spark
4631,spark.jars.excludes,"Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in spark.jars.packages to avoid dependency conflicts.",environment,spark
4632,spark.jars.ivy,"Path to specify the Ivy user directory, used for the local Ivy cache and package files from spark.jars.packages. This will override the Ivy property ivy.default.ivy.user.dir which defaults to ~/.ivy2.",environment,spark
4633,spark.jars.ivySettings,"Path to an Ivy settings file to customize resolution of jars specified using spark.jars.packages instead of the built-in defaults, such as maven central. Additional repositories given by the command-line option --repositories or spark.jars.repositories will also be included. Useful for allowing Spark to resolve artifacts from behind a firewall e.g. via an in-house artifact server like Artifactory. Details on the settings file format can be found at Settings Files",environment,spark
4634,spark.jars.repositories,Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages or spark.jars.packages.,environment,spark
4635,spark.pyspark.driver.python,Python binary executable to use for PySpark in driver. (default is spark.pyspark.python),environment,spark
4636,spark.pyspark.python,Python binary executable to use for PySpark in both driver and executors.,environment,spark
4637,spark.reducer.maxSizeInFlight,"Maximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless  otherwise specified. Since each output requires us to create a buffer to receive it, this  represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory.",reliability,spark
4638,spark.reducer.maxReqsInFlight,"This configuration limits the number of remote requests to fetch blocks at any given point. When the number of hosts in the cluster increase, it might lead to very large number of inbound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated.",reliability,spark
4639,spark.reducer.maxBlocksInFlightPerAddress,"This configuration limits the number of remote blocks being fetched per reduce task from a given host port. When a large number of blocks are being requested from a given address in a single fetch or simultaneously, this could crash the serving executor or Node Manager. This is especially useful to reduce the load on the Node Manager when external shuffle is enabled. You can mitigate this issue by setting it to a lower value.",reliability,spark
4640,spark.maxRemoteBlockSizeFetchToMem,"The remote block will be fetched to disk when size of the block is above this threshold in bytes. This is to avoid a giant request that takes too much memory.  By default, this is only enabled for blocks > 2GB, as those cannot be fetched directly into memory, no matter what resources are available.  But it can be turned down to a much lower value (eg. 200m) to avoid using too much memory on smaller blocks as well. Note this configuration will affect both shuffle fetch and block manager remote block fetch. For users who enabled external shuffle service, this feature can only be used when external shuffle service is newer than Spark 2.2.",reliability,spark
4641,spark.shuffle.compress,Whether to compress map output files. Generally a good idea. Compression will use spark.io.compression.codec.,others,spark
4642,spark.shuffle.file.buffer,"Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise  specified. These buffers reduce the number of disk seeks and system calls made in creating  intermediate shuffle files.",performance,spark
4643,spark.shuffle.io.maxRetries,(Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is set to a non-zero value. This retry logic helps stabilize large shuffles in the face of long GC pauses or transient network connectivity issues.,reliability,spark
4644,spark.shuffle.io.numConnectionsPerPeer,"(Netty only) Connections between hosts are reused in order to reduce connection buildup for large clusters. For clusters with many hard disks and few hosts, this may result in insufficient concurrency to saturate all disks, and so users may consider increasing this value.",performance,spark
4645,spark.shuffle.io.preferDirectBufs,"(Netty only) Off-heap buffers are used to reduce garbage collection during shuffle and cache block transfer. For environments where off-heap memory is tightly limited, users may wish to turn this off to force all allocations from Netty to be on-heap.",performance,spark
4646,spark.shuffle.io.retryWait,"(Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying is 15 seconds by default, calculated as maxRetries * retryWait.",reliability,spark
4647,spark.shuffle.service.enabled,"Enables the external shuffle service. This service preserves the shuffle files written by executors so the executors can be safely removed. This must be enabled if spark.dynamicAllocation.enabled is ""true"". The external shuffle service must be set up in order to enable it. See dynamic allocation configuration and setup documentation for more information.",performance,spark
4648,spark.shuffle.service.port,Port on which the external shuffle service will run.,environment,spark
4649,spark.shuffle.service.index.cache.size,Cache entries limited to the specified memory footprint in bytes.,performance,spark
4650,spark.shuffle.maxChunksBeingTransferred,"The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see spark.shuffle.io.maxRetries and spark.shuffle.io.retryWait), if those limits are reached the task will fail with fetch failure.",reliability,spark
4651,spark.shuffle.sort.bypassMergeThreshold,"(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions.",performance,spark
4652,spark.shuffle.spill.compress,Whether to compress data spilled during shuffles. Compression will use spark.io.compression.codec.,performance,spark
4653,spark.shuffle.accurateBlockThreshold,Threshold in bytes above which the size of shuffle blocks in HighlyCompressedMapStatus is  accurately recorded. This helps to prevent OOM by avoiding underestimating shuffle  block size when fetch shuffle blocks.,reliability,spark
4654,spark.shuffle.registration.timeout,Timeout in milliseconds for registration to the external shuffle service.,reliability,spark
4655,spark.shuffle.registration.maxAttempts,"When we fail to register to the external shuffle service, we will retry for maxAttempts times.",reliability,spark
4656,spark.eventLog.logBlockUpdates.enabled,"Whether to log events for every block update, if spark.eventLog.enabled is true. *Warning*: This will increase the size of the event log considerably.",debuggability,spark
4657,spark.eventLog.longForm.enabled,"If true, use the long form of call sites in the event log. Otherwise use the short form.",debuggability,spark
4658,spark.eventLog.compress,"Whether to compress logged events, if spark.eventLog.enabled is true. Compression will use spark.io.compression.codec.",others,spark
4659,spark.eventLog.dir,"Base directory in which Spark events are logged, if spark.eventLog.enabled is true. Within this base directory, Spark creates a sub-directory for each application, and logs the events specific to the application in this directory. Users may want to set this to a unified location like an HDFS directory so history files can be read by the history server.",debuggability,spark
4660,spark.eventLog.enabled,"Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.",debuggability,spark
4661,spark.eventLog.overwrite,Whether to overwrite any existing files.,security,spark
4662,spark.eventLog.buffer.kb,"Buffer size to use when writing to output streams, in KiB unless otherwise specified.",performance,spark
4663,spark.ui.dagGraph.retainedRootRDDs,How many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.,performance,spark
4664,spark.ui.enabled,Whether to run the web UI for the Spark application.,others,spark
4665,spark.ui.killEnabled,Allows jobs and stages to be killed from the web UI.,others,spark
4666,spark.ui.liveUpdate.period,"How often to update live entities. -1 means ""never update"" when replaying applications, meaning only the last write will happen. For live applications, this avoids a few operations that we can live without when rapidly processing incoming task events.",reliability,spark
4667,spark.ui.liveUpdate.minFlushPeriod,Minimum time elapsed before stale UI data is flushed. This avoids UI staleness when incoming task events are not fired frequently.,reliability,spark
4668,spark.ui.port,"Port for your application's dashboard, which shows memory and workload data.",environment,spark
4669,spark.ui.retainedJobs,"How many jobs the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",performance,spark
4670,spark.ui.retainedStages,"How many stages the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",others,spark
4671,spark.ui.retainedTasks,"How many tasks the Spark UI and status APIs remember before garbage collecting. This is a target maximum, and fewer elements may be retained in some circumstances.",others,spark
4672,spark.ui.reverseProxy,"Enable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts. Use it with caution, as worker and application UI will not be accessible directly, you will only be able to access them through spark master/proxy public URL. This setting affects all the workers and application UIs running in the cluster and must be set on all the workers, drivers and masters.",security,spark
4673,spark.ui.reverseProxyUrl,This is the URL where your proxy is running. This URL is for proxy which is running in front of Spark Master. This is useful when running proxy for authentication e.g. OAuth proxy. Make sure this is a complete URL including scheme (http/https) and port to reach your proxy.,security,spark
4674,spark.ui.showConsoleProgress,"Show the progress bar in the console. The progress bar shows the progress of stages that run for longer than 500ms. If multiple stages run at the same time, multiple progress bars will be displayed on the same line. Note: In shell environment, the default value of spark.ui.showConsoleProgress is true.",performance,spark
4675,spark.worker.ui.retainedExecutors,How many finished executors the Spark UI and status APIs remember before garbage collecting.,others,spark
4676,spark.worker.ui.retainedDrivers,How many finished drivers the Spark UI and status APIs remember before garbage collecting.,others,spark
4677,spark.sql.ui.retainedExecutions,How many finished executions the Spark UI and status APIs remember before garbage collecting.,others,spark
4678,spark.streaming.ui.retainedBatches,How many finished batches the Spark UI and status APIs remember before garbage collecting.,others,spark
4679,spark.ui.retainedDeadExecutors,How many dead executors the Spark UI and status APIs remember before garbage collecting.,others,spark
4680,spark.ui.filters,"Comma separated list of filter class names to apply to the Spark Web UI. The filter should be a standard  javax servlet Filter. Filter parameters can also be specified in the configuration, by setting config entries of the form spark.<class name of filter>.param.<param name>=<value> For example: spark.ui.filters=com.test.filter1 spark.com.test.filter1.param.name1=foo spark.com.test.filter1.param.name2=bar",security,spark
4681,spark.ui.requestHeaderSize,"The maximum allowed size for a HTTP request header, in bytes unless otherwise specified. This setting applies for the Spark History Server too.",reliability,spark
4682,spark.broadcast.compress,Whether to compress broadcast variables before sending them. Generally a good idea. Compression will use spark.io.compression.codec.,performance,spark
4683,spark.checkpoint.compress,Whether to compress RDD checkpoints. Generally a good idea. Compression will use spark.io.compression.codec.,performance,spark
4684,spark.io.compression.codec,"The codec used to compress internal data such as RDD partitions, event log, broadcast variables and shuffle outputs. By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify the codec, e.g. org.apache.spark.io.LZ4CompressionCodec, org.apache.spark.io.LZFCompressionCodec, org.apache.spark.io.SnappyCompressionCodec, and org.apache.spark.io.ZStdCompressionCodec.",performance,spark
4685,spark.io.compression.lz4.blockSize,"Block size in bytes used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.",performance,spark
4686,spark.io.compression.snappy.blockSize,"Block size in bytes used in Snappy compression, in the case when Snappy compression codec is used. Lowering this block size will also lower shuffle memory usage when Snappy is used.",performance,spark
4687,spark.io.compression.zstd.level,Compression level for Zstd compression codec. Increasing the compression level will result in better compression at the expense of more CPU and memory.,performance,spark
4688,spark.io.compression.zstd.bufferSize,"Buffer size in bytes used in Zstd compression, in the case when Zstd compression codec is used. Lowering this size will lower the shuffle memory usage when Zstd is used, but it might increase the compression cost because of excessive JNI call overhead.",performance,spark
4689,spark.kryo.classesToRegister,"If you use Kryo serialization, give a comma-separated list of custom class names to register with Kryo. See the tuning guide for more details.",others,spark
4690,spark.kryo.referenceTracking,"Whether to track references to the same object when serializing data with Kryo, which is necessary if your object graphs have loops and useful for efficiency if they contain multiple copies of the same object. Can be disabled to improve performance if you know this is not the case.",performance,spark
4691,spark.kryo.registrationRequired,"Whether to require registration with Kryo. If set to 'true', Kryo will throw an exception if an unregistered class is serialized. If set to false (the default), Kryo will write unregistered class names along with each object. Writing class names can cause significant performance overhead, so enabling this option can enforce strictly that a user has not omitted classes from registration.",performance,spark
4692,spark.kryo.registrator,"If you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This property is useful if you need to register your classes in a custom way, e.g. to specify a custom field serializer. Otherwise spark.kryo.classesToRegister is simpler. It should be set to classes that extend KryoRegistrator. See the tuning guide for more details.",others,spark
4693,spark.kryo.unsafe,Whether to use unsafe based Kryo serializer. Can be substantially faster by using Unsafe Based IO.,security,spark
4694,spark.kryoserializer.buffer.max,"Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified. This must be larger than any object you attempt to serialize and must be less than 2048m. Increase this if you get a ""buffer limit exceeded"" exception inside Kryo.",performance,spark
4695,spark.kryoserializer.buffer,"Initial size of Kryo's serialization buffer, in KiB unless otherwise specified.  Note that there will be one buffer per core on each worker. This buffer will grow up to spark.kryoserializer.buffer.max if needed.",performance,spark
4696,spark.rdd.compress,Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER in Java and Scala or StorageLevel.MEMORY_ONLY in Python). Can save substantial space at the cost of some extra CPU time. Compression will use spark.io.compression.codec.,performance,spark
4697,spark.serializer,"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form. The default of Java serialization works with any Serializable Java object but is quite slow, so we recommend using org.apache.spark.serializer.KryoSerializer and configuring Kryo serialization when speed is necessary. Can be any subclass of org.apache.spark.Serializer.",performance,spark
4698,spark.serializer.objectStreamReset,"When serializing using org.apache.spark.serializer.JavaSerializer, the serializer caches objects to prevent writing redundant data, however that stops garbage collection of those objects. By calling 'reset' you flush that info from the serializer, and allow old objects to be collected. To turn off this periodic reset set it to -1. By default it will reset the serializer every 100 objects.",performance,spark
4699,spark.memory.fraction,"Fraction of (heap space - 300MB) used for execution and storage. The lower this is, the more frequently spills and cached data eviction occur. The purpose of this config is to set aside memory for internal metadata, user data structures, and imprecise size estimation in the case of sparse, unusually large records. Leaving this at the default value is recommended. For more detail, including important information about correctly tuning JVM garbage collection when increasing this value, see this description.",performance,spark
4700,spark.memory.storageFraction,"Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended. For more detail, see this description.",performance,spark
4701,spark.memory.offHeap.enabled,"If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory  use is enabled, then spark.memory.offHeap.size must be positive.",performance,spark
4702,spark.memory.offHeap.size,"The absolute amount of memory in bytes which can be used for off-heap allocation. This setting has no impact on heap memory usage, so if your executors' total memory consumption  must fit within some hard limit then be sure to shrink your JVM heap size accordingly. This must be set to a positive value when spark.memory.offHeap.enabled=true.",performance,spark
4703,spark.memory.useLegacyMode,"Whether to enable the legacy memory management mode used in Spark 1.5 and before. The legacy mode rigidly partitions the heap space into fixed-size regions, potentially leading to excessive spilling if the application was not tuned. The following deprecated memory fraction configurations are not read unless this is enabled: spark.shuffle.memoryFraction spark.storage.memoryFraction spark.storage.unrollFraction",performance,spark
4704,spark.shuffle.memoryFraction,"(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for aggregation and cogroups during shuffles. At any given time, the collective size of all in-memory maps used for shuffles is bounded by this limit, beyond which the contents will begin to spill to disk. If spills are often, consider increasing this value at the expense of spark.storage.memoryFraction.",performance,spark
4705,spark.storage.memoryFraction,"(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for Spark's memory cache. This should not be larger than the ""old"" generation of objects in the JVM, which by default is given 0.6 of the heap, but you can increase it if you configure your own old generation size.",performance,spark
4706,spark.storage.unrollFraction,(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory. This is dynamically allocated by dropping existing blocks when there is not enough free storage space to unroll the new block in its entirety.,reliability,spark
4707,spark.storage.replication.proactive,Enables proactive block replication for RDD blocks. Cached RDD block replicas lost due to executor failures are replenished if there are any existing available replicas. This tries to get the replication level of the block to the initial number.,reliability,spark
4708,spark.cleaner.periodicGC.interval,"Controls how often to trigger a garbage collection. This context cleaner triggers cleanups only when weak references are garbage collected. In long-running applications with large driver JVMs, where there is little memory pressure on the driver, this may happen very occasionally or not at all. Not cleaning at all may lead to executors running out of disk space after a while.",reliability,spark
4709,spark.cleaner.referenceTracking,Enables or disables context cleaning.,reliability,spark
4710,spark.cleaner.referenceTracking.blocking,"Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by spark.cleaner.referenceTracking.blocking.shuffle Spark property).",reliability,spark
4711,spark.cleaner.referenceTracking.blocking.shuffle,Controls whether the cleaning thread should block on shuffle cleanup tasks.,reliability,spark
4712,spark.cleaner.referenceTracking.cleanCheckpoints,Controls whether to clean checkpoint files if the reference is out of scope.,reliability,spark
4713,spark.broadcast.blockSize,"Size of each piece of a block for TorrentBroadcastFactory, in KiB unless otherwise  specified. Too large a value decreases parallelism during broadcast (makes it slower); however,  if it is too small, BlockManager might take a performance hit.",performance,spark
4714,spark.broadcast.checksum,"Whether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can help detect corrupted blocks, at the cost of computing and sending a little more data. It's possible to disable it if the network has other mechanisms to guarantee data won't be corrupted during broadcast.",reliability,spark
4715,spark.executor.cores,"The number of cores to use on each executor. In standalone and Mesos coarse-grained modes, for more detail, see this description.",performance,spark
4716,spark.default.parallelism,"Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.",performance,spark
4717,spark.executor.heartbeatInterval,Interval between each executor's heartbeats to the driver.  Heartbeats let the driver know that the executor is still alive and update it with metrics for in-progress tasks. spark.executor.heartbeatInterval should be significantly less than spark.network.timeout,reliability,spark
4718,spark.files.fetchTimeout,Communication timeout to use when fetching files added through SparkContext.addFile() from the driver.,reliability,spark
4719,spark.files.useFetchCache,"If set to true (default), file fetching will use a local cache that is shared by executors that belong to the same application, which can improve task launching performance when running many executors on the same host. If set to false, these caching optimizations will be disabled and all executors will fetch their own copies of files. This optimization may be disabled in order to use Spark local directories that reside on NFS filesystems (see SPARK-6313 for more details).",performance,spark
4720,spark.files.overwrite,Whether to overwrite files added through SparkContext.addFile() when the target file exists and its contents do not match those of the source.,reliability,spark
4721,spark.files.maxPartitionBytes,The maximum number of bytes to pack into a single partition when reading files.,performance,spark
4722,spark.files.openCostInBytes,"The estimated cost to open a file, measured by the number of bytes could be scanned at the same time. This is used when putting multiple files into a partition. It is better to overestimate, then the partitions with small files will be faster than partitions with bigger files.",performance,spark
4723,spark.hadoop.cloneConf,"If set to true, clones a new Hadoop Configuration object for each task.  This option should be enabled to work around Configuration thread-safety issues (see SPARK-2546 for more details). This is disabled by default in order to avoid unexpected performance regressions for jobs that are not affected by these issues.",performance,spark
4724,spark.hadoop.validateOutputSpecs,"If set to true, validates the output specification (e.g. checking if the output directory already exists) used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing output directories. We recommend that users do not disable this except if trying to achieve compatibility with previous versions of Spark. Simply use Hadoop's FileSystem API to delete output directories by hand. This setting is ignored for jobs generated through Spark Streaming's StreamingContext, since data may need to be rewritten to pre-existing output directories during checkpoint recovery.",reliability,spark
4725,spark.storage.memoryMapThreshold,"Size in bytes of a block above which Spark memory maps when reading a block from disk. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the operating system.",performance,spark
4726,spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,"The file output committer algorithm version, valid algorithm version number: 1 or 2. Version 2 may have better performance, but version 1 may handle failures better in certain situations, as per MAPREDUCE-4815.",environment,spark
4727,spark.rpc.message.maxSize,"Maximum message size (in MB) to allow in ""control plane"" communication; generally only applies to map output size information sent between executors and the driver. Increase this if you are running jobs with many thousands of map and reduce tasks and see messages about the RPC message size.",performance,spark
4728,spark.blockManager.port,Port for all block managers to listen on. These exist on both the driver and the executors.,environment,spark
4729,spark.driver.blockManager.port,"Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors.",environment,spark
4730,spark.driver.bindAddress,"Hostname or IP address where to bind listening sockets. This config overrides the SPARK_LOCAL_IP environment variable (see below). It also allows a different address from the local one to be advertised to executors or external systems. This is useful, for example, when running containers with bridged networking. For this to properly work, the different ports used by the driver (RPC, block manager and UI) need to be forwarded from the container's host.",environment,spark
4731,spark.driver.host,Hostname or IP address for the driver. This is used for communicating with the executors and the standalone Master.,environment,spark
4732,spark.driver.port,Port for the driver to listen on. This is used for communicating with the executors and the standalone Master.,environment,spark
4733,spark.network.timeout,"Default timeout for all network interactions. This config will be used in place of spark.core.connection.ack.wait.timeout, spark.storage.blockManagerSlaveTimeoutMs, spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or spark.rpc.lookupTimeout if they are not configured.",reliability,spark
4734,spark.port.maxRetries,"Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.",reliability,spark
4735,spark.rpc.numRetries,Number of times to retry before an RPC task gives up. An RPC task will run at most times of this number.,reliability,spark
4736,spark.rpc.retry.wait,Duration for an RPC ask operation to wait before retrying.,reliability,spark
4737,spark.rpc.askTimeout,Duration for an RPC ask operation to wait before timing out.,reliability,spark
4738,spark.rpc.lookupTimeout,Duration for an RPC remote endpoint lookup operation to wait before timing out.,reliability,spark
4739,spark.core.connection.ack.wait.timeout,"How long for the connection to wait for ack to occur before timing out and giving up. To avoid unwilling timeout caused by long pause like GC, you can set larger value.",reliability,spark
4740,spark.cores.max,"When running on a standalone deploy cluster or a Mesos cluster in ""coarse-grained"" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos.",reliability,spark
4741,spark.locality.wait,"How long to wait to launch a data-local task before giving up and launching it on a less-local node. The same wait will be used to step through multiple locality levels (process-local, node-local, rack-local and then any). It is also possible to customize the waiting time for each level by setting spark.locality.wait.node, etc. You should increase this setting if your tasks are long and see poor locality, but the default usually works well.",reliability,spark
4742,spark.locality.wait.node,"Customize the locality wait for node locality. For example, you can set this to 0 to skip node locality and search immediately for rack locality (if your cluster has rack information).",performance,spark
4743,spark.locality.wait.process,Customize the locality wait for process locality. This affects tasks that attempt to access cached data in a particular executor process.,performance,spark
4744,spark.locality.wait.rack,Customize the locality wait for rack locality.,performance,spark
4745,spark.scheduler.maxRegisteredResourcesWaitingTime,Maximum amount of time to wait for resources to register before scheduling begins.,reliability,spark
4746,spark.scheduler.minRegisteredResourcesRatio,"The minimum ratio of registered resources (registered resources / total expected resources) (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode and Mesos coarse-grained mode ['spark.cores.max' value is total expected resources for Mesos coarse-grained mode] ) to wait for before scheduling begins. Specified as a double between 0.0 and 1.0. Regardless of whether the minimum ratio of resources has been reached, the maximum amount of time it will wait before scheduling begins is controlled by config spark.scheduler.maxRegisteredResourcesWaitingTime.",others,spark
4747,spark.scheduler.mode,The scheduling mode between jobs submitted to the same SparkContext. Can be set to FAIR to use fair sharing instead of queueing jobs one after another. Useful for multi-user services.,performance,spark
4748,spark.scheduler.revive.interval,The interval length for the scheduler to revive the worker resource offers to run tasks.,reliability,spark
4749,spark.scheduler.listenerbus.eventqueue.capacity,"Capacity for event queue in Spark listener bus, must be greater than 0. Consider increasing value (e.g. 20000) if listener events are dropped. Increasing this value may result in the driver using more memory.",performance,spark
4750,spark.scheduler.blacklist.unschedulableTaskSetTimeout,The timeout in seconds to wait to acquire a new executor and schedule a task before aborting a TaskSet which is unschedulable because of being completely blacklisted.,reliability,spark
4751,spark.blacklist.enabled,"If set to ""true"", prevent Spark from scheduling tasks on executors that have been blacklisted due to too many task failures. The blacklisting algorithm can be further controlled by the other ""spark.blacklist"" configuration options.",reliability,spark
4752,spark.blacklist.timeout,"(Experimental) How long a node or executor is blacklisted for the entire application, before it is unconditionally removed from the blacklist to attempt running new tasks.",reliability,spark
4753,spark.blacklist.task.maxTaskAttemptsPerExecutor,"(Experimental) For a given task, how many times it can be retried on one executor before the executor is blacklisted for that task.",reliability,spark
4754,spark.blacklist.task.maxTaskAttemptsPerNode,"(Experimental) For a given task, how many times it can be retried on one node, before the entire node is blacklisted for that task.",reliability,spark
4755,spark.blacklist.stage.maxFailedTasksPerExecutor,"(Experimental) How many different tasks must fail on one executor, within one stage, before the executor is blacklisted for that stage.",reliability,spark
4756,spark.blacklist.stage.maxFailedExecutorsPerNode,"(Experimental) How many different executors are marked as blacklisted for a given stage, before the entire node is marked as failed for the stage.",reliability,spark
4757,spark.blacklist.application.maxFailedTasksPerExecutor,"(Experimental) How many different tasks must fail on one executor, in successful task sets, before the executor is blacklisted for the entire application.  Blacklisted executors will be automatically added back to the pool of available resources after the timeout specified by spark.blacklist.timeout.  Note that with dynamic allocation, though, the executors may get marked as idle and be reclaimed by the cluster manager.",reliability,spark
4758,spark.blacklist.application.maxFailedExecutorsPerNode,"(Experimental) How many different executors must be blacklisted for the entire application, before the node is blacklisted for the entire application.  Blacklisted nodes will be automatically added back to the pool of available resources after the timeout specified by spark.blacklist.timeout.  Note that with dynamic allocation, though, the executors on the node may get marked as idle and be reclaimed by the cluster manager.",reliability,spark
4759,spark.blacklist.killBlacklistedExecutors,"(Experimental) If set to ""true"", allow Spark to automatically kill the executors  when they are blacklisted on fetch failure or blacklisted for the entire application,  as controlled by spark.blacklist.application.*. Note that, when an entire node is added  to the blacklist, all of the executors on that node will be killed.",reliability,spark
4760,spark.blacklist.application.fetchFailure.enabled,"(Experimental) If set to ""true"", Spark will blacklist the executor immediately when a fetch failure happens. If external shuffle service is enabled, then the whole node will be blacklisted.",reliability,spark
4761,spark.speculation,"If set to ""true"", performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched.",performance,spark
4762,spark.speculation.interval,How often Spark will check for tasks to speculate.,reliability,spark
4763,spark.speculation.multiplier,How many times slower a task is than the median to be considered for speculation.,performance,spark
4764,spark.speculation.quantile,Fraction of tasks which must be complete before speculation is enabled for a particular stage.,reliability,spark
4765,spark.task.cpus,Number of cores to allocate for each task.,performance,spark
4766,spark.task.maxFailures,Number of failures of any particular task before giving up on the job. The total number of failures spread across different tasks will not cause the job to fail; a particular task has to fail this number of attempts. Should be greater than or equal to 1. Number of allowed retries = this value - 1.,reliability,spark
4767,spark.task.reaper.enabled,"Enables monitoring of killed / interrupted tasks. When set to true, any task which is killed will be monitored by the executor until that task actually finishes executing. See the other spark.task.reaper.* configurations for details on how to control the exact behavior of this monitoring. When set to false (the default), task killing will use an older code path which lacks such monitoring.",debuggability,spark
4768,spark.task.reaper.pollingInterval,"When spark.task.reaper.enabled = true, this setting controls the frequency at which executors will poll the status of killed tasks. If a killed task is still running when polled then a warning will be logged and, by default, a thread-dump of the task will be logged (this thread dump can be disabled via the spark.task.reaper.threadDump setting, which is documented below).",debuggability,spark
4769,spark.task.reaper.threadDump,"When spark.task.reaper.enabled = true, this setting controls whether task thread dumps are logged during periodic polling of killed tasks. Set this to false to disable collection of thread dumps.",debuggability,spark
4770,spark.task.reaper.killTimeout,"When spark.task.reaper.enabled = true, this setting specifies a timeout after which the executor JVM will kill itself if a killed task has not stopped running. The default value, -1, disables this mechanism and prevents the executor from self-destructing. The purpose of this setting is to act as a safety-net to prevent runaway noncancellable tasks from rendering an executor unusable.",reliability,spark
4771,spark.stage.maxConsecutiveAttempts,Number of consecutive stage attempts allowed before a stage is aborted.,reliability,spark
4772,spark.dynamicAllocation.enabled,"Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description here. This requires spark.shuffle.service.enabled to be set. The following configurations are also relevant: spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorAllocationRatio",performance,spark
4773,spark.dynamicAllocation.executorIdleTimeout,"If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this description.",reliability,spark
4774,spark.dynamicAllocation.cachedExecutorIdleTimeout,"If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this description.",reliability,spark
4775,spark.dynamicAllocation.initialExecutors,"Initial number of executors to run if dynamic allocation is enabled. If `--num-executors` (or `spark.executor.instances`) is set and larger than this value, it will be used as the initial number of executors.",performance,spark
4776,spark.dynamicAllocation.maxExecutors,Upper bound for the number of executors if dynamic allocation is enabled.,performance,spark
4777,spark.dynamicAllocation.minExecutors,Lower bound for the number of executors if dynamic allocation is enabled.,performance,spark
4778,spark.dynamicAllocation.executorAllocationRatio,"By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors settings",performance,spark
4779,spark.dynamicAllocation.schedulerBacklogTimeout,"If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description.",reliability,spark
4780,spark.dynamicAllocation.sustainedSchedulerBacklogTimeout,"Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests. For more detail, see this description.",reliability,spark
4781,spark.streaming.backpressure.enabled,"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values spark.streaming.receiver.maxRate and spark.streaming.kafka.maxRatePerPartition if they are set (see below).",performance,spark
4782,spark.streaming.backpressure.initialRate,This is the initial maximum receiving rate at which each receiver will receive data for the first batch when the backpressure mechanism is enabled.,performance,spark
4783,spark.streaming.blockInterval,Interval at which data received by Spark Streaming receivers is chunked into blocks of data before storing them in Spark. Minimum recommended - 50 ms. See the performance tuning section in the Spark Streaming programing guide for more details.,reliability,spark
4784,spark.streaming.receiver.maxRate,"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details.",reliability,spark
4785,spark.streaming.receiver.writeAheadLog.enable,Enable write-ahead logs for receivers. All the input data received through receivers will be saved to write-ahead logs that will allow it to be recovered after driver failures. See the deployment guide in the Spark Streaming programing guide for more details.,debuggability,spark
4786,spark.streaming.unpersist,Force RDDs generated and persisted by Spark Streaming to be automatically unpersisted from Spark's memory. The raw input data received by Spark Streaming is also automatically cleared. Setting this to false will allow the raw data and persisted RDDs to be accessible outside the streaming application as they will not be cleared automatically. But it comes at the cost of higher memory usage in Spark.,others,spark
4787,spark.streaming.stopGracefullyOnShutdown,"If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.",reliability,spark
4788,spark.streaming.kafka.maxRatePerPartition,Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API. See the Kafka Integration guide for more details.,reliability,spark
4789,spark.streaming.kafka.minRatePerPartition,Minimum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API.,reliability,spark
4790,spark.streaming.kafka.maxRetries,Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the new Kafka direct stream API.,reliability,spark
4791,spark.streaming.ui.retainedBatches,How many batches the Spark Streaming UI and status APIs remember before garbage collecting.,performance,spark
4792,spark.streaming.driver.writeAheadLog.closeFileAfterWrite,Whether to close the file after writing a write-ahead log record on the driver. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the metadata WAL on the driver.,debuggability,spark
4793,spark.streaming.receiver.writeAheadLog.closeFileAfterWrite,Whether to close the file after writing a write-ahead log record on the receivers. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the data WAL on the receivers.,debuggability,spark
4794,spark.r.numRBackendThreads,Number of threads used by RBackend to handle RPC calls from SparkR package.,performance,spark
4795,spark.r.command,Executable for executing R scripts in cluster modes for both driver and workers.,others,spark
4796,spark.r.driver.command,Executable for executing R scripts in client modes for driver. Ignored in cluster modes.,others,spark
4797,spark.r.shell.command,"Executable for executing sparkR shell in client modes for driver. Ignored in cluster modes. It is the same as environment variable SPARKR_DRIVER_R, but take precedence over it. spark.r.shell.command is used for sparkR shell while spark.r.driver.command is used for running R script.",others,spark
4798,spark.r.backendConnectionTimeout,Connection timeout set by R process on its connection to RBackend in seconds.,reliability,spark
4799,spark.r.heartBeatInterval,Interval for heartbeats sent from SparkR backend to R process to prevent connection timeout.,reliability,spark
4800,spark.graphx.pregel.checkpointInterval,Checkpoint interval for graph and message in Pregel. It used to avoid stackOverflowError due to long lineage chains after lots of iterations. The checkpoint is disabled by default.,reliability,spark
4801,spark.deploy.recoveryMode,The recovery mode setting to recover submitted Spark jobs with cluster mode when it failed and relaunches. This is only applicable for cluster mode when running with Standalone or Mesos.,reliability,spark
4802,spark.deploy.zookeeper.url,"When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper URL to connect to.",environment,spark
4803,spark.deploy.zookeeper.dir,"When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper directory to store recovery state.",environment,spark
4804,JAVA_HOME,Location where Java is installed (if it's not on your default PATH).,environment,spark
4805,PYSPARK_PYTHON,"Python binary executable to use for PySpark in both driver and workers (default is python2.7 if available, otherwise python).",environment,spark
4806,PYSPARK_DRIVER_PYTHON,Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON). Property spark.pyspark.driver.python take precedence if it is set,environment,spark
4807,SPARKR_DRIVER_R,R binary executable to use for SparkR shell (default is R).,environment,spark
4808,SPARK_LOCAL_IP,IP address of the machine to bind to.,environment,spark
4809,SPARK_PUBLIC_DNS,Hostname your Spark program will advertise to other machines.,environment,spark
4810,workers,"Number of main Squid processes or ""workers"" to fork and maintain.",performance,squid
4811,cpu_affinity_map,Sets 1:1 mapping between Squid processes and CPU cores.,performance,squid
4812,shared_memory_locking,"Whether to ensure that all required shared memory is available by ""locking"" that shared memory into RAM when Squid starts. The alternative is faster startup time followed by slightly slower  performance and, if not enough RAM is actually available during runtime, mysterious crashes.",performance,squid
4813,hopeless_kid_revival_delay,"Normally, when a kid process dies, Squid immediately restarts the kid. A kid experiencing frequent deaths is marked as ""hopeless"" for the duration specified by this directive. Hopeless kids are not automatically restarted.",reliability,squid
4814,auth_param,This is used to define parameters for the various authentication schemes supported by Squid.,security,squid
4815,authenticate_cache_garbage_interval,The time period between garbage collection across the username cache.This is a trade-off between memory utilization (long intervals - say 2 days) and CPU (short intervals - say 1 minute). ,reliability,squid
4816,authenticate_ttl,"The time a user & their credentials stay in the logged in user cache since their last request. When the garbage interval passes, all user credentials that have passed their TTL are removed from memory.",security,squid
4817,authenticate_ip_ttl,"If you use proxy authentication and the 'max_user_ip' ACL, this directive controls how long Squid remembers the IP addresses associated with each user.",reliability,squid
4818,external_acl_type,This option defines external acl classes using a helper program to look up the status,security,squid
4819,acl,Defining an Access List,security,squid
4820,proxy_protocol_access,Determine which client proxies can be trusted to provide correct information regarding real client IP address using PROXY protocol.,security,squid
4821,follow_x_forwarded_for,Determine which client proxies can be trusted to provide correct information regarding real client IP address.,security,squid
4822,acl_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in acl matching.,security,squid
4823,delay_pool_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in delay pools.,security,squid
4824,log_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address in the access log.,security,squid
4825,tproxy_uses_indirect_client,Controls whether the indirect client address (see follow_x_forwarded_for) is used instead of the direct client address when spoofing the outgoing client.,security,squid
4826,spoof_client_ip,Control client IP address spoofing of TPROXY traffic based on defined access lists.,security,squid
4827,http_access,Allowing or Denying access based on defined access lists,security,squid
4828,adapted_http_access,Allowing or Denying access based on defined access lists,security,squid
4829,http_reply_access,Allow replies to client requests. This is complementary to http_access.,security,squid
4830,icp_access,Allowing or Denying access to the ICP port based on defined access lists,security,squid
4831,htcp_access,Allowing or Denying access to the HTCP port based on defined access lists,security,squid
4832,htcp_clr_access,Allowing or Denying access to purge content using HTCP based on defined access lists.,security,squid
4833,miss_access,Determines whether network access is permitted when satisfying a request.,security,squid
4834,ident_lookup_access,"A list of ACL elements which, if matched, cause an ident (RFC 931) lookup to be performed for this request. ",security,squid
4835,reply_body_max_size,"This option specifies the maximum size of a reply body. It can be used to prevent users from downloading very large files, such as MP3's and movies.",reliability,squid
4836,on_unsupported_protocol,Determines Squid behavior when encountering strange requests at the beginning of an accepted TCP connection or the beginning of a bumped CONNECT tunnel.,reliability,squid
4837,http_port,The socket addresses where Squid will listen for HTTP client requests.,environment,squid
4838,https_port,The socket address where Squid will listen for client requests made over TLS or SSL connections.,environment,squid
4839,ftp_port,Enables Native FTP proxy by specifying the socket address where Squid listens for FTP client requests.,environment,squid
4840,tcp_outgoing_tos,"Allows you to select a TOS/Diffserv value for packets outgoing on the server side, based on an ACL.",others,squid
4841,clientside_tos,"Allows you to select a TOS/DSCP value for packets being transmitted on the client-side, based on an ACL.",others,squid
4842,tcp_outgoing_mark,"Allows you to apply a Netfilter mark value to outgoing packets on the server side, based on an ACL.",others,squid
4843,clientside_mark,"Allows you to apply a Netfilter mark value to packets being transmitted on the client-side, based on an ACL.",others,squid
4844,qos_flows,"Allows you to select a TOS/DSCP value to mark outgoing connections to the client, based on where the reply was sourced.",others,squid
4845,tcp_outgoing_address,Allows you to map requests to different outgoing IP addresses based on the username or source address of the user making the request.,environment,squid
4846,host_verify_strict,"Regardless of this option setting, when dealing with intercepted traffic, Squid always verifies that the destination IP address matches the Host header domain or IP (called 'authority form URL').",security,squid
4847,client_dst_passthru,With NAT or TPROXY intercepted traffic Squid may pass the request directly to the original client destination IP or seek a faster source using the HTTP Host header.,performance,squid
4848,ssl_unclean_shutdown,Some browsers (especially MSIE) bugs out on SSL shutdown messages.,debuggability,squid
4849,ssl_engine,The OpenSSL engine to use. You will need to set this if you would like to use hardware SSL acceleration for example.,others,squid
4850,sslproxy_session_ttl,Sets the timeout value for SSL sessions,reliability,squid
4851,sslproxy_session_cache_size,Sets the cache size to use for ssl session,performance,squid
4852,sslproxy_foreign_intermediate_certs,"Many origin servers fail to send their full server certificate chain for verification, assuming the client already has or can easily locate any missing intermediate certificates.",security,squid
4853,sslproxy_cert_sign_hash,Sets the hashing algorithm to use when signing generated certificates. Valid algorithm names depend on the OpenSSL library used.,security,squid
4854,ssl_bump,"This option is consulted when a CONNECT request is received on an http_port (or a new connection is intercepted at an https_port), provided that port was configured with an ssl-bump flag.",environment,squid
4855,sslproxy_cert_error,Use this ACL to bypass server certificate validation errors.,security,squid
4856,sslpassword_program,Specify a program used for entering SSL key passphrases when using encrypted SSL certificate keys.,security,squid
4857,sslcrtd_program,Specify the location and options of the executable for certificate generator.,environment,squid
4858,sslcrtd_children,Specifies the maximum number of certificate generation processes that Squid may spawn (numberofchildren) and several related options.,performance,squid
4859,sslcrtvalidator_program,Specify the location and options of the executable for ssl_crt_validator process.,environment,squid
4860,sslcrtvalidator_children,Specifies the maximum number of certificate validation processes that Squid may spawn (numberofchildren) and several related options.,reliability,squid
4861,cache_peer,To specify other caches in a hierarchy,performance,squid
4862,cache_peer_access,Restricts usage of cache_peer proxies.,security,squid
4863,neighbor_type_domain,Modify the cache_peer neighbor type when passing requests about specific domains to the peer.,performance,squid
4864,dead_peer_timeout,"This controls how long Squid waits to declare a peer cache as ""dead.""",reliability,squid
4865,forward_max_tries,Limits the number of attempts to forward the request.,reliability,squid
4866,cache_mem,cache_mem' specifies the ideal amount of memory to be used,performance,squid
4867,maximum_object_size_in_memory,Objects greater than this size will not be attempted to kept in the memory cache. This should be set high enough to keep objects accessed frequently in memory to improve performance whilst low enough to keep larger objects from hoarding cache_mem.,performance,squid
4868,memory_cache_shared,Controls whether the memory cache is shared among SMP workers.,performance,squid
4869,memory_cache_mode,Controls which objects to keep in the memory cache (cache_mem),performance,squid
4870,memory_replacement_policy,The memory replacement policy parameter determines which objects are purged from memory when memory space is needed.,reliability,squid
4871,cache_replacement_policy,The cache replacement policy parameter determines which objects are evicted (replaced) when disk space is needed.,performance,squid
4872,minimum_object_size,Objects smaller than this size will NOT be saved on disk.,performance,squid
4873,maximum_object_size,Set the default value for max-size parameter on any cache_dir.,performance,squid
4874,store_dir_select_algorithm,How Squid selects which cache_dir to use when the response object will fit into more than one.,performance,squid
4875,max_open_disk_fds,To avoid having disk as the I/O bottleneck Squid can optionally bypass the on-disk cache if more than this amount of disk file descriptors are open.,performance,squid
4876,cache_swap_low,The low-water mark for AUFS/UFS/diskd cache object eviction by the cache_replacement_policy algorithm.,performance,squid
4877,cache_swap_high,The high-water mark for AUFS/UFS/diskd cache object eviction by the cache_replacement_policy algorithm.,performance,squid
4878,logformat,Defines an access log format.,debuggability,squid
4879,access_log,Configures whether and how Squid logs HTTP and ICP transactions.,debuggability,squid
4880,icap_log,"ICAP log files record ICAP transaction summaries, one line per transaction.",debuggability,squid
4881,logfile_daemon,"Specify the path to the logfile-writing daemon. This daemon is used to write the access and store logs, if configured.",debuggability,squid
4882,stats_collection,This options allows you to control which requests gets accounted in performance counters.,others,squid
4883,cache_store_log,"Logs the activities of the storage manager.  Shows which objects are ejected from the cache, and which objects are saved and for how long.",debuggability,squid
4884,cache_swap_state,"Location for the cache ""swap.state"" file.",environment,squid
4885,logfile_rotate,Specifies the default number of logfile rotations to make when you type 'squid -k rotate'.,reliability,squid
4886,mime_table,Path to Squid's icon configuration file.,environment,squid
4887,log_mime_hdrs,The Cache can record both the request and the response MIME headers for each HTTP transaction.  The headers are encoded safely and will appear as two bracketed fields at the end of the access log (for either the native or httpd-emulated log formats).  To enable this logging set log_mime_hdrs to 'on'.,debuggability,squid
4888,pid_filename,A filename to write the process-id to.,environment,squid
4889,client_netmask,A netmask for client addresses in logfiles and cachemgr output. Change this to protect the privacy of your cache clients.,security,squid
4890,strip_query_terms,"By default, Squid strips query terms from requested URLs before logging.  This protects your user's privacy and reduces log size.",security,squid
4891,buffered_logs,Whether to write/send access_log records ASAP or accumulate them and then write/send them in larger chunks. Buffering may improve performance because it decreases the number of I/Os.,performance,squid
4892,netdb_filename,Where Squid stores it's netdb journal. When enabled this journal preserves netdb state between restarts.,environment,squid
4893,cache_log,Squid administrative logging file. This is where general information about Squid behavior goes.,debuggability,squid
4894,debug_options,"Logging options are set as section,level where each source file is assigned a unique section.",debuggability,squid
4895,coredump_dir,"By default Squid leaves core files in the directory from where it was started. If you set 'coredump_dir' to a directory that exists, Squid will chdir() to that directory at startup and coredump files will be left there.",environment,squid
4896,ftp_user,"If you want the anonymous login password to be more informative (and enable the use of picky FTP servers), set this to something reasonable for your domain, like wwwuser@somewhere.net",security,squid
4897,diskd_program,Specify the location of the diskd executable.,environment,squid
4898,unlinkd_program,Specify the location of the executable for file deletion process.,environment,squid
4899,pinger_program,Specify the location of the executable for the pinger process.,environment,squid
4900,pinger_enable,Control whether the pinger is active at run-time.,others,squid
4901,url_rewrite_program,Specify the location of the executable URL rewriter to use.,environment,squid
4902,url_rewrite_children,Specifies the maximum number of redirector processes that Squid may spawn (numberofchildren) and several related options.,reliability,squid
4903,url_rewrite_host_header,"If defined, this access list specifies which requests are sent to the redirector processes.",security,squid
4904,url_rewrite_bypass,"When this is 'on', a request will not go through the redirector if all the helpers are busy. If this is 'off' and the redirector queue grows too large, the action is prescribed by the on-persistent-overload option.",others,squid
4905,url_rewrite_extras,Specifies a string to be append to request line format for the rewriter helper.,others,squid
4906,url_rewrite_timeout,Squid times active requests to redirector.,reliability,squid
4907,store_id_program,Specify the location of the executable StoreID helper to use.,environment,squid
4908,store_id_extras,Specifies a string to be append to request line format for the StoreId helper.,others,squid
4909,store_id_access,"If defined, this access list specifies which requests are sent to the StoreID processes.",security,squid
4910,store_id_bypass,"When this is 'on', a request will not go through the helper if all helpers are busy. If this is 'off' and the helper queue grows too large, the action is prescribed by the on-persistent-overload option.",others,squid
4911,cache,Requests denied by this directive will not be served from the cache and their responses will not be stored in the cache.,performance,squid
4912,send_hit,"Responses denied by this directive will not be served from the cache (but may still be cached, see store_miss).",performance,squid
4913,store_miss,"Responses denied by this directive will not be cached (but may still be served from the cache, see send_hit).",performance,squid
4914,max_stale,This option puts an upper limit on how stale content Squid will serve from the cache if cache validation fails.,performance,squid
4915,read_ahead_gap,The amount of data the cache will buffer ahead of what has been sent to the client when retrieving an object from another server.,performance,squid
4916,negative_ttl,Set the Default Time-to-Live (TTL) for failed requests.,reliability,squid
4917,positive_dns_ttl,Upper limit on how long Squid will cache positive DNS responses.,reliability,squid
4918,negative_dns_ttl,Time-to-Live (TTL) for negative caching of failed DNS lookups.,reliability,squid
4919,range_offset_limit,Sets an upper limit on how far (number of bytes) into the file  a Range request may be to cause Squid to prefetch the whole file.,performance,squid
4920,minimum_expiry_time,The minimum caching time according to (Expires - Date) headers Squid honors if the object can't be revalidated.,reliability,squid
4921,store_avg_object_size,"Average object size, used to estimate number of objects your cache can hold.  The default is 13 KB.",performance,squid
4922,request_header_max_size,This specifies the maximum size for HTTP headers in a request.,performance,squid
4923,reply_header_max_size,This specifies the maximum size for HTTP headers in a reply.,performance,squid
4924,request_body_max_size,This specifies the maximum size for an HTTP request body.,performance,squid
4925,client_request_buffer_max_size,This specifies the maximum buffer size of a client request. It prevents squid eating too much memory when somebody uploads a large file.,performance,squid
4926,broken_posts,"A list of ACL elements which, if matched, causes Squid to send an extra CRLF pair after the body of a PUT/POST request.",security,squid
4927,adaptation_uses_indirect_client,Controls whether the indirect client IP address (instead of the direct client IP address) is passed to adaptation services.,others,squid
4928,via,"If set (default), Squid will include a Via header in requests and replies as required by RFC2616.",others,squid
4929,vary_ignore_expire,Many HTTP servers supporting Vary gives such objects immediate expiry time with no cache-control header when requested by a HTTP/1.0 client. This option enables Squid to ignore such expiry times until HTTP/1.1 is fully implemented.,reliability,squid
4930,request_entities,Set this directive to on if you have clients which insists on sending request entities in GET or HEAD requests. But be warned that there is server software (both proxies and web servers) which can fail to properly process this kind of request which may make you vulnerable to cache pollution attacks if enabled.,security,squid
4931,request_header_replace,"This option allows you to change the contents of headers denied with request_header_access above, by replacing them with some fixed string.",others,squid
4932,reply_header_replace,"This option allows you to change the contents of headers denied with reply_header_access above, by replacing them with some fixed string.",others,squid
4933,request_header_add,"This option adds header fields to outgoing HTTP requests (i.e., request headers sent by Squid to the next HTTP hop such as a cache peer or an origin server).",others,squid
4934,reply_header_add,"This option adds header fields to outgoing HTTP responses (i.e., response headers delivered by Squid to the client). This option has no effect on cache hit detection.",others,squid
4935,note,This option used to log custom information about the master transaction.,debuggability,squid
4936,relaxed_header_parser,"In the default ""on"" setting Squid accepts certain forms of non-compliant HTTP messages where it is unambiguous what the sending application intended even if the message is not correctly formatted. The messages is then normalized to the correct form when forwarded by Squid.",reliability,squid
4937,collapsed_forwarding,This option controls whether Squid is allowed to merge multiple potentially cachable requests for the same URI before Squid knows whether the response is going to be cachable.,performance,squid
4938,collapsed_forwarding_shared_entries_limit,This limits the size of a table used for sharing information about collapsible entries among SMP workers.,performance,squid
4939,forward_timeout,This parameter specifies how long Squid should at most attempt in finding a forwarding path for the request before giving up.,reliability,squid
4940,connect_timeout,This parameter specifies how long to wait for the TCP connect to the requested server or peer to complete before Squid should attempt to find another path where to forward the request.,reliability,squid
4941,peer_connect_timeout,This parameter specifies how long to wait for a pending TCP connection to a peer cache.,reliability,squid
4942,read_timeout,"After each successful read(), the timeout will be extended by this amount.  If no data is read again after this amount of time, the request is aborted and logged with ERR_READ_TIMEOUT.",reliability,squid
4943,write_timeout,This timeout is tracked for all connections that have data available for writing and are waiting for the socket to become ready.,reliability,squid
4944,request_timeout,How long to wait for complete HTTP request headers after initial connection establishment.,reliability,squid
4945,request_start_timeout,How long to wait for the first request byte after initial connection establishment.,reliability,squid
4946,client_idle_pconn_timeout,How long to wait for the next HTTP request on a persistent client connection after the previous request completes.,reliability,squid
4947,ftp_client_idle_timeout,How long to wait for an FTP request on a connection to Squid ftp_port.,reliability,squid
4948,client_lifetime,The maximum amount of time a client (browser) is allowed to remain connected to the cache process.,reliability,squid
4949,pconn_lifetime,Desired maximum lifetime of a persistent connection.,reliability,squid
4950,half_closed_clients,"Some clients may shutdown the sending side of their TCP connections, while leaving their receiving sides open.",reliability,squid
4951,server_idle_pconn_timeout,Timeout for idle persistent connections to servers and other proxies.,reliability,squid
4952,ident_timeout,Maximum time to wait for IDENT lookups to complete.,reliability,squid
4953,shutdown_lifetime,"When SIGTERM or SIGHUP is received, the cache is put into ""shutdown pending"" mode until all active sockets are closed.",performance,squid
4954,cache_mgr,"Email-address of local cache manager who will receive mail if the cache dies.  The default is ""webmaster"".",reliability,squid
4955,mail_program,Email program used to send mail if the cache dies.,reliability,squid
4956,cache_effective_user,"If you start Squid as root, it will change its effective/real UID/GID to the user specified below.",others,squid
4957,cache_effective_group,Squid sets the GID to the effective user's default group ID (taken from the password file) and supplementary group list from the groups membership.,others,squid
4958,httpd_suppress_version_string,Suppress Squid version string info in HTTP headers and HTML error pages.,debuggability,squid
4959,visible_hostname,"If you want to present a special hostname in error messages, etc, define this.  Otherwise, the return value of gethostname() will be used.",debuggability,squid
4960,unique_hostname,If you want to have multiple machines with the same 'visible_hostname' you must give each machine a different 'unique_hostname' so forwarding loops can be detected.,environment,squid
4961,hostname_aliases,A list of other DNS names your cache has.,others,squid
4962,umask,"Minimum umask which should be enforced while the proxy is running, in addition to the umask set at startup.",security,squid
4963,announce_period,This is how frequently to send cache announcements.,others,squid
4964,announce_host,Set the hostname where announce registration messages will be sent.,environment,squid
4965,announce_file,The contents of this file will be included in the announce registration messages.,others,squid
4966,announce_port,Set the port where announce registration messages will be sent.,environment,squid
4967,httpd_accel_surrogate_id,"Surrogates (http://www.esi.org/architecture_spec_1.0.html) need an identification token to allow control targeting. Because a farm of surrogates may all perform the same tasks, they may share an identification token.",others,squid
4968,http_accel_surrogate_remote,Set this to on to have squid behave as a remote surrogate.,others,squid
4969,esi_parser,Selects the XML parsing library to use when interpreting responses with Edge Side Includes.,others,squid
4970,delay_pools,This represents the number of delay pools to be used.,performance,squid
4971,delay_class,This defines the class of each delay pool.,others,squid
4972,delay_access,This is used to determine which delay pool a request falls into.,others,squid
4973,delay_initial_bucket_level,"The initial bucket percentage is used to determine how much is put in each bucket when squid starts, is reconfigured, or first notices a host accessing it (in class 2 and class 3, individual hosts and networks only have buckets associated with them once they have been ""seen"" by squid).",others,squid
4974,client_delay_pools,This option specifies the number of client delay pools used. It must preceed other client_delay_* options.,performance,squid
4975,client_delay_initial_bucket_level,This option determines the initial bucket size as a percentage of max_bucket_size from client_delay_parameters.,others,squid
4976,client_persistent_connections,Persistent connection support for clients.,reliability,squid
4977,server_persistent_connections,Persistent connection support for servers.,reliability,squid
4978,persistent_connection_after_error,With this directive the use of persistent connections after HTTP errors can be disabled.,reliability,squid
4979,detect_broken_pconn,By enabling this directive Squid attempts to detect such broken replies and automatically assume the reply is finished after 10 seconds timeout.,reliability,squid
4980,digest_generation,This controls whether the server will generate a Cache Digest of its contents.,performance,squid
4981,digest_bits_per_entry,This is the number of bits of the server's Cache Digest which will be associated with the Digest entry for a given HTTP Method and URL (public key) combination.,performance,squid
4982,digest_rebuild_period,This is the wait time between Cache Digest rebuilds.,reliability,squid
4983,digest_rewrite_period,This is the wait time between Cache Digest writes to disk.,reliability,squid
4984,digest_swapout_chunk_size,This is the number of bytes of the Cache Digest to write to disk at a time.,performance,squid
4985,digest_rebuild_chunk_percentage,This is the percentage of the Cache Digest to be scanned at a time.,performance,squid
4986,snmp_port,The port number where Squid listens for SNMP requests.,environment,squid
4987,snmp_access,Allowing or denying access to the SNMP port.,security,squid
4988,snmp_incoming_address,used for the SNMP socket receiving messages from SNMP agents.,environment,squid
4989,snmp_outgoing_address,used for SNMP packets returned to SNMP agents.,environment,squid
4990,icp_port,The port number where Squid sends and receives ICP queries to and from neighbor caches.,environment,squid
4991,htcp_port,The port number where Squid sends and receives HTCP queries to and from neighbor caches.,environment,squid
4992,log_icp_queries,"If set, ICP queries are logged to access.log. You may wish do disable this if your ICP load is VERY high to speed things up or to simplify log analysis.",debuggability,squid
4993,udp_incoming_address,used for UDP packets received from other caches.,environment,squid
4994,udp_outgoing_address,used for UDP packets sent out to other caches.,environment,squid
4995,icp_hit_stale,"If you want to return ICP_HIT for stale cache objects, set this option to 'on'.  If you have sibling relationships with caches in other administrative domains, this should be 'off'.",performance,squid
4996,minimum_direct_hops,"If using the ICMP pinging stuff, do direct fetches for sites which are no more than this many hops away.",performance,squid
4997,minimum_direct_rtt,"If using the ICMP pinging stuff, do direct fetches for sites which are no more than this many rtt milliseconds away.",performance,squid
4998,netdb_low,The low water mark for the ICMP measurement database.,performance,squid
4999,netdb_high,The high water mark for the ICMP measurement database.,performance,squid
5000,netdb_ping_period,The minimum period for measuring a site.  There will be at least this much delay between successive pings to the same network.,reliability,squid
5001,query_icmp,"If you want to ask your peers to include ICMP data in their ICP replies, enable this option.",others,squid
5002,test_reachability,"When this is 'on', ICP MISS replies will be ICP_MISS_NOFETCH instead of ICP_MISS if the target host is NOT in the ICMP database, or has a zero RTT.",reliability,squid
5003,icp_query_timeout,Normally Squid will automatically determine an optimal ICP query timeout value based on the round-trip-time of recent ICP queries.,performance,squid
5004,maximum_icp_query_timeout,Normally the ICP query timeout is determined dynamically.  But sometimes it can lead to very large values (say 5 seconds). Use this option to put an upper limit on the dynamic timeout value.,reliability,squid
5005,minimum_icp_query_timeout,"Normally the ICP query timeout is determined dynamically.  But sometimes it can lead to very small timeouts, even lower than the normal latency variance on your link due to traffic. Use this option to put an lower limit on the dynamic timeout value. ",reliability,squid
5006,background_ping_rate,Controls how often the ICP pings are sent to siblings that have background-ping set.,reliability,squid
5007,mcast_miss_addr,"If you enable this option, every ""cache miss"" URL will be sent out on the specified multicast address.",others,squid
5008,mcast_miss_ttl,This is the time-to-live value for packets multicasted when multicasting off cache miss URLs is enabled. ,reliability,squid
5009,mcast_miss_port,This is the port number to be used in conjunction with 'mcast_miss_addr'.,environment,squid
5010,mcast_miss_encode_key,The URLs that are sent in the multicast miss stream are encrypted.,security,squid
5011,icon_directory,Where the icons are stored. ,environment,squid
5012,global_internal_static,"This directive controls is Squid should intercept all requests for /squid-internal-static/ no matter which host the URL is requesting (default on setting), or if nothing special should be done for such URLs (off setting).",others,squid
5013,short_icon_urls,If this is enabled Squid will use short URLs for icons. If disabled it will revert to the old behavior of including it's own name and port in the URL.,others,squid
5014,error_directory,If you wish to create your own versions of the default error files to customize them to suit your company copy the error/template files to another directory and point this tag at them.,debuggability,squid
5015,error_default_language,Set the default language which squid will send error pages in if no existing translation matches the clients language preferences.,debuggability,squid
5016,error_log_languages,Log to cache.log what languages users are attempting to auto-negotiate for translations.,debuggability,squid
5017,err_page_stylesheet,CSS Stylesheet to pattern the display of Squid default error pages.,debuggability,squid
5018,err_html_text,"HTML text to include in error messages.  Make this a ""mailto"" URL to your admin address, or maybe just a link to your organizations Web page.",debuggability,squid
5019,email_err_data,"If enabled, information about the occurred error will be included in the mailto links of the ERR pages (if %W is set) so that the email body contains the data.",debuggability,squid
5020,nonhierarchical_direct,"By default, Squid will send any non-hierarchical requests (not cacheable request type) direct to origin servers. When this is set to ""off"", Squid will prefer to send these requests to parents.",others,squid
5021,prefer_direct,Normally Squid tries to use parents for most requests. If you for some reason like it to first try going direct and only use a parent if going direct fails set this to on.,others,squid
5022,cache_miss_revalidate,This option determines whether Squid on cache MISS will pass the client revalidation request to the server or tries to fetch new content for caching. ,performance,squid
5023,client_ip_max_connections,Set an absolute limit on the number of connections a single client IP can use. Any more than this and Squid will begin to drop new connections from the client until it closes some links.,reliability,squid
5024,tcp_recv_bufsize,Size of receive buffer to set for TCP sockets.  Probably just as easy to change your kernel's default.,performance,squid
5025,icap_enable,"If you want to enable the ICAP module support, set this to on.",others,squid
5026,icap_connect_timeout,This parameter specifies how long to wait for the TCP connect to the requested ICAP server to complete before giving up and either terminating the HTTP transaction or bypassing the failure.,reliability,squid
5027,icap_io_timeout,"This parameter specifies how long to wait for an I/O activity on an established, active ICAP connection before giving up and either terminating the HTTP transaction or bypassing the failure.",reliability,squid
5028,icap_service_failure_limit,"The limit specifies the number of failures that Squid tolerates when establishing a new TCP connection with an ICAP service. If the number of failures exceeds the limit, the ICAP service is not used for new ICAP requests until it is time to refresh its OPTIONS.",reliability,squid
5029,icap_service_revival_delay,"The delay specifies the number of seconds to wait after an ICAP OPTIONS request failure before requesting the options again. The failed ICAP service is considered ""down"" until fresh OPTIONS are fetched.",reliability,squid
5030,icap_preview_enable,"The ICAP Preview feature allows the ICAP server to handle the HTTP message by looking only at the beginning of the message body or even without receiving the body at all. In some environments,  previews greatly speedup ICAP processing.",performance,squid
5031,icap_preview_size,The default size of preview data to be sent to the ICAP server.,performance,squid
5032,icap_default_options_ttl,The default TTL value for ICAP OPTIONS responses that don't have an Options-TTL header.,reliability,squid
5033,icap_persistent_connections,Whether or not Squid should use persistent connections to an ICAP server.,reliability,squid
5034,adaptation_send_client_ip,"If enabled, Squid shares HTTP client IP information with adaptation services. For ICAP, Squid adds the X-Client-IP header to ICAP requests. For eCAP, Squid sets the libecap::metaClientIp transaction option.",others,squid
5035,adaptation_send_username,This sends authenticated HTTP client username (if available) to the adaptation service.,others,squid
5036,icap_client_username_header,ICAP request header name to use for adaptation_send_username.,others,squid
5037,icap_client_username_encode,Whether to base64 encode the authenticated client username.,others,squid
5038,ecap_enable,Controls whether eCAP support is enabled.,others,squid
5039,loadable_modules,Instructs Squid to load the specified dynamic module(s) or activate preloaded module(s).,others,squid
5040,adaptation_service_set,"Configures an ordered set of similar, redundant services. This is useful when hot standby or backup adaptation servers are available.",reliability,squid
5041,adaptation_service_chain,"Configures a list of complementary services that will be applied one-by-one, forming an adaptation chain or pipeline. This is useful when Squid must perform different adaptations on the same message.",reliability,squid
5042,adaptation_service_iteration_limit,Limits the number of iterations allowed when applying adaptation services to a message.,reliability,squid
5043,adaptation_meta,This option allows Squid administrator to add custom ICAP request headers or eCAP options to Squid ICAP requests or eCAP transactions. Use it to pass custom authentication tokens and other transaction-state related meta information to an ICAP/eCAP service.,others,squid
5044,icap_retry_limit,"Limits the number of retries allowed. Communication errors due to persistent connection race conditions are unavoidable, automatically retried, and do not count against this limit.",reliability,squid
5045,check_hostnames,For security and stability reasons Squid can check hostnames for Internet standard RFC compliance. If you want Squid to perform these checks turn this directive on.,security,squid
5046,allow_underscore,Underscore characters is not strictly allowed in Internet hostnames but nevertheless used by many sites. Set this to off if you want Squid to be strict about the standard.,others,squid
5047,dns_retransmit_interval,Initial retransmit interval for DNS queries. The interval is doubled each time all configured DNS servers have been tried.,reliability,squid
5048,dns_timeout,DNS Query timeout. If no response is received to a DNS query within this time all DNS servers for the queried domain are assumed to be unavailable.,reliability,squid
5049,dns_packet_max,Maximum number of bytes packet size to advertise via EDNS.,reliability,squid
5050,append_domain,Appends local domain name to hostnames without any dots in them.,others,squid
5051,ignore_unknown_nameservers,"By default Squid checks that DNS responses are received from the same IP addresses they are sent to.  If they don't match, Squid ignores the response and writes a warning message to cache.log.  You can allow responses from unknown nameservers by setting this option to 'off'.",reliability,squid
5052,dns_v4_first,With the IPv6 Internet being as fast or faster than IPv4 Internet for most networks Squid prefers to contact websites over IPv6.,performance,squid
5053,ipcache_size,Maximum number of DNS IP cache entries.,performance,squid
5054,ipcache_low,"The size, low-, and high-water marks for the IP cache.",performance,squid
5055,ipcache_high,"The size, low-, and high-water marks for the IP cache.",performance,squid
5056,fqdncache_size,Maximum number of FQDN cache entries.,performance,squid
5057,configuration_includes_quoted_values,"If set, Squid will recognize each ""quoted string"" after a configuration directive as a single parameter. The quotes are stripped before the parameter value is interpreted or used.",others,squid
5058,memory_pools,"If set, Squid will keep pools of allocated (but unused) memory available for future use.  If memory is a premium on your system and you believe your malloc library outperforms Squid routines, disable this.",performance,squid
5059,forwarded_for,"If set to ""on"", Squid will append your client's IP address in the HTTP requests it forwards.",others,squid
5060,cachemgr_passwd,Specify passwords for cachemgr operations.,security,squid
5061,client_db,"If you want to disable collecting per-client statistics, turn off client_db here.",others,squid
5062,refresh_all_ims,"When you enable this option, squid will always check the origin server for an update when a client sends an If-Modified-Since request.",others,squid
5063,reload_into_ims,"When you enable this option, client no-cache or ''reload'' requests will be changed to If-Modified-Since requests. Doing this VIOLATES the HTTP standard.  Enabling this feature could make you liable for problems which it causes.",reliability,squid
5064,connect_retries,Limits the number of reopening attempts when establishing a single TCP connection. All these attempts must still complete before the applicable connection opening timeout expires.,reliability,squid
5065,retry_on_error,"If set to ON Squid will automatically retry requests when receiving an error response with status 403 (Forbidden), 500 (Internal Error), 501 or 503 (Service not available). Status 502 and 504 (Gateway errors) are always retried.",reliability,squid
5066,as_whois_server,"WHOIS server to query for AS numbers.  NOTE: AS numbers are queried only when Squid starts up, not for every request.",environment,squid
5067,offline_mode,Enable this option and Squid will never try to validate cached objects.,performance,squid
5068,chroot,Specifies a directory where Squid should do a chroot() while initializing.  This also causes Squid to fully drop root privileges after initializing.,security,squid
5069,high_response_time_warning,"If the one-minute median response time exceeds this value, Squid prints a WARNING with debug level 0 to get the administrators attention.",debuggability,squid
5070,high_page_fault_warning,"If the one-minute average page fault rate exceeds this value, Squid prints a WARNING with debug level 0 to get the administrators attention.",debuggability,squid
5071,high_memory_warning,"If the memory usage (as determined by gnumalloc, if available and used) exceeds this amount, Squid prints a WARNING with debug level 0 to get the administrators attention.",debuggability,squid
5072,sleep_after_fork,"When this is set to a non-zero value, the main Squid process sleeps the specified number of microseconds after a fork() system call.",others,squid
5073,eui_lookup,Whether to lookup the EUI or MAC address of a connected client.,performance,squid
5074,force_request_body_continuation,"This option controls how Squid handles data upload requests from HTTP and FTP agents that require a ""Please Continue"" control message response to actually send the request body to Squid. It is mostly useful in adaptation environments.",others,squid
5075,server_pconn_for_nonretriable,This option provides fine-grained control over persistent connection reuse when forwarding HTTP requests that Squid cannot retry.,performance,squid
5076,yarn.ipc.client.factory.class,Factory to create client IPC classes.,others,yarn
5077,yarn.ipc.server.factory.class,Factory to create server IPC classes.,others,yarn
5078,yarn.ipc.record.factory.class,Factory to create serializeable records.,others,yarn
5079,yarn.ipc.rpc.class,RPC class implementation,others,yarn
5080,yarn.resourcemanager.hostname,The hostname of the RM.,environment,yarn
5081,yarn.resourcemanager.address,The address of the applications manager interface in the RM.,environment,yarn
5082,yarn.resourcemanager.bind-host,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.resourcemanager.address and yarn.resourcemanager.webapp.address, respectively. This is most useful for making RM listen to all interfaces by setting to 0.0.0.0.",environment,yarn
5083,yarn.resourcemanager.auto-update.containers,"If set to true, then ALL container updates will be automatically sent to the NM in the next heartbeat",reliability,yarn
5084,yarn.resourcemanager.client.thread-count,The number of threads used to handle applications manager requests.,performance,yarn
5085,yarn.resourcemanager.amlauncher.thread-count,Number of threads used to launch/cleanup AM.,performance,yarn
5086,yarn.resourcemanager.nodemanager-connect-retries,Retry times to connect with NM.,reliability,yarn
5087,yarn.dispatcher.drain-events.timeout,"Timeout in milliseconds when YARN dispatcher tries to drain the events. Typically, this happens when service is stopping. e.g. RM drains the ATS events dispatcher when stopping.",reliability,yarn
5088,yarn.am.liveness-monitor.expiry-interval-ms,The expiry interval for application master reporting.,reliability,yarn
5089,yarn.resourcemanager.principal,The Kerberos principal for the resource manager.,security,yarn
5090,yarn.resourcemanager.scheduler.address,The address of the scheduler interface.,environment,yarn
5091,yarn.resourcemanager.scheduler.client.thread-count,Number of threads to handle scheduler interface.,performance,yarn
5092,yarn.resourcemanager.application-master-service.processors,Comma separated class names of ApplicationMasterServiceProcessor implementations. The processors will be applied in the order they are specified.,others,yarn
5093,yarn.http.policy,This configures the HTTP endpoint for YARN Daemons.The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https,security,yarn
5094,yarn.resourcemanager.webapp.address,"The http address of the RM web application. If only a host is provided as the value, the webapp will be served on a random port.",environment,yarn
5095,yarn.resourcemanager.webapp.https.address,"The https address of the RM web application. If only a host is provided as the value, the webapp will be served on a random port.",environment,yarn
5096,yarn.resourcemanager.webapp.spnego-keytab-file,The Kerberos keytab file to be used for spnego filter for the RM web interface.,security,yarn
5097,yarn.resourcemanager.webapp.spnego-principal,The Kerberos principal to be used for spnego filter for the RM web interface.,security,yarn
5098,yarn.resourcemanager.webapp.ui-actions.enabled,Add button to kill application in the RM Application view.,others,yarn
5099,yarn.webapp.ui2.enable,To enable RM web ui2 application.,others,yarn
5100,yarn.webapp.ui2.war-file-path,Explicitly provide WAR file path for ui2 if needed.,environment,yarn
5101,yarn.acl.enable,Are acls enabled.,others,yarn
5102,yarn.acl.reservation-enable,Are reservation acls enabled.,security,yarn
5103,yarn.admin.acl,ACL of who can be admin of the YARN cluster.,security,yarn
5104,yarn.resourcemanager.admin.address,The address of the RM admin interface.,environment,yarn
5105,yarn.resourcemanager.admin.client.thread-count,Number of threads used to handle RM admin interface.,performance,yarn
5106,yarn.resourcemanager.connect.max-wait.ms,Maximum time to wait to establish connection to ResourceManager.,reliability,yarn
5107,yarn.resourcemanager.connect.retry-interval.ms,How often to try connecting to the ResourceManager.,reliability,yarn
5108,yarn.resourcemanager.am.max-attempts,"The maximum number of application attempts. It's a global setting for all application masters. Each application master can specify its individual maximum number of application attempts via the API, but the individual number cannot be more than the global upper bound. If it is, the resourcemanager will override it. The default number is set to 2, to allow at least one retry for AM.",reliability,yarn
5109,yarn.resourcemanager.container.liveness-monitor.interval-ms,How often to check that containers are still alive.,reliability,yarn
5110,yarn.resourcemanager.keytab,The keytab for the resource manager.,environment,yarn
5111,yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,Flag to enable override of the default kerberos authentication filter with the RM authentication filter to allow authentication using delegation tokens(fallback to kerberos if the tokens are missing). Only applicable when the http authentication type is kerberos.,security,yarn
5112,yarn.resourcemanager.webapp.cross-origin.enabled,Flag to enable cross-origin (CORS) support in the RM. This flag requires the CORS filter initializer to be added to the filter initializers list in core-site.xml.,environment,yarn
5113,yarn.nm.liveness-monitor.expiry-interval-ms,How long to wait until a node manager is considered dead.,reliability,yarn
5114,yarn.resourcemanager.nodes.include-path,Path to file with nodes to include.,environment,yarn
5115,yarn.resourcemanager.nodes.exclude-path,Path to file with nodes to exclude.,environment,yarn
5116,yarn.resourcemanager.node-ip-cache.expiry-interval-secs,The expiry interval for node IP caching. -1 disables the caching,reliability,yarn
5117,yarn.resourcemanager.resource-tracker.client.thread-count,Number of threads to handle resource tracker calls.,performance,yarn
5118,yarn.resourcemanager.scheduler.class,The class to use as the resource scheduler.,performance,yarn
5119,yarn.scheduler.minimum-allocation-mb,"The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager.",performance,yarn
5120,yarn.scheduler.maximum-allocation-mb,The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an InvalidResourceRequestException.,performance,yarn
5121,yarn.scheduler.minimum-allocation-vcores,"The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource manager.",performance,yarn
5122,yarn.scheduler.maximum-allocation-vcores,The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException.,performance,yarn
5123,yarn.scheduler.include-port-in-node-name,"Used by node labels. If set to true, the port should be included in the node name. Only usable if your scheduler supports node labels.",others,yarn
5124,yarn.resourcemanager.recovery.enabled,"Enable RM to recover state after starting. If true, then yarn.resourcemanager.store.class must be specified.",reliability,yarn
5125,yarn.resourcemanager.fail-fast,"Should RM fail fast if it encounters any errors. By defalt, it points to ${yarn.fail-fast}. Errors include: 1) exceptions when state-store write/read operations fails.",reliability,yarn
5126,yarn.fail-fast,"Should YARN fail fast if it encounters any errors. This is a global config for all other components including RM,NM etc. If no value is set for component-specific config (e.g yarn.resourcemanager.fail-fast), this value will be the default.",reliability,yarn
5127,yarn.resourcemanager.work-preserving-recovery.enabled,Enable RM work preserving recovery. This configuration is private to YARN for experimenting the feature.,reliability,yarn
5128,yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms,"Set the amount of time RM waits before allocating new containers on work-preserving-recovery. Such wait period gives RM a chance to settle down resyncing with NMs in the cluster on recovery, before assigning new containers to applications.",reliability,yarn
5129,yarn.resourcemanager.store.class,"The class to use as the persistent store. If org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore is used, the store is implicitly fenced; meaning a single ResourceManager is able to use the store at any point in time. More details on this implicit fencing, along with setting up appropriate ACLs is discussed under yarn.resourcemanager.zk-state-store.root-node.acl.",environment,yarn
5130,yarn.resourcemanager.ha.failover-controller.active-standby-elector.zk.retries,"When automatic failover is enabled, number of zookeeper operation retry times in ActiveStandbyElector",reliability,yarn
5131,yarn.resourcemanager.state-store.max-completed-applications,"The maximum number of completed applications RM state store keeps, less than or equals to ${yarn.resourcemanager.max-completed-applications}. By default, it equals to ${yarn.resourcemanager.max-completed-applications}. This ensures that the applications kept in the state store are consistent with the applications remembered in RM memory. Any values larger than ${yarn.resourcemanager.max-completed-applications} will be reset to ${yarn.resourcemanager.max-completed-applications}. Note that this value impacts the RM recovery performance.Typically, a smaller value indicates better performance on RM recovery.",reliability,yarn
5132,yarn.resourcemanager.zk-state-store.parent-path,Full path of the ZooKeeper znode where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore as the value for yarn.resourcemanager.store.class,environment,yarn
5133,yarn.resourcemanager.zk-state-store.root-node.acl,"ACLs to be used for the root znode when using ZKRMStateStore in an HA scenario for fencing. ZKRMStateStore supports implicit fencing to allow a single ResourceManager write-access to the store. For fencing, the ResourceManagers in the cluster share read-write-admin privileges on the root node, but the Active ResourceManager claims exclusive create-delete permissions. By default, when this property is not set, we use the ACLs from yarn.resourcemanager.zk-acl for shared admin access and rm-address:random-number for username-based exclusive create-delete access. This property allows users to set ACLs of their choice instead of using the default mechanism. For fencing to work, the ACLs should be carefully set differently on each ResourceManger such that all the ResourceManagers have shared admin access and the Active ResourceManger takes over (exclusively) the create-delete access.",security,yarn
5134,yarn.resourcemanager.fs.state-store.uri,URI pointing to the location of the FileSystem path where RM state will be stored. This must be supplied when using org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore as the value for yarn.resourcemanager.store.class,environment,yarn
5135,yarn.resourcemanager.fs.state-store.retry-policy-spec,"hdfs client retry policy specification. hdfs client retry is always enabled. Specified in pairs of sleep-time and number-of-retries and (t0, n0), (t1, n1), ..., the first n0 retries sleep t0 milliseconds on average, the following n1 retries sleep t1 milliseconds on average, and so on.",reliability,yarn
5136,yarn.resourcemanager.fs.state-store.num-retries,the number of retries to recover from IOException in FileSystemRMStateStore.,reliability,yarn
5137,yarn.resourcemanager.fs.state-store.retry-interval-ms,Retry interval in milliseconds in FileSystemRMStateStore.,reliability,yarn
5138,yarn.resourcemanager.leveldb-state-store.path,Local path where the RM state will be stored when using org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore as the value for yarn.resourcemanager.store.class,environment,yarn
5139,yarn.resourcemanager.leveldb-state-store.compaction-interval-secs,The time in seconds between full compactions of the leveldb database. Setting the interval to zero disables the full compaction cycles.,reliability,yarn
5140,yarn.resourcemanager.ha.enabled,"Enable RM high-availability. When enabled, (1) The RM starts in the Standby mode by default, and transitions to the Active mode when prompted to. (2) The nodes in the RM ensemble are listed in yarn.resourcemanager.ha.rm-ids (3) The id of each RM either comes from yarn.resourcemanager.ha.id if yarn.resourcemanager.ha.id is explicitly specified or can be figured out by matching yarn.resourcemanager.address.{id} with local address (4) The actual physical addresses come from the configs of the pattern - {rpc-config}.{id}",others,yarn
5141,yarn.resourcemanager.ha.automatic-failover.enabled,"Enable automatic failover. By default, it is enabled only when HA is enabled",reliability,yarn
5142,yarn.resourcemanager.ha.automatic-failover.embedded,"Enable embedded automatic failover. By default, it is enabled only when HA is enabled. The embedded elector relies on the RM state store to handle fencing, and is primarily intended to be used in conjunction with ZKRMStateStore.",reliability,yarn
5143,yarn.resourcemanager.ha.automatic-failover.zk-base-path,"The base znode path to use for storing leader information, when using ZooKeeper based leader election.",environment,yarn
5144,yarn.resourcemanager.zk-appid-node.split-index,"Index at which last section of application id (with each section separated by _ in application id) will be split so that application znode stored in zookeeper RM state store will be stored as two different znodes (parent-child). Split is done from the end. For instance, with no split, appid znode will be of the form application_1352994193343_0001. If the value of this config is 1, the appid znode will be broken into two parts application_1352994193343_000 and 1 respectively with former being the parent node. application_1352994193343_0002 will then be stored as 2 under the parent node application_1352994193343_000. This config can take values from 0 to 4. 0 means there will be no split. If configuration value is outside this range, it will be treated as config value of 0(i.e. no split). A value larger than 0 (up to 4) should be configured if you are storing a large number of apps in ZK based RM state store and state store operations are failing due to LenError in Zookeeper.",others,yarn
5145,yarn.resourcemanager.zk-delegation-token-node.split-index,"Index at which the RM Delegation Token ids will be split so that the delegation token znodes stored in the zookeeper RM state store will be stored as two different znodes (parent-child). The split is done from the end. For instance, with no split, a delegation token znode will be of the form RMDelegationToken_123456789. If the value of this config is 1, the delegation token znode will be broken into two parts: RMDelegationToken_12345678 and 9 respectively with former being the parent node. This config can take values from 0 to 4. 0 means there will be no split. If the value is outside this range, it will be treated as 0 (i.e. no split). A value larger than 0 (up to 4) should be configured if you are running a large number of applications, with long-lived delegation tokens and state store operations (e.g. failover) are failing due to LenError in Zookeeper.",others,yarn
5146,yarn.resourcemanager.zk-max-znode-size.bytes,Specifies the maximum size of the data that can be stored in a znode. Value should be same or less than jute.maxbuffer configured in zookeeper. Default value configured is 1MB.,reliability,yarn
5147,yarn.resourcemanager.cluster-id,"Name of the cluster. In a HA setting, this is used to ensure the RM participates in leader election for this cluster and ensures it does not affect other clusters",others,yarn
5148,yarn.resourcemanager.ha.rm-ids,The list of RM nodes in the cluster when HA is enabled. See description of yarn.resourcemanager.ha .enabled for full details on how this is used.,others,yarn
5149,yarn.resourcemanager.ha.id,"The id (string) of the current RM. When HA is enabled, this is an optional config. The id of current RM can be set by explicitly specifying yarn.resourcemanager.ha.id or figured out by matching yarn.resourcemanager.address.{id} with local address See description of yarn.resourcemanager.ha.enabled for full details on how this is used.",others,yarn
5150,yarn.client.failover-proxy-provider,"When HA is enabled, the class to be used by Clients, AMs and NMs to failover to the Active RM. It should extend org.apache.hadoop.yarn.client.RMFailoverProxyProvider",others,yarn
5151,yarn.client.failover-max-attempts,"When HA is enabled, the max number of times FailoverProxyProvider should attempt failover. When set, this overrides the yarn.resourcemanager.connect.max-wait.ms. When not set, this is inferred from yarn.resourcemanager.connect.max-wait.ms.",reliability,yarn
5152,yarn.client.failover-sleep-base-ms,"When HA is enabled, the sleep base (in milliseconds) to be used for calculating the exponential delay between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.",reliability,yarn
5153,yarn.client.failover-sleep-max-ms,"When HA is enabled, the maximum sleep time (in milliseconds) between failovers. When set, this overrides the yarn.resourcemanager.connect.* settings. When not set, yarn.resourcemanager.connect.retry-interval.ms is used instead.",reliability,yarn
5154,yarn.client.failover-retries,"When HA is enabled, the number of retries per attempt to connect to a ResourceManager. In other words, it is the ipc.client.connect.max.retries to be used during failover attempts",reliability,yarn
5155,yarn.client.failover-retries-on-socket-timeouts,"When HA is enabled, the number of retries per attempt to connect to a ResourceManager on socket timeouts. In other words, it is the ipc.client.connect.max.retries.on.timeouts to be used during failover attempts",reliability,yarn
5156,yarn.resourcemanager.max-completed-applications,The maximum number of completed applications RM keeps.,reliability,yarn
5157,yarn.resourcemanager.delayed.delegation-token.removal-interval-ms,Interval at which the delayed token removal thread runs,reliability,yarn
5158,yarn.resourcemanager.delegation-token.max-conf-size-bytes,"Maximum size in bytes for configurations that can be provided by application to RM for delegation token renewal. By experiment, it's roughly 128 bytes per key-value pair. The default value 12800 allows roughly 100 configs, may be less.",performance,yarn
5159,yarn.resourcemanager.proxy-user-privileges.enabled,"If true, ResourceManager will have proxy-user privileges. Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to do localization and log-aggregation on behalf of the user. If this is set to true, ResourceManager is able to request new hdfs delegation tokens on behalf of the user. This is needed by long-running-service, because the hdfs tokens will eventually expire and YARN requires new valid tokens to do localization and log-aggregation. Note that to enable this use case, the corresponding HDFS NameNode has to configure ResourceManager as the proxy-user so that ResourceManager can itself ask for new tokens on behalf of the user when tokens are past their max-life-time.",security,yarn
5160,yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs,Interval for the roll over for the master key used to generate application tokens,security,yarn
5161,yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,Interval for the roll over for the master key used to generate container tokens. It is expected to be much greater than yarn.nm.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.rm.container-allocation.expiry-interval-ms. Otherwise the behavior is undefined.,reliability,yarn
5162,yarn.resourcemanager.nodemanagers.heartbeat-interval-ms,The heart-beat interval in milliseconds for every NodeManager in the cluster.,reliability,yarn
5163,yarn.resourcemanager.nodemanager.minimum.version,"The minimum allowed version of a connecting nodemanager. The valid values are NONE (no version checking), EqualToRM (the nodemanager's version is equal to or greater than the RM version), or a Version String.",environment,yarn
5164,yarn.resourcemanager.scheduler.monitor.enable,Enable a set of periodic monitors (specified in yarn.resourcemanager.scheduler.monitor.policies) that affect the scheduler.,performance,yarn
5165,yarn.resourcemanager.scheduler.monitor.policies,"The list of SchedulingEditPolicy classes that interact with the scheduler. A particular module may be incompatible with the scheduler, other policies, or a configuration of either.",performance,yarn
5166,yarn.resourcemanager.configuration.provider-class,"The class to use as the configuration provider. If org.apache.hadoop.yarn.LocalConfigurationProvider is used, the local configuration will be loaded. If org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider is used, the configuration which will be loaded should be uploaded to remote File system first.",others,yarn
5167,yarn.resourcemanager.configuration.file-system-based-store,The value specifies the file system (e.g. HDFS) path where ResourceManager loads configuration if yarn.resourcemanager.configuration.provider-class is set to org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider.,environment,yarn
5168,yarn.resourcemanager.system-metrics-publisher.enabled,"The setting that controls whether yarn system metrics is published to the Timeline server (version one) or not, by RM. This configuration is now deprecated in favor of yarn.system-metrics-publisher.enabled.",others,yarn
5169,yarn.system-metrics-publisher.enabled,The setting that controls whether yarn system metrics is published on the Timeline service or not by RM And NM.,others,yarn
5170,yarn.rm.system-metrics-publisher.emit-container-events,The setting that controls whether yarn container events are published to the timeline service or not by RM. This configuration setting is for ATS V2.,others,yarn
5171,yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size,Number of worker threads that send the yarn system metrics data.,performance,yarn
5172,yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory,Number of diagnostics/failure messages can be saved in RM for log aggregation. It also defines the number of diagnostics/failure messages can be shown in log aggregation web ui.,debuggability,yarn
5173,yarn.resourcemanager.delegation-token-renewer.thread-count,RM DelegationTokenRenewer thread count,others,yarn
5174,yarn.resourcemanager.delegation.key.update-interval,RM secret key update interval in ms,security,yarn
5175,yarn.resourcemanager.delegation.token.max-lifetime,RM delegation token maximum lifetime in ms,reliability,yarn
5176,yarn.resourcemanager.delegation.token.renew-interval,RM delegation token update interval in ms,security,yarn
5177,yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size,Thread pool size for RMApplicationHistoryWriter.,performance,yarn
5178,yarn.resourcemanager.metrics.runtime.buckets,Comma-separated list of values (in minutes) for schedule queue related metrics.,performance,yarn
5179,yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,Interval for the roll over for the master key used to generate NodeManager tokens. It is expected to be set to a value much larger than yarn.nm.liveness-monitor.expiry-interval-ms.,reliability,yarn
5180,yarn.resourcemanager.reservation-system.enable,Flag to enable the ResourceManager reservation system.,others,yarn
5181,yarn.resourcemanager.reservation-system.class,"The Java class to use as the ResourceManager reservation system. By default, is set to org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem when using CapacityScheduler and is set to org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem when using FairScheduler.",others,yarn
5182,yarn.resourcemanager.reservation-system.plan.follower,"The plan follower policy class name to use for the ResourceManager reservation system. By default, is set to org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower is used when using CapacityScheduler, and is set to org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower when using FairScheduler.",environment,yarn
5183,yarn.resourcemanager.reservation-system.planfollower.time-step,Step size of the reservation system in ms,others,yarn
5184,yarn.resourcemanager.rm.container-allocation.expiry-interval-ms,The expiry interval for a container,reliability,yarn
5185,yarn.nodemanager.hostname,The hostname of the NM.,environment,yarn
5186,yarn.nodemanager.address,The address of the container manager in the NM.,environment,yarn
5187,yarn.nodemanager.bind-host,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.nodemanager.address and yarn.nodemanager.webapp.address, respectively. This is most useful for making NM listen to all interfaces by setting to 0.0.0.0.",environment,yarn
5188,yarn.nodemanager.admin-env,Environment variables that should be forwarded from the NodeManager's environment to the container's.,environment,yarn
5189,yarn.nodemanager.env-whitelist,Environment variables that containers may override rather than use NodeManager's default.,environment,yarn
5190,yarn.nodemanager.container-executor.class,who will execute(launch) the containers.,others,yarn
5191,yarn.nodemanager.container-state-transition-listener.classes,Comma separated List of container state transition listeners.,others,yarn
5192,yarn.nodemanager.container-manager.thread-count,Number of threads container manager uses.,performance,yarn
5193,yarn.nodemanager.collector-service.thread-count,Number of threads collector service uses.,performance,yarn
5194,yarn.nodemanager.delete.thread-count,Number of threads used in cleanup.,performance,yarn
5195,yarn.nodemanager.opportunistic-containers-max-queue-length,Max number of OPPORTUNISTIC containers to queue at the nodemanager.,reliability,yarn
5196,yarn.nodemanager.delete.debug-delay-sec,"Number of seconds after an application finishes before the nodemanager's DeletionService will delete the application's localized file directory and log directory. To diagnose YARN application problems, set this property's value large enough (for example, to 600 = 10 minutes) to permit examination of these directories. After changing the property's value, you must restart the nodemanager in order for it to have an effect. The roots of YARN applications' work directories is configurable with the yarn.nodemanager.local-dirs property (see below), and the roots of the YARN applications' log directories is configurable with the yarn.nodemanager.log-dirs property (see also below).",reliability,yarn
5197,yarn.nodemanager.keytab,Keytab for NM.,environment,yarn
5198,yarn.nodemanager.local-dirs,"List of directories to store localized files in. An application's localized file directory will be found in: ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}. Individual containers' work directories, called container_${contid}, will be subdirectories of this.",environment,yarn
5199,yarn.nodemanager.local-cache.max-files-per-directory,"It limits the maximum number of files which will be localized in a single local directory. If the limit is reached then sub-directories will be created and new files will be localized in them. If it is set to a value less than or equal to 36 [which are sub-directories (0-9 and then a-z)] then NodeManager will fail to start. For example; [for public cache] if this is configured with a value of 40 ( 4 files + 36 sub-directories) and the local-dir is ""/tmp/local-dir1"" then it will allow 4 files to be created directly inside ""/tmp/local-dir1/filecache"". For files that are localized further it will create a sub-directory ""0"" inside ""/tmp/local-dir1/filecache"" and will localize files inside it until it becomes full. If a file is removed from a sub-directory that is marked full, then that sub-directory will be used back again to localize files.",reliability,yarn
5200,yarn.nodemanager.localizer.address,Address where the localizer IPC is.,environment,yarn
5201,yarn.nodemanager.collector-service.address,Address where the collector service IPC is.,environment,yarn
5202,yarn.nodemanager.localizer.cache.cleanup.interval-ms,Interval in between cache cleanups.,performance,yarn
5203,yarn.nodemanager.localizer.cache.target-size-mb,"Target size of localizer cache in MB, per nodemanager. It is a target retention size that only includes resources with PUBLIC and PRIVATE visibility and excludes resources with APPLICATION visibility",performance,yarn
5204,yarn.nodemanager.localizer.client.thread-count,Number of threads to handle localization requests.,performance,yarn
5205,yarn.nodemanager.localizer.fetch.thread-count,Number of threads to use for localization fetching.,performance,yarn
5206,yarn.nodemanager.log-dirs,"Where to store container logs. An application's localized log directory will be found in ${yarn.nodemanager.log-dirs}/application_${appid}. Individual containers' log directories will be below this, in directories named container_{$contid}. Each container directory will contain the files stderr, stdin, and syslog generated by that container.",environment,yarn
5207,yarn.nodemanager.default-container-executor.log-dirs.permissions,The permissions settings used for the creation of container directories when using DefaultContainerExecutor. This follows standard user/group/all permissions format.,security,yarn
5208,yarn.log-aggregation-enable,"Whether to enable log aggregation. Log aggregation collects each container's logs and moves these logs onto a file-system, for e.g. HDFS, after the application completes. Users can configure the ""yarn.nodemanager.remote-app-log-dir"" and ""yarn.nodemanager.remote-app-log-dir-suffix"" properties to determine where these logs are moved to. Users can access the logs via the Application Timeline Server.",debuggability,yarn
5209,yarn.log-aggregation.retain-seconds,How long to keep aggregation logs before deleting them. -1 disables. Be careful set this too small and you will spam the name node.,reliability,yarn
5210,yarn.log-aggregation.retain-check-interval-seconds,How long to wait between aggregated log retention checks. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful set this too small and you will spam the name node.,reliability,yarn
5211,yarn.log-aggregation.file-formats,"Specify which log file controllers we will support. The first file controller we add will be used to write the aggregated logs. This comma separated configuration will work with the configuration: yarn.log-aggregation.file-controller.%s.class which defines the supported file controller's class. By default, the TFile controller would be used. The user could override this configuration by adding more file controllers. To support back-ward compatibility, make sure that we always add TFile file controller.",others,yarn
5212,yarn.log-aggregation.file-controller.TFile.class,Class that supports TFile read and write operations.,others,yarn
5213,yarn.log-aggregation-status.time-out.ms,"How long for ResourceManager to wait for NodeManager to report its log aggregation status. If waiting time of which the log aggregation status is reported from NodeManager exceeds the configured value, RM will report log aggregation status for this NodeManager as TIME_OUT",reliability,yarn
5214,yarn.nodemanager.log.retain-seconds,Time in seconds to retain user logs. Only applicable if log aggregation is disabled,reliability,yarn
5215,yarn.nodemanager.remote-app-log-dir,Where to aggregate logs to.,environment,yarn
5216,yarn.nodemanager.remote-app-log-dir-suffix,The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam},environment,yarn
5217,yarn.nodemanager.log-container-debug-info.enabled,"Generate additional logs about container launches. Currently, this creates a copy of the launch script and lists the directory contents of the container work dir. When listing directory contents, we follow symlinks to a max-depth of 5(including symlinks which point to outside the container work dir) which may lead to a slowness in launching containers.",debuggability,yarn
5218,yarn.nodemanager.resource.memory-mb,"Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.",performance,yarn
5219,yarn.nodemanager.resource.system-reserved-memory-mb,"Amount of physical memory, in MB, that is reserved for non-YARN processes. This configuration is only used if yarn.nodemanager.resource.detect-hardware-capabilities is set to true and yarn.nodemanager.resource.memory-mb is -1. If set to -1, this amount is calculated as 20% of (system memory - 2*HADOOP_HEAPSIZE)",performance,yarn
5220,yarn.nodemanager.pmem-check-enabled,Whether physical memory limits will be enforced for containers.,reliability,yarn
5221,yarn.nodemanager.vmem-check-enabled,Whether virtual memory limits will be enforced for containers.,reliability,yarn
5222,yarn.nodemanager.vmem-pmem-ratio,"Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.",performance,yarn
5223,yarn.nodemanager.resource.cpu-vcores,"Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.",performance,yarn
5224,yarn.nodemanager.resource.count-logical-processors-as-cores,Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.,performance,yarn
5225,yarn.nodemanager.resource.pcores-vcores-multiplier,Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.,performance,yarn
5226,yarn.nodemanager.logaggregation.threadpool-size-max,Thread pool size for LogAggregationService in Node Manager.,performance,yarn
5227,yarn.nodemanager.resource.percentage-physical-cpu-limit,Percentage of CPU that can be allocated for containers. This setting allows users to limit the amount of CPU that YARN containers use. Currently functional only on Linux using cgroups. The default is to use 100% of CPU.,performance,yarn
5228,yarn.nodemanager.resource.detect-hardware-capabilities,Enable auto-detection of node capabilities such as memory and CPU.,performance,yarn
5229,yarn.nodemanager.webapp.address,NM Webapp address.,environment,yarn
5230,yarn.nodemanager.webapp.https.address,The https adddress of the NM web application.,environment,yarn
5231,yarn.nodemanager.webapp.spnego-keytab-file,The Kerberos keytab file to be used for spnego filter for the NM web interface.,security,yarn
5232,yarn.nodemanager.webapp.spnego-principal,The Kerberos principal to be used for spnego filter for the NM web interface.,security,yarn
5233,yarn.nodemanager.resource-monitor.interval-ms,"How often to monitor the node and the containers. If 0 or negative, monitoring is disabled.",reliability,yarn
5234,yarn.nodemanager.resource-calculator.class,Class that calculates current resource utilization.,performance,yarn
5235,yarn.nodemanager.container-monitor.enabled,Enable container monitor,others,yarn
5236,yarn.nodemanager.container-monitor.interval-ms,"How often to monitor containers. If not set, the value for yarn.nodemanager.resource-monitor.interval-ms will be used. If 0 or negative, container monitoring is disabled.",reliability,yarn
5237,yarn.nodemanager.container-monitor.resource-calculator.class,"Class that calculates containers current resource utilization. If not set, the value for yarn.nodemanager.resource-calculator.class will be used.",performance,yarn
5238,yarn.nodemanager.health-checker.interval-ms,Frequency of running node health script.,reliability,yarn
5239,yarn.nodemanager.health-checker.script.timeout-ms,Script time out period.,reliability,yarn
5240,yarn.nodemanager.health-checker.script.path,The health check script to run.,reliability,yarn
5241,yarn.nodemanager.health-checker.script.opts,The arguments to pass to the health check script.,reliability,yarn
5242,yarn.nodemanager.disk-health-checker.interval-ms,Frequency of running disk health checker code.,reliability,yarn
5243,yarn.nodemanager.disk-health-checker.min-healthy-disks,"The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers. This correspond to both yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.",reliability,yarn
5244,yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,"The maximum percentage of disk space utilization allowed after which a disk is marked as bad. Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.",reliability,yarn
5245,yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage,"The low threshold percentage of disk space used when a bad disk is marked as good. Values can range from 0.0 to 100.0. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs. Note that if its value is more than yarn.nodemanager.disk-health-checker. max-disk-utilization-per-disk-percentage or not set, it will be set to the same value as yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.",reliability,yarn
5246,yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,The minimum space that must be available on a disk for it to be used. This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.,reliability,yarn
5247,yarn.nodemanager.linux-container-executor.path,The path to the Linux container executor.,environment,yarn
5248,yarn.nodemanager.linux-container-executor.resources-handler.class,The class which should help the LCE handle resources.,performance,yarn
5249,yarn.nodemanager.linux-container-executor.cgroups.hierarchy,"The cgroups hierarchy under which to place YARN proccesses (cannot contain commas). If yarn.nodemanager.linux-container-executor.cgroups.mount is false (that is, if cgroups have been pre-configured) and the YARN user has write access to the parent directory, then the directory will be created. If the directory already exists, the administrator has to give YARN write permissions to it recursively. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.",security,yarn
5250,yarn.nodemanager.linux-container-executor.cgroups.mount,Whether the LCE should attempt to mount cgroups if not found. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.,reliability,yarn
5251,yarn.nodemanager.linux-container-executor.cgroups.mount-path,"This property sets the path from which YARN will read the CGroups configuration. YARN has built-in functionality to discover the system CGroup mount paths, so use this property only if YARN's automatic mount path discovery does not work. The path specified by this property must exist before the NodeManager is launched. If yarn.nodemanager.linux-container-executor.cgroups.mount is set to true, YARN will first try to mount the CGroups at the specified path before reading them. If yarn.nodemanager.linux-container-executor.cgroups.mount is set to false, YARN will read the CGroups at the specified path. If this property is empty, YARN tries to detect the CGroups location. Please refer to NodeManagerCgroups.html in the documentation for further details. This property only applies when the LCE resources handler is set to CgroupsLCEResourcesHandler.",environment,yarn
5252,yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms,Delay in ms between attempts to remove linux cgroup,reliability,yarn
5253,yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,"This determines which of the two modes that LCE should use on a non-secure cluster. If this value is set to true, then all containers will be launched as the user specified in yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user. If this value is set to false, then containers will run as the user who submitted the application.",security,yarn
5254,yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,The UNIX user that containers will run as when Linux-container-executor is used in nonsecure mode (a use case for this is using cgroups) if the yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users is set to true.,security,yarn
5255,yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern,The allowed pattern for UNIX user names enforced by Linux-container-executor when used in nonsecure mode (use case for this is using cgroups). The default value is taken from /usr/sbin/adduser,security,yarn
5256,yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage,"This flag determines whether apps should run with strict resource limits or be allowed to consume spare resources if they need them. For example, turning the flag on will restrict apps to use only their share of CPU, even if the node has spare CPU cycles. The default value is false i.e. use available resources. Please note that turning this flag on may reduce job throughput on the cluster.",reliability,yarn
5257,yarn.nodemanager.runtime.linux.allowed-runtimes,Comma separated list of runtimes that are allowed when using LinuxContainerExecutor. The allowed values are default and docker.,reliability,yarn
5258,yarn.nodemanager.runtime.linux.docker.capabilities,"This configuration setting determines the capabilities assigned to docker containers when they are launched. While these may not be case-sensitive from a docker perspective, it is best to keep these uppercase. To run without any capabilites, set this value to ""none"" or ""NONE""",reliability,yarn
5259,yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed,This configuration setting determines if privileged docker containers are allowed on this cluster. Use with extreme care.,security,yarn
5260,yarn.nodemanager.runtime.linux.docker.privileged-containers.acl,This configuration setting determines who is allowed to run privileged docker containers on this cluster. Use with extreme care.,security,yarn
5261,yarn.nodemanager.runtime.linux.docker.allowed-container-networks,The set of networks allowed when launching containers using the DockerContainerRuntime.,environment,yarn
5262,yarn.nodemanager.runtime.linux.docker.default-container-network,The network used when launching containers using the DockerContainerRuntime when no network is specified in the request . This network must be one of the (configurable) set of allowed container networks.,environment,yarn
5263,yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed,Property to enable docker user remapping,others,yarn
5264,yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold,lower limit for acceptable uids of user remapped user,reliability,yarn
5265,yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold,lower limit for acceptable gids of user remapped user,reliability,yarn
5266,yarn.nodemanager.windows-container.memory-limit.enabled,This flag determines whether memory limit will be set for the Windows Job Object of the containers launched by the default container executor.,reliability,yarn
5267,yarn.nodemanager.windows-container.cpu-limit.enabled,This flag determines whether CPU limit will be set for the Windows Job Object of the containers launched by the default container executor.,reliability,yarn
5268,yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms,Interval of time the linux container executor should try cleaning up cgroups entry when cleaning up a container.,reliability,yarn
5269,yarn.nodemanager.linux-container-executor.group,The UNIX group that the linux-container-executor should run as.,environment,yarn
5270,yarn.nodemanager.log-aggregation.compression-type,T-file compression types used to compress aggregated logs.,performance,yarn
5271,yarn.nodemanager.principal,The kerberos principal for the node manager.,security,yarn
5272,yarn.nodemanager.aux-services,A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers,others,yarn
5273,yarn.nodemanager.sleep-delay-before-sigkill.ms,No. of ms to wait between sending a SIGTERM and SIGKILL to a container,reliability,yarn
5274,yarn.nodemanager.process-kill-wait.ms,Max time to wait for a process to come up when trying to cleanup a container,reliability,yarn
5275,yarn.nodemanager.resourcemanager.minimum.version,"The minimum allowed version of a resourcemanager that a nodemanager will connect to. The valid values are NONE (no version checking), EqualToNM (the resourcemanager's version is equal to or greater than the NM version), or a Version String.",environment,yarn
5276,yarn.nodemanager.container-diagnostics-maximum-size,Maximum size of contain's diagnostics to keep for relaunching container case.,reliability,yarn
5277,yarn.nodemanager.container-retry-minimum-interval-ms,Minimum container restart interval in milliseconds.,reliability,yarn
5278,yarn.client.nodemanager-client-async.thread-pool-max-size,Max number of threads in NMClientAsync to process container management events,performance,yarn
5279,yarn.client.nodemanager-connect.max-wait-ms,Max time to wait to establish a connection to NM,reliability,yarn
5280,yarn.client.nodemanager-connect.retry-interval-ms,Time interval between each attempt to connect to NM,reliability,yarn
5281,yarn.nodemanager.resourcemanager.connect.max-wait.ms,"Max time to wait for NM to connect to RM. When not set, proxy will fall back to use value of yarn.resourcemanager.connect.max-wait.ms.",reliability,yarn
5282,yarn.nodemanager.resourcemanager.connect.retry-interval.ms,"Time interval between each NM attempt to connect to RM. When not set, proxy will fall back to use value of yarn.resourcemanager.connect.retry-interval.ms.",reliability,yarn
5283,yarn.client.max-cached-nodemanagers-proxies,Maximum number of proxy connections to cache for node managers. If set to a value greater than zero then the cache is enabled and the NMClient and MRAppMaster will cache the specified number of node manager proxies. There will be at max one proxy per node manager. Ex. configuring it to a value of 5 will make sure that client will at max have 5 proxies cached with 5 different node managers. These connections for these proxies will be timed out if idle for more than the system wide idle timeout period. Note that this could cause issues on large clusters as many connections could linger simultaneously and lead to a large number of connection threads. The token used for authentication will be used only at connection creation time. If a new token is received then the earlier connection should be closed in order to use the new token. This and (yarn.client.nodemanager-client-async.thread-pool-max-size) are related and should be in sync (no need for them to be equal). If the value of this property is zero then the connection cache is disabled and connections will use a zero idle timeout to prevent too many connection threads on large clusters.,performance,yarn
5284,yarn.nodemanager.recovery.enabled,Enable the node manager to recover after starting,reliability,yarn
5285,yarn.nodemanager.recovery.dir,The local filesystem directory in which the node manager will store state when recovery is enabled.,environment,yarn
5286,yarn.nodemanager.recovery.compaction-interval-secs,The time in seconds between full compactions of the NM state database. Setting the interval to zero disables the full compaction cycles.,reliability,yarn
5287,yarn.nodemanager.recovery.supervised,Whether the nodemanager is running under supervision. A nodemanager that supports recovery and is running under supervision will not try to cleanup containers as it exits with the assumption it will be immediately be restarted and recover containers.,reliability,yarn
5288,yarn.nodemanager.container-executor.os.sched.priority.adjustment,"Adjustment to the container OS scheduling priority. In Linux, passed directly to the nice command. If unspecified then containers are launched without any explicit OS priority.",others,yarn
5289,yarn.nodemanager.container-metrics.enable,Flag to enable container metrics,others,yarn
5290,yarn.nodemanager.container-metrics.period-ms,Container metrics flush period in ms. Set to -1 for flush on completion.,reliability,yarn
5291,yarn.nodemanager.container-metrics.unregister-delay-ms,The delay time ms to unregister container metrics after completion.,reliability,yarn
5292,yarn.nodemanager.container-monitor.process-tree.class,Class used to calculate current container resource utilization.,performance,yarn
5293,yarn.nodemanager.disk-health-checker.enable,Flag to enable NodeManager disk health checker,reliability,yarn
5294,yarn.nodemanager.log.deletion-threads-count,Number of threads to use in NM log cleanup. Used when log aggregation is disabled.,performance,yarn
5295,yarn.nodemanager.windows-secure-container-executor.group,The Windows group that the windows-container-executor should run as.,others,yarn
5296,yarn.nodemanager.docker-container-executor.exec-name,Name or path to the Docker client.,environment,yarn
5297,yarn.nodemanager.docker-container-executor.image-name,The Docker image name to use for DockerContainerExecutor,environment,yarn
5298,yarn.web-proxy.principal,"The kerberos principal for the proxy, if the proxy is not running as part of the RM.",security,yarn
5299,yarn.web-proxy.keytab,"Keytab for WebAppProxy, if the proxy is not running as part of the RM.",security,yarn
5300,yarn.web-proxy.address,"The address for the web proxy as HOST:PORT, if this is not given then the proxy will run as part of the RM",environment,yarn
5301,yarn.application.classpath,"CLASSPATH for YARN applications. A comma-separated list of CLASSPATH entries. When this value is empty, the following default CLASSPATH for YARN applications would be used. ",environment,yarn
5302,yarn.timeline-service.version,"Indicate what is the current version of the running timeline service. For example, if ""yarn.timeline-service.version"" is 1.5, and ""yarn.timeline-service.enabled"" is true, it means the cluster will and should bring up the timeline service v.1.5 (and nothing else). On the client side, if the client uses the same version of timeline service, it should succeed. If the client chooses to use a smaller version in spite of this, then depending on how robust the compatibility story is between versions, the results may vary.",environment,yarn
5303,yarn.timeline-service.enabled,"In the server side it indicates whether timeline service is enabled or not. And in the client side, users can enable it to indicate whether client wants to use timeline service. If it's enabled in the client side along with security, then yarn client tries to fetch the delegation tokens for the timeline server.",others,yarn
5304,yarn.timeline-service.hostname,The hostname of the timeline service web application.,environment,yarn
5305,yarn.timeline-service.address,This is default address for the timeline server to start the RPC server.,environment,yarn
5306,yarn.timeline-service.webapp.address,The http address of the timeline service web application.,environment,yarn
5307,yarn.timeline-service.webapp.https.address,The https address of the timeline service web application.,environment,yarn
5308,yarn.timeline-service.bind-host,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.timeline-service.address and yarn.timeline-service.webapp.address, respectively. This is most useful for making the service listen to all interfaces by setting to 0.0.0.0.",environment,yarn
5309,yarn.timeline-service.generic-application-history.max-applications,Defines the max number of applications could be fetched using REST API or application history protocol and shown in timeline server web ui.,reliability,yarn
5310,yarn.timeline-service.store-class,Store class name for timeline store.,others,yarn
5311,yarn.timeline-service.ttl-enable,Enable age off of timeline store data.,others,yarn
5312,yarn.timeline-service.ttl-ms,Time to live for timeline store data in milliseconds.,reliability,yarn
5313,yarn.timeline-service.leveldb-timeline-store.path,Store file name for leveldb timeline store.,environment,yarn
5314,yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms,Length of time to wait between deletion cycles of leveldb timeline store in milliseconds.,reliability,yarn
5315,yarn.timeline-service.leveldb-timeline-store.read-cache-size,Size of read cache for uncompressed blocks for leveldb timeline store in bytes.,performance,yarn
5316,yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,Size of cache for recently read entity start times for leveldb timeline store in number of entities.,performance,yarn
5317,yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size,Size of cache for recently written entity start times for leveldb timeline store in number of entities.,performance,yarn
5318,yarn.timeline-service.handler-thread-count,Handler thread count to serve the client RPC requests.,performance,yarn
5319,yarn.timeline-service.http-authentication.type,Defines authentication used for the timeline server HTTP endpoint. Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#,security,yarn
5320,yarn.timeline-service.http-authentication.simple.anonymous.allowed,Indicates if anonymous requests are allowed by the timeline server when using 'simple' authentication.,security,yarn
5321,yarn.timeline-service.principal,The Kerberos principal for the timeline server.,security,yarn
5322,yarn.timeline-service.keytab,The Kerberos keytab for the timeline server.,security,yarn
5323,yarn.timeline-service.ui-names,Comma separated list of UIs that will be hosted,others,yarn
5324,yarn.timeline-service.client.max-retries,Default maximum number of retries for timeline service client and value -1 means no limit.,reliability,yarn
5325,yarn.timeline-service.client.best-effort,"Client policy for whether timeline operations are non-fatal. Should the failure to obtain a delegation token be considered an application failure (option = false), or should the client attempt to continue to publish information without it (option=true)",reliability,yarn
5326,yarn.timeline-service.client.retry-interval-ms,Default retry time interval for timeline servive client.,reliability,yarn
5327,yarn.timeline-service.client.drain-entities.timeout.ms,The time period for which timeline v2 client will wait for draining leftover entities after stop.,reliability,yarn
5328,yarn.timeline-service.recovery.enabled,"Enable timeline server to recover state after starting. If true, then yarn.timeline-service.state-store-class must be specified.",reliability,yarn
5329,yarn.timeline-service.state-store-class,Store class name for timeline state store.,others,yarn
5330,yarn.timeline-service.leveldb-state-store.path,Store file name for leveldb state store.,environment,yarn
5331,yarn.timeline-service.entity-group-fs-store.cache-store-class,Caching storage timeline server v1.5 is using.,others,yarn
5332,yarn.timeline-service.entity-group-fs-store.active-dir,HDFS path to store active application's timeline data,environment,yarn
5333,yarn.timeline-service.entity-group-fs-store.done-dir,HDFS path to store done application's timeline data,environment,yarn
5334,yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes,"Plugins that can translate a timeline entity read request into a list of timeline entity group ids, separated by commas.",environment,yarn
5335,yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath,Classpath for all plugins defined in yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes.,environment,yarn
5336,yarn.timeline-service.entity-group-fs-store.summary-store,Summary storage for ATS v1.5,environment,yarn
5337,yarn.timeline-service.entity-group-fs-store.scan-interval-seconds,Scan interval for ATS v1.5 entity group file system storage reader.This value controls how frequent the reader will scan the HDFS active directory for application status.,reliability,yarn
5338,yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds,Scan interval for ATS v1.5 entity group file system storage cleaner.This value controls how frequent the reader will scan the HDFS done directory for stale application data.,reliability,yarn
5339,yarn.timeline-service.entity-group-fs-store.retain-seconds,How long the ATS v1.5 entity group file system storage will keep an application's data in the done directory.,reliability,yarn
5340,yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size,Read cache size for the leveldb cache storage in ATS v1.5 plugin storage.,performance,yarn
5341,yarn.timeline-service.entity-group-fs-store.app-cache-size,"Size of the reader cache for ATS v1.5 reader. This value controls how many entity groups the ATS v1.5 server should cache. If the number of active read entity groups is greater than the number of caches items, some reads may return empty data. This value must be greater than 0.",performance,yarn
5342,yarn.timeline-service.client.fd-flush-interval-secs,Flush interval for ATS v1.5 writer. This value controls how frequent the writer will flush the HDFS FSStream for the entity/domain.,reliability,yarn
5343,yarn.timeline-service.client.fd-clean-interval-secs,"Scan interval for ATS v1.5 writer. This value controls how frequent the writer will scan the HDFS FSStream for the entity/domain. If the FSStream is stale for a long time, this FSStream will be close.",reliability,yarn
5344,yarn.timeline-service.client.fd-retain-secs,"How long the ATS v1.5 writer will keep an FSStream open. If this fsstream does not write anything for this configured time, it will be close.",reliability,yarn
5345,yarn.timeline-service.writer.class,Storage implementation ATS v2 will use for the TimelineWriter service.,others,yarn
5346,yarn.timeline-service.reader.class,Storage implementation ATS v2 will use for the TimelineReader service.,others,yarn
5347,yarn.timeline-service.client.internal-timers-ttl-secs,"How long the internal Timer Tasks can be alive in writer. If there is no write operation for this configured time, the internal timer tasks will be close.",reliability,yarn
5348,yarn.timeline-service.writer.flush-interval-seconds,The setting that controls how often the timeline collector flushes the timeline writer.,reliability,yarn
5349,yarn.timeline-service.app-collector.linger-period.ms,"Time period till which the application collector will be alive in NM, after the application master container finishes.",reliability,yarn
5350,yarn.timeline-service.timeline-client.number-of-async-entities-to-merge,Time line V2 client tries to merge these many number of async entities (if available) and then call the REST ATS V2 API to submit.,reliability,yarn
5351,yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds ,The setting that controls how long the final value of a metric of a completed app is retained before merging into the flow sum. Up to this time after an application is completed out-of-order values that arrive can be recognized and discarded at the cost of increased storage.,reliability,yarn
5352,yarn.timeline-service.hbase.coprocessor.jar.hdfs.location ,The default hdfs location for flowrun coprocessor jar.,environment,yarn
5353,yarn.timeline-service.hbase-schema.prefix,"The value of this parameter sets the prefix for all tables that are part of timeline service in the hbase storage schema. It can be set to ""dev."" or ""staging."" if it is to be used for development or staging instances. This way the data in production tables stays in a separate set of tables prefixed by ""prod."".",others,yarn
5354,yarn.timeline-service.hbase.configuration.file ,"Optional URL to an hbase-site.xml configuration file to be used to connect to the timeline-service hbase cluster. If empty or not specified, then the HBase configuration will be loaded from the classpath. When specified the values in the specified configuration file will override those from the ones that are present on the classpath.",environment,yarn
5355,yarn.sharedcache.enabled,Whether the shared cache is enabled,others,yarn
5356,yarn.sharedcache.root-dir,The root directory for the shared cache,environment,yarn
5357,yarn.sharedcache.nested-level,The level of nested directories before getting to the checksum directories. It must be non-negative.,others,yarn
5358,yarn.sharedcache.store.class,The implementation to be used for the SCM store,others,yarn
5359,yarn.sharedcache.app-checker.class,The implementation to be used for the SCM app-checker,others,yarn
5360,yarn.sharedcache.store.in-memory.staleness-period-mins,A resource in the in-memory store is considered stale if the time since the last reference exceeds the staleness period. This value is specified in minutes.,reliability,yarn
5361,yarn.sharedcache.store.in-memory.initial-delay-mins,Initial delay before the in-memory store runs its first check to remove dead initial applications. Specified in minutes.,reliability,yarn
5362,yarn.sharedcache.store.in-memory.check-period-mins,The frequency at which the in-memory store checks to remove dead initial applications. Specified in minutes.,reliability,yarn
5363,yarn.sharedcache.admin.address,The address of the admin interface in the SCM (shared cache manager),environment,yarn
5364,yarn.sharedcache.admin.thread-count,The number of threads used to handle SCM admin interface (1 by default),performance,yarn
5365,yarn.sharedcache.webapp.address,The address of the web application in the SCM (shared cache manager),environment,yarn
5366,yarn.sharedcache.cleaner.period-mins,The frequency at which a cleaner task runs. Specified in minutes.,reliability,yarn
5367,yarn.sharedcache.cleaner.initial-delay-mins,Initial delay before the first cleaner task is scheduled. Specified in minutes.,reliability,yarn
5368,yarn.sharedcache.cleaner.resource-sleep-ms,The time to sleep between processing each shared cache resource. Specified in milliseconds.,reliability,yarn
5369,yarn.sharedcache.uploader.server.address,The address of the node manager interface in the SCM (shared cache manager),environment,yarn
5370,yarn.sharedcache.uploader.server.thread-count,The number of threads used to handle shared cache manager requests from the node manager (50 by default),performance,yarn
5371,yarn.sharedcache.client-server.address,The address of the client interface in the SCM (shared cache manager),environment,yarn
5372,yarn.sharedcache.client-server.thread-count,The number of threads used to handle shared cache manager requests from clients (50 by default),performance,yarn
5373,yarn.sharedcache.checksum.algo.impl,The algorithm used to compute checksums of files (SHA-256 by default),reliability,yarn
5374,yarn.sharedcache.nm.uploader.replication.factor,The replication factor for the node manager uploader for the shared cache (10 by default),reliability,yarn
5375,yarn.sharedcache.nm.uploader.thread-count,The number of threads used to upload files from a node manager instance (20 by default),performance,yarn
5376,security.applicationhistory.protocol.acl,ACL protocol for use in the Timeline server.,security,yarn
5377,yarn.is.minicluster,Set to true for MiniYARNCluster unit tests,others,yarn
5378,yarn.minicluster.control-resource-monitoring,Set for MiniYARNCluster unit tests to control resource monitoring,debuggability,yarn
5379,yarn.minicluster.fixed.ports,Set to false in order to allow MiniYARNCluster to run tests without port conflicts.,environment,yarn
5380,yarn.minicluster.use-rpc,Set to false in order to allow the NodeManager in MiniYARNCluster to use RPC to talk to the RM.,others,yarn
5381,yarn.minicluster.yarn.nodemanager.resource.memory-mb,As yarn.nodemanager.resource.memory-mb property but for the NodeManager in a MiniYARNCluster.,performance,yarn
5382,yarn.node-labels.enabled,Enable node labels feature,others,yarn
5383,yarn.node-labels.fs-store.retry-policy-spec,"Retry policy used for FileSystem node label store. The policy is specified by N pairs of sleep-time in milliseconds and number-of-retries ""s1,n1,s2,n2,..."".",reliability,yarn
5384,yarn.node-labels.fs-store.root-dir,URI for NodeLabelManager. The default value is /tmp/hadoop-yarn-${user}/node-labels/ in the local filesystem.,environment,yarn
5385,yarn.node-labels.configuration-type,"Set configuration type for node labels. Administrators can specify ""centralized"", ""delegated-centralized"" or ""distributed"".",others,yarn
5386,yarn.nodemanager.node-labels.provider,"When ""yarn.node-labels.configuration-type"" is configured with ""distributed"" in RM, Administrators can configure in NM the provider for the node labels by configuring this parameter. Administrators can configure ""config"", ""script"" or the class name of the provider. Configured class needs to extend org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider. If ""config"" is configured, then ""ConfigurationNodeLabelsProvider"" and if ""script"" is configured, then ""ScriptNodeLabelsProvider"" will be used.",others,yarn
5387,yarn.nodemanager.node-labels.provider.fetch-interval-ms,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"", ""Script"" or the configured class extends AbstractNodeLabelsProvider, then periodically node labels are retrieved from the node labels provider. This configuration is to define the interval period. If -1 is configured then node labels are retrieved from provider only during initialization. Defaults to 10 mins.",others,yarn
5388,yarn.nodemanager.node-labels.resync-interval-ms,"Interval at which NM syncs its node labels with RM. NM will send its loaded labels every x intervals configured, along with heartbeat to RM.",reliability,yarn
5389,yarn.nodemanager.node-labels.provider.configured-node-partition,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""config"" then ConfigurationNodeLabelsProvider fetches the partition label from this parameter.",others,yarn
5390,yarn.nodemanager.node-labels.provider.fetch-timeout-ms,"When ""yarn.nodemanager.node-labels.provider"" is configured with ""Script"" then this configuration provides the timeout period after which it will interrupt the script which queries the Node labels. Defaults to 20 mins.",reliability,yarn
5391,yarn.resourcemanager.node-labels.provider,"When node labels ""yarn.node-labels.configuration-type"" is of type ""delegated-centralized"", administrators should configure the class for fetching node labels by ResourceManager. Configured class needs to extend org.apache.hadoop.yarn.server.resourcemanager.nodelabels. RMNodeLabelsMappingProvider.",others,yarn
5392,yarn.resourcemanager.node-labels.provider.fetch-interval-ms,"When ""yarn.node-labels.configuration-type"" is configured with ""delegated-centralized"", then periodically node labels are retrieved from the node labels provider. This configuration is to define the interval. If -1 is configured then node labels are retrieved from provider only once for each node after it registers. Defaults to 30 mins.",reliability,yarn
5393,yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs,Timeout in seconds for YARN node graceful decommission. This is the maximal time to wait for running containers and applications to complete before transition a DECOMMISSIONING node into DECOMMISSIONED.,reliability,yarn
5394,yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs,Timeout in seconds of DecommissioningNodesWatcher internal polling.,reliability,yarn
5395,yarn.nodemanager.node-labels.provider.script.path,"The Node Label script to run. Script output Line starting with ""NODE_PARTITION:"" will be considered as Node Label Partition. In case of multiple lines have this pattern, then last one will be considered",environment,yarn
5396,yarn.nodemanager.node-labels.provider.script.opts,The arguments to pass to the Node label script.,others,yarn
5397,yarn.federation.enabled,Flag to indicate whether the RM is participating in Federation or not.,others,yarn
5398,yarn.federation.machine-list,Machine list file to be loaded by the FederationSubCluster Resolver,environment,yarn
5399,yarn.federation.subcluster-resolver.class,Class name for SubClusterResolver,others,yarn
5400,yarn.federation.state-store.class,Store class name for federation state store,others,yarn
5401,yarn.federation.cache-ttl.secs,The time in seconds after which the federation state store local cache will be refreshed periodically,reliability,yarn
5402,yarn.federation.registry.base-dir,The registry base directory for federation.,environment,yarn
5403,yarn.registry.class,The registry implementation to use.,others,yarn
5404,yarn.client.application-client-protocol.poll-interval-ms,The interval that the yarn client library uses to poll the completion status of the asynchronous API of application client protocol.,reliability,yarn
5405,yarn.client.application-client-protocol.poll-timeout-ms,The duration (in ms) the YARN client waits for an expected state change to occur. -1 means unlimited wait time.,reliability,yarn
5406,yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled,"RSS usage of a process computed via /proc/pid/stat is not very accurate as it includes shared pages of a process. /proc/pid/smaps provides useful information like Private_Dirty, Private_Clean, Shared_Dirty, Shared_Clean which can be used for computing more accurate RSS. When this flag is enabled, RSS is computed as Min(Shared_Dirty, Pss) + Private_Clean + Private_Dirty. It excludes read-only shared mappings in RSS computation.",others,yarn
5407,yarn.log.server.url,URL for log aggregation server,environment,yarn
5408,yarn.log.server.web-service.url,URL for log aggregation server web service,environment,yarn
5409,yarn.tracking.url.generator,RM Application Tracking URL,debuggability,yarn
5410,yarn.authorization-provider,Class to be used for YarnAuthorizationProvider,security,yarn
5411,yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,"Defines how often NMs wake up to upload log files. The default value is -1. By default, the logs will be uploaded when the application is finished. By setting this configure, logs can be uploaded periodically when the application is running. The minimum rolling-interval-seconds can be set is 3600.",reliability,yarn
5412,yarn.intermediate-data-encryption.enable,"Enable/disable intermediate-data encryption at YARN level. For now, this only is used by the FileSystemRMStateStore to setup right file-system security attributes.",security,yarn
5413,yarn.nodemanager.webapp.cross-origin.enabled,Flag to enable cross-origin (CORS) support in the NM. This flag requires the CORS filter initializer to be added to the filter initializers list in core-site.xml.,others,yarn
5414,yarn.cluster.max-application-priority,"Defines maximum application priority in a cluster. If an application is submitted with a priority higher than this value, it will be reset to this maximum value.",others,yarn
5415,yarn.nodemanager.log-aggregation.policy.class,The default log aggregation policy class. Applications can override it via LogAggregationContext. This configuration can provide some cluster-side default behavior so that if the application doesn't specify any policy via LogAggregationContext administrators of the cluster can adjust the policy globally.,debuggability,yarn
5416,yarn.nodemanager.log-aggregation.policy.parameters,The default parameters for the log aggregation policy. Applications can override it via LogAggregationContext. This configuration can provide some cluster-side default behavior so that if the application doesn't specify any policy via LogAggregationContext administrators of the cluster can adjust the policy globally.,debuggability,yarn
5417,yarn.nodemanager.amrmproxy.enabled,Enable/Disable AMRMProxyService in the node manager. This service is used to intercept calls from the application masters to the resource manager.,others,yarn
5418,yarn.nodemanager.amrmproxy.address,The address of the AMRMProxyService listener.,environment,yarn
5419,yarn.nodemanager.amrmproxy.client.thread-count,The number of threads used to handle requests by the AMRMProxyService.,performance,yarn
5420,yarn.nodemanager.amrmproxy.interceptor-class.pipeline,The comma separated list of class names that implement the RequestInterceptor interface. This is used by the AMRMProxyService to create the request processing pipeline for applications.,others,yarn
5421,yarn.nodemanager.amrmproxy.ha.enable,Whether AMRMProxy HA is enabled.,others,yarn
5422,yarn.nodemanager.distributed-scheduling.enabled,Setting that controls whether distributed scheduling is enabled.,performance,yarn
5423,yarn.resourcemanager.opportunistic-container-allocation.enabled,Setting that controls whether opportunistic container allocation is enabled.,others,yarn
5424,yarn.resourcemanager.opportunistic-container-allocation.nodes-used,Number of nodes to be used by the Opportunistic Container Allocator for dispatching containers during container allocation.,performance,yarn
5425,yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms,Frequency for computing least loaded NMs.,performance,yarn
5426,yarn.resourcemanager.nm-container-queuing.load-comparator,Comparator for determining node load for Distributed Scheduling.,others,yarn
5427,yarn.resourcemanager.nm-container-queuing.queue-limit-stdev,Value of standard deviation used for calculation of queue limit thresholds.,others,yarn
5428,yarn.resourcemanager.nm-container-queuing.min-queue-length,Min length of container queue at NodeManager.,others,yarn
5429,yarn.resourcemanager.nm-container-queuing.max-queue-length,Max length of container queue at NodeManager.,reliability,yarn
5430,yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms,Min queue wait time for a container at a NodeManager.,reliability,yarn
5431,yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms,Max queue wait time for a container queue at a NodeManager.,reliability,yarn
5432,yarn.nodemanager.opportunistic-containers-use-pause-for-preemption,Use container pause as the preemption policy over kill in the container queue at a NodeManager.,reliability,yarn
5433,yarn.nodemanager.container.stderr.pattern,"Error filename pattern, to identify the file in the container's Log directory which contain the container's error log. As error file redirection is done by client/AM and yarn will not be aware of the error file name. YARN uses this pattern to identify the error file and tail the error log as diagnostics when the container execution returns non zero value. Filename patterns are case sensitive and should match the specifications of FileSystem.globStatus(Path) api. If multiple filenames matches the pattern, first file matching the pattern will be picked.",debuggability,yarn
5434,yarn.nodemanager.container.stderr.tail.bytes ,"Size of the container error file which needs to be tailed, in bytes.",reliability,yarn
5435,yarn.node-labels.fs-store.impl.class,Choose different implementation of node label's storage,others,yarn
5436,yarn.resourcemanager.webapp.rest-csrf.enabled,Enable the CSRF filter for the RM web app,security,yarn
5437,yarn.resourcemanager.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,security,yarn
5438,yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,security,yarn
5439,yarn.nodemanager.webapp.rest-csrf.enabled,Enable the CSRF filter for the NM web app,others,yarn
5440,yarn.nodemanager.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,security,yarn
5441,yarn.nodemanager.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,security,yarn
5442,yarn.nodemanager.disk-validator,The name of disk validator.,environment,yarn
5443,yarn.timeline-service.webapp.rest-csrf.enabled,Enable the CSRF filter for the timeline service web app,security,yarn
5444,yarn.timeline-service.webapp.rest-csrf.custom-header,Optional parameter that indicates the custom header name to use for CSRF protection.,security,yarn
5445,yarn.timeline-service.webapp.rest-csrf.methods-to-ignore,Optional parameter that indicates the list of HTTP methods that do not require CSRF protection,security,yarn
5446,yarn.webapp.xfs-filter.enabled,Enable the XFS filter for YARN,others,yarn
5447,yarn.resourcemanager.webapp.xfs-filter.xframe-options,Property specifying the xframe options value.,security,yarn
5448,yarn.nodemanager.webapp.xfs-filter.xframe-options,Property specifying the xframe options value.,security,yarn
5449,yarn.timeline-service.webapp.xfs-filter.xframe-options,Property specifying the xframe options value.,security,yarn
5450,yarn.resourcemanager.node-removal-untracked.timeout-ms,"The least amount of time(msec.) an inactive (decommissioned or shutdown) node can stay in the nodes list of the resourcemanager after being declared untracked. A node is marked untracked if and only if it is absent from both include and exclude nodemanager lists on the RM. All inactive nodes are checked twice per timeout interval or every 10 minutes, whichever is lesser, and marked appropriately. The same is done when refreshNodes command (graceful or otherwise) is invoked.",reliability,yarn
5451,yarn.resourcemanager.application-timeouts.monitor.interval-ms,The RMAppLifetimeMonitor Service uses this value as monitor interval,reliability,yarn
5452,yarn.app.attempt.diagnostics.limit.kc,"Defines the limit of the diagnostics message of an application attempt, in kilo characters (character count * 1024). When using ZooKeeper to store application state behavior, it's important to limit the size of the diagnostic messages to prevent YARN from overwhelming ZooKeeper. In cases where yarn.resourcemanager.state-store.max-completed-applications is set to a large number, it may be desirable to reduce the value of this property to limit the total data stored.",reliability,yarn
5453,yarn.timeline-service.http-cross-origin.enabled,"Flag to enable cross-origin (CORS) support for timeline service v1.x or Timeline Reader in timeline service v2. For timeline service v2, also add org.apache.hadoop.security.HttpCrossOriginFilterInitializer to the configuration hadoop.http.filter.initializers in core-site.xml.",others,yarn
5454,yarn.scheduler.queue-placement-rules,"Comma-separated list of PlacementRules to determine how applications submitted by certain users get mapped to certain queues. Default is user-group, which corresponds to UserGroupMappingPlacementRule.",others,yarn
5455,yarn.timeline-service.entity-group-fs-store.with-user-dir,It is TimelineClient 1.5 configuration whether to store active application's timeline data with in user directory i.e ${yarn.timeline-service.entity-group-fs-store.active-dir}/${user.name},environment,yarn
5456,yarn.router.clientrm.interceptor-class.pipeline,The comma separated list of class names that implement the RequestInterceptor interface. This is used by the RouterClientRMService to create the request processing pipeline for users.,others,yarn
5457,yarn.router.pipeline.cache-max-size,Size of LRU cache for Router ClientRM Service and RMAdmin Service.,performance,yarn
5458,yarn.router.rmadmin.interceptor-class.pipeline,The comma separated list of class names that implement the RequestInterceptor interface. This is used by the RouterRMAdminService to create the request processing pipeline for users.,others,yarn
5459,yarn.router.bind-host,"The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in yarn.router.address and yarn.router.webapp.address, respectively. This is most useful for making Router listen to all interfaces by setting to 0.0.0.0.",environment,yarn
5460,yarn.router.webapp.interceptor-class.pipeline,The comma separated list of class names that implement the RequestInterceptor interface. This is used by the RouterWebServices to create the request processing pipeline for users.,others,yarn
5461,yarn.router.webapp.address,"The http address of the Router web application. If only a host is provided as the value, the webapp will be served on a random port.",environment,yarn
5462,yarn.router.webapp.https.address,"The https address of the Router web application. If only a host is provided as the value, the webapp will be served on a random port.",environment,yarn
5463,yarn.resourcemanager.display.per-user-apps,Flag to enable display of applications per user as an admin configuration.,security,yarn
5464,yarn.scheduler.configuration.store.class,"The type of configuration store to use for scheduler configurations. Default is ""file"", which uses file based capacity-scheduler.xml to retrieve and change scheduler configuration. To enable API based scheduler configuration, use either ""memory"" (in memory storage, no persistence across restarts), ""leveldb"" (leveldb based storage), or ""zk"" (zookeeper based storage). API based configuration is only useful when using a scheduler which supports mutable configuration. Currently only capacity scheduler supports this.",others,yarn
5465,yarn.scheduler.configuration.mutation.acl-policy.class,The class to use for configuration mutation ACL policy if using a mutable configuration provider. Controls whether a mutation request is allowed. The DefaultConfigurationMutationACLPolicy checks if the requestor is a YARN admin.,security,yarn
5466,yarn.scheduler.configuration.leveldb-store.path,"The storage path for LevelDB implementation of configuration store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"".",environment,yarn
5467,yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,"The compaction interval for LevelDB configuration store in secs, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"". Default is one day.",performance,yarn
5468,yarn.scheduler.configuration.store.max-logs,"The max number of configuration change log entries kept in config store, when yarn.scheduler.configuration.store.class is configured to be ""leveldb"" or ""zk"". Default is 1000 for either.",reliability,yarn
5469,yarn.scheduler.configuration.zk-store.parent-path,ZK root node path for configuration store when using zookeeper-based configuration store.,environment,yarn
