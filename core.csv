name,value,description,label,software
hadoop.tmp.dir,/tmp/hadoop-${user.name},A base for other temporary directories.,environment,core
hadoop.security.authorization,FALSE,Is service-level authorization enabled?,security,core
hadoop.security.instrumentation.requires.admin,FALSE,"Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS).",security,core
hadoop.security.authentication,simple,"Possible values are simple (no authentication), and kerberos",security,core
hadoop.security.group.mapping,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback,"Class for user to group mapping (get groups for a given user) for ACL. The default implementation, org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, will determine if the Java Native Interface (JNI) is available. If JNI is available the implementation will use the API within hadoop to resolve a list of groups for a user. If JNI is not available then the shell implementation, ShellBasedUnixGroupsMapping, is used. This implementation shells out to the Linux/Unix environment with the bash -c groups command to resolve a list of groups for a user.",security,core
hadoop.security.dns.nameserver,,The host name or IP address of the name server (DNS) which a service Node should use to determine its own host name for Kerberos Login. Requires hadoop.security.dns.interface. Most clusters will not require this setting.,environment,core
hadoop.security.groups.cache.secs,300,"This is the config controlling the validity of the entries in the cache containing the user->group mapping. When this duration has expired, then the implementation of the group mapping provider is invoked to get the groups of the user and then cached back.",security,core
hadoop.security.groups.cache.warn.after.ms,5000,"If looking up a single user to group takes longer than this amount of milliseconds, we will log a warning message.",reliability,core
hadoop.security.groups.cache.background.reload,FALSE,"Whether to reload expired user->group mappings using a background thread pool. If set to true, a pool of hadoop.security.groups.cache.background.reload.threads is created to update the cache in the background.",security,core
hadoop.security.groups.cache.background.reload.threads,3,Only relevant if hadoop.security.groups.cache.background.reload is true. Controls the number of concurrent background user->group cache entry refreshes. Pending refresh requests beyond this value are queued and processed when a thread is free.,performance,core
hadoop.security.groups.shell.command.timeout,0s,"Used by the ShellBasedUnixGroupsMapping class, this property controls how long to wait for the underlying shell command that is run to fetch groups. Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes longer than the value configured, the command is aborted and the groups resolver would return a result of no groups found. A value of 0s (default) would mean an infinite wait (i.e. wait until the command exits on its own).",reliability,core
hadoop.security.group.mapping.ldap.connection.timeout.ms,60000,"This property is the connection timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't establish a connection within the specified period, it will abort the connect attempt. Non-positive value means no LDAP connection timeout is specified in which case it waits for the connection to establish until the underlying network times out.",reliability,core
hadoop.security.group.mapping.ldap.read.timeout.ms,60000,"This property is the read timeout (in milliseconds) for LDAP operations. If the LDAP provider doesn't get a LDAP response within the specified period, it will abort the read attempt. Non-positive value means no read timeout is specified in which case it waits for the response infinitely.",reliability,core
hadoop.security.group.mapping.ldap.ssl,FALSE,Whether or not to use SSL when connecting to the LDAP server.,security,core
hadoop.security.group.mapping.ldap.ssl.keystore,,File path to the SSL keystore that contains the SSL certificate required by the LDAP server.,environment,core
hadoop.security.group.mapping.ldap.ssl.keystore.password.file,,"The path to a file containing the password of the LDAP SSL keystore. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.ssl.keystore.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",security,core
hadoop.security.group.mapping.ldap.ssl.keystore.password,,The password of the LDAP SSL keystore. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,security,core
hadoop.security.credential.clear-text-fallback,TRUE,true or false to indicate whether or not to fall back to storing credential password as clear text. The default value is true. This property only works when the password can't not be found from credential providers.,security,core
hadoop.security.credential.provider.path,,A comma-separated list of URLs that indicates the type and location of a list of providers that should be consulted.,security,core
hadoop.security.group.mapping.ldap.ssl.truststore,,File path to the SSL truststore that contains the root certificate used to sign the LDAP server's certificate. Specify this if the LDAP server's certificate is not signed by a well known certificate authority.,environment,core
hadoop.security.group.mapping.ldap.ssl.truststore.password.file,,The path to a file containing the password of the LDAP SSL truststore. IMPORTANT: This file should be readable only by the Unix user running the daemons.,security,core
hadoop.security.group.mapping.ldap.bind.password.file,,"The path to a file containing the password of the bind user. If the password is not configured in credential providers and the property hadoop.security.group.mapping.ldap.bind.password is not set, LDAPGroupsMapping reads password from the file. IMPORTANT: This file should be readable only by the Unix user running the daemons and should be a local file.",security,core
hadoop.security.group.mapping.ldap.bind.password,,The password of the bind user. this property name is used as an alias to get the password from credential providers. If the password can not be found and hadoop.security.credential.clear-text-fallback is true LDAPGroupsMapping uses the value of this property for password.,security,core
hadoop.security.group.mapping.ldap.search.attr.group.name,cn,The attribute of the group object that identifies the group name. The default will usually be appropriate for all LDAP systems.,others,core
hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,0,The number of levels to go up the group hierarchy when determining which groups a user is part of. 0 Will represent checking just the group that the user belongs to. Each additional level will raise the time it takes to execute a query by at most hadoop.security.group.mapping.ldap.directory.search.timeout. The default will usually be appropriate for all LDAP systems.,security,core
hadoop.security.group.mapping.ldap.posix.attr.gid.name,gidNumber,The attribute of posixAccount indicating the group id.,others,core
hadoop.security.group.mapping.ldap.directory.search.timeout,10000,The attribute applied to the LDAP SearchControl properties to set a maximum time limit when searching and awaiting a result. Set to 0 if infinite wait period is desired. Default is 10 seconds. Units in milliseconds.,reliability,core
fs.azure.user.agent.prefix,unknown,"WASB passes User-Agent header to the Azure back-end. The default value contains WASB version, Java Runtime version, Azure Client library version, and the value of the configuration option fs.azure.user.agent.prefix.",others,core
hadoop.service.shutdown.timeout,30s,"Timeout to wait for each shutdown operation to complete. If a hook takes longer than this time to complete, it will be interrupted, so the service will shutdown. This allows the service shutdown to recover from a blocked operation. Some shutdown hooks may need more time than this, for example when a large amount of data needs to be uploaded to an object store. In this situation: increase the timeout. The minimum duration of the timeout is 1 second, ""1s"".",reliability,core
hadoop.rpc.protection,authentication,"A comma-separated list of protection values for secured sasl connections. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. hadoop.security.saslproperties.resolver.class can be used to override the hadoop.rpc.protection for a connection at the server side.",security,core
io.file.buffer.size,4096,"The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,core
io.bytes.per.checksum,512,The number of bytes per checksum. Must not be larger than io.file.buffer.size.,performance,core
io.seqfile.local.dir,${hadoop.tmp.dir}/io/local,The local directory where sequence file stores intermediate data files during merge. May be a comma-separated list of directories on different devices in order to spread disk i/o. Directories that do not exist are ignored.,environment,core
fs.defaultFS,file:///,"The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for a filesystem.",others,core
fs.default.name,file:///,Deprecated. Use (fs.defaultFS) property instead,others,core
fs.trash.checkpoint.interval,0,"Number of minutes between trash checkpoints. Should be smaller or equal to fs.trash.interval. If zero, the value is set to the value of fs.trash.interval. Every time the checkpointer runs it creates a new checkpoint out of current and removes checkpoints created more than fs.trash.interval minutes ago.",reliability,core
fs.AbstractFileSystem.har.impl,org.apache.hadoop.fs.HarFs,The AbstractFileSystem for har: uris.,others,core
fs.AbstractFileSystem.ftp.impl,org.apache.hadoop.fs.ftp.FtpFs,The FileSystem for Ftp: uris.,others,core
fs.AbstractFileSystem.webhdfs.impl,org.apache.hadoop.fs.WebHdfs,The FileSystem for webhdfs: uris.,others,core
fs.AbstractFileSystem.swebhdfs.impl,org.apache.hadoop.fs.SWebHdfs,The FileSystem for swebhdfs: uris.,others,core
fs.ftp.host,0.0.0.0,FTP filesystem connects to this server,environment,core
fs.ftp.host.port,21,FTP filesystem connects to fs.ftp.host on this port,environment,core
fs.s3.block.size,67108864,Block size to use when writing files to S3.,performance,core
fs.s3.buffer.dir,${hadoop.tmp.dir}/s3,Determines where on the local filesystem the s3:/s3n: filesystem should store files before sending them to S3 (or after retrieving them from S3).,environment,core
fs.s3.maxRetries,4,"The maximum number of retries for reading or writing files to S3, before we signal failure to the application.",reliability,core
fs.s3.sleepTimeSeconds,10,The number of seconds to sleep between each S3 retry.,reliability,core
fs.swift.impl,org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem,The implementation class of the OpenStack Swift Filesystem,others,core
fs.s3n.block.size,67108864,Block size to use when reading files using the native S3 filesystem (s3n: URIs).,performance,core
fs.s3n.multipart.uploads.block.size,67108864,The block size for multipart uploads to native S3 filesystem. Default size is 64MB.,performance,core
fs.s3n.multipart.copy.block.size,5368709120,The block size for multipart copy in native S3 filesystem. Default size is 5GB.,performance,core
fs.s3n.server-side-encryption-algorithm,,"Specify a server-side encryption algorithm for S3. Unset by default, and the only other currently allowable value is AES256.",security,core
fs.s3a.proxy.port,,"Proxy server port. If this property is not set but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with the value of fs.s3a.connection.ssl.enabled).",environment,core
fs.s3a.proxy.username,,Username for authenticating with proxy server.,security,core
fs.s3a.proxy.password,,Password for authenticating with proxy server.,security,core
fs.s3a.proxy.workstation,,Workstation for authenticating with proxy server.,security,core
fs.s3a.attempts.maximum,20,How many times we should retry commands on transient errors.,reliability,core
fs.s3a.connection.establish.timeout,5000,Socket connection setup timeout in milliseconds.,reliability,core
fs.s3a.connection.timeout,200000,Socket connection timeout in milliseconds.,reliability,core
fs.s3a.socket.recv.buffer,8192,Socket receive buffer hint to amazon connector. Represented in bytes.,performance,core
fs.s3a.paging.maximum,5000,How many keys to request from S3 when doing directory listings at a time.,performance,core
fs.s3a.threads.max,10,The total number of threads available in the filesystem for data uploads *or any other queued filesystem operation*.,performance,core
fs.s3a.multipart.size,100M,"How big (in bytes) to split upload or copy operations up into. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,core
fs.s3a.multipart.threshold,2147483647,"How big (in bytes) to split upload or copy operations up into. This also controls the partition size in renamed files, as rename() involves copying the source file(s). A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,core
fs.s3a.multipart.purge.age,86400,Minimum age in seconds of multipart uploads to purge.,reliability,core
fs.s3a.server-side-encryption-algorithm,,"Specify a server-side encryption algorithm for s3a: file system. Unset by default. It supports the following values: 'AES256' (for SSE-S3), 'SSE-KMS' and 'SSE-C'.",security,core
fs.s3a.server-side-encryption.key,,"Specific encryption key to use if fs.s3a.server-side-encryption-algorithm has been set to 'SSE-KMS' or 'SSE-C'. In the case of SSE-C, the value of this property should be the Base64 encoded key. If you are using SSE-KMS and leave this property empty, you'll be using your default's S3 KMS key, otherwise you should set this property to the specific KMS key id.",security,core
fs.s3a.signing-algorithm,,Override the default signing algorithm so legacy implementations can still be used,others,core
fs.s3a.block.size,32M,"Block size to use when reading files using s3a: file system. A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.",performance,core
fs.s3a.buffer.dir,${hadoop.tmp.dir}/s3a,Comma separated list of directories that will be used to buffer file uploads to.,environment,core
fs.s3a.fast.upload.buffer,disk,"The buffering mechanism to use when using S3A fast upload (fs.s3a.fast.upload=true). Values: disk, array, bytebuffer. This configuration option has no effect if fs.s3a.fast.upload is false. ""disk"" will use the directories listed in fs.s3a.buffer.dir as the location(s) to save data prior to being uploaded. ""array"" uses arrays in the JVM heap ""bytebuffer"" uses off-heap memory within the JVM. Both ""array"" and ""bytebuffer"" will consume memory in a single stream up to the number of blocks set by: fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks. If using either of these mechanisms, keep this value low The total number of threads performing work across all threads is set by fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued work items.",performance,core
fs.s3a.fast.upload.active.blocks,4,"Maximum Number of blocks a single output stream can have active (uploading, or queued to the central FileSystem instance's pool of queued operations. This stops a single stream overloading the shared thread pool.",performance,core
fs.s3a.user.agent.prefix,,"Sets a custom value that will be prepended to the User-Agent header sent in HTTP requests to the S3 back-end by S3AFileSystem. The User-Agent header always includes the Hadoop version number followed by a string generated by the AWS SDK. An example is ""User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6"". If this optional property is set, then its value is prepended to create a customized User-Agent. For example, if this configuration property was set to ""MyApp"", then an example of the resulting User-Agent would be ""User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6"".",others,core
fs.s3a.metadatastore.authoritative,FALSE,"When true, allow MetadataStore implementations to act as source of truth for getting file status and directory listings. Even if this is set to true, MetadataStore implementations may choose not to return authoritative results. If the configured MetadataStore does not support being authoritative, this setting will have no effect.",others,core
fs.s3a.metadatastore.impl,org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore,"Fully-qualified name of the class that implements the MetadataStore to be used by s3a. The default class, NullMetadataStore, has no effect: s3a will continue to treat the backing S3 service as the one and only source of truth for file and directory metadata.",others,core
fs.s3a.s3guard.cli.prune.age,86400000,Default age (in milliseconds) after which to prune metadata from the metadatastore when the prune command is run. Can be overridden on the command-line.,reliability,core
fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem,The implementation class of the S3A Filesystem,others,core
fs.s3a.s3guard.ddb.table,,"The DynamoDB table name to operate. Without this property, the respective S3 bucket name will be used.",others,core
fs.s3a.s3guard.ddb.max.retries,9,"Max retries on batched DynamoDB operations before giving up and throwing an IOException. Each retry is delayed with an exponential backoff timer which starts at 100 milliseconds and approximately doubles each time. The minimum wait before throwing an exception is sum(100, 200, 400, 800, .. 100*2^N-1 ) == 100 * ((2^N)-1) So N = 9 yields at least 51.1 seconds (51,100) milliseconds of blocking before throwing an IOException.",reliability,core
fs.s3a.s3guard.ddb.background.sleep,25,Length (in milliseconds) of pause between each batch of deletes when pruning metadata. Prevents prune operations (which can typically be low priority background operations) from overly interfering with other I/O operations.,reliability,core
fs.AbstractFileSystem.s3a.impl,org.apache.hadoop.fs.s3a.S3A,The implementation class of the S3A AbstractFileSystem.,others,core
fs.wasb.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem,The implementation class of the Native Azure Filesystem,others,core
fs.wasbs.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure,The implementation class of the Secure Native Azure Filesystem,others,core
fs.azure.authorization,FALSE,"Config flag to enable authorization support in WASB. Setting it to ""true"" enables authorization support to WASB. Currently WASB authorization requires a remote service to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url configuration",security,core
fs.azure.authorization.caching.enable,TRUE,Config flag to enable caching of authorization results and saskeys in WASB. This flag is relevant only when fs.azure.authorization is enabled.,security,core
io.seqfile.compress.blocksize,1000000,The minimum block size for compression in block compressed SequenceFiles.,performance,core
io.mapfile.bloom.size,1048576,"The size of BloomFilter-s used in BloomMapFile. Each time this many keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter). Larger values minimize the number of filters, which slightly increases the performance, but may waste too much space if the total number of keys is usually much smaller than this number.",performance,core
hadoop.util.hash.type,murmur,The default implementation of Hash. Currently this can take one of the two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.,others,core
ipc.client.connection.maxidletime,10000,The maximum time in msec after which a client will bring down the connection to the server.,reliability,core
ipc.client.connect.max.retries,10,Indicates the number of retries a client will make to establish a server connection.,reliability,core
ipc.client.connect.retry.interval,1000,Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.,reliability,core
ipc.client.connect.timeout,20000,Indicates the number of milliseconds a client will wait for the socket to establish a server connection.,reliability,core
ipc.client.connect.max.retries.on.timeouts,45,Indicates the number of retries a client will make on socket timeout to establish a server connection.,reliability,core
ipc.ping.interval,60000,"Timeout on waiting response from server, in milliseconds. The client will send ping when the interval is passed without receiving bytes, if ipc.client.ping is set to true.",reliability,core
ipc.client.rpc-timeout.ms,0,"Timeout on waiting response from server, in milliseconds. If ipc.client.ping is set to true and this rpc-timeout is greater than the value of ipc.ping.interval, the effective value of the rpc-timeout is rounded up to multiple of ipc.ping.interval.",reliability,core
ipc.server.listen.queue.size,128,Indicates the length of the listen queue for servers accepting client connections.,performance,core
hadoop.socks.server,,Address (host:port) of the SOCKS server to be used by the SocksSocketFactory.,environment,core
net.topology.node.switch.mapping.impl,org.apache.hadoop.net.ScriptBasedMapping,"The default implementation of the DNSToSwitchMapping. It invokes a script specified in net.topology.script.file.name to resolve node names. If the value for net.topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.",others,core
net.topology.impl,org.apache.hadoop.net.NetworkTopology,The default implementation of NetworkTopology which is classic three layer one.,others,core
net.topology.script.file.name,,"The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.",others,core
net.topology.table.file.name,,"The file name for a topology file, which is used when the net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.",others,core
file.stream-buffer-size,4096,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,core
file.bytes-per-checksum,512,The number of bytes per checksum. Must not be larger than file.stream-buffer-size,performance,core
file.blocksize,67108864,Block size,performance,core
s3.stream-buffer-size,4096,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,core
s3.bytes-per-checksum,512,The number of bytes per checksum. Must not be larger than s3.stream-buffer-size,performance,core
s3.client-write-packet-size,65536,Packet size for clients to write,performance,core
s3.blocksize,67108864,Block size,performance,core
s3native.stream-buffer-size,4096,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,core
s3native.bytes-per-checksum,512,The number of bytes per checksum. Must not be larger than s3native.stream-buffer-size,performance,core
s3native.client-write-packet-size,65536,Packet size for clients to write,performance,core
s3native.blocksize,67108864,Block size,performance,core
ftp.stream-buffer-size,4096,"The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.",performance,core
ftp.bytes-per-checksum,512,The number of bytes per checksum. Must not be larger than ftp.stream-buffer-size,performance,core
ftp.client-write-packet-size,65536,Packet size for clients to write,performance,core
ftp.blocksize,67108864,Block size,performance,core
tfile.io.chunk.size,1048576,Value chunk size in bytes. Default to 1MB. Values of the length less than the chunk size is guaranteed to have known value length in read time (See also TFile.Reader.Scanner.Entry.isValueLengthKnown()).,performance,core
tfile.fs.output.buffer.size,262144,Buffer size used for FSDataOutputStream in bytes.,performance,core
tfile.fs.input.buffer.size,262144,Buffer size used for FSDataInputStream in bytes.,performance,core
hadoop.http.authentication.type,simple,Defines authentication used for Oozie HTTP endpoint. Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#,security,core
hadoop.http.authentication.signature.secret.file,${user.home}/hadoop-http-auth-signature-secret,The signature secret for signing the authentication tokens. The same secret should be used for JT/NN/DN/TT configurations.,security,core
hadoop.http.cross-origin.allowed-headers,"X-Requested-With,Content-Type,Accept,Origin",Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support.,others,core
hadoop.http.cross-origin.max-age,1800,The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support.,reliability,core
dfs.ha.fencing.ssh.connect-timeout,30000,"SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.",reliability,core
ha.zookeeper.session-timeout.ms,10000,"The session timeout to use when the ZKFC connects to ZooKeeper. Setting this value to a lower value implies that server crashes will be detected more quickly, but risks triggering failover too aggressively in the case of a transient error or network blip.",reliability,core
ha.zookeeper.auth,,"A comma-separated list of ZooKeeper authentications to add when connecting to ZooKeeper. These are specified in the same format as used by the ""addauth"" command in the ZK CLI. It is important that the authentications specified here are sufficient to access znodes with the ACL specified in ha.zookeeper.acl. If the auths contain secrets, you may instead specify a path to a file, prefixed with the '@' symbol, and the value of this configuration will be loaded from within.",security,core
ha.health-monitor.connect-retry-interval.ms,1000,How often to retry connecting to the service.,reliability,core
ha.health-monitor.check-interval.ms,1000,How often to check the service.,reliability,core
ha.health-monitor.sleep-after-disconnect.ms,1000,How long to sleep after an unexpected RPC error.,reliability,core
ha.health-monitor.rpc-timeout.ms,45000,Timeout for the actual monitorHealth() calls.,reliability,core
ha.failover-controller.new-active.rpc-timeout.ms,60000,Timeout that the FC waits for the new active to become active,reliability,core
ha.failover-controller.graceful-fence.rpc-timeout.ms,5000,Timeout that the FC waits for the old active to go to standby,reliability,core
ha.failover-controller.graceful-fence.connection.retries,1,FC connection retries for graceful fencing,reliability,core
ha.failover-controller.cli-check.rpc-timeout.ms,20000,"Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState",reliability,core
ipc.client.fallback-to-simple-auth-allowed,FALSE,"When a client is configured to attempt a secure connection, but attempts to connect to an insecure server, that server may instruct the client to switch to SASL SIMPLE (unsecure) authentication. This setting controls whether or not the client will accept this instruction from the server. When false (the default), the client will not allow the fallback to SIMPLE authentication, and will abort the connection.",security,core
hadoop.security.crypto.codec.classes.aes.ctr.nopadding,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec","Comma-separated list of crypto codec implementations for AES/CTR/NoPadding. The first implementation will be used if available, others are fallbacks.",others,core
hadoop.security.crypto.buffer.size,8192,The buffer size used by CryptoInputStream and CryptoOutputStream.,performance,core
hadoop.security.kms.client.authentication.retry-count,1,Number of time to retry connecting to KMS on authentication failure,reliability,core
hadoop.security.kms.client.encrypted.key.cache.size,500,Size of the EncryptedKeyVersion cache Queue for each key,performance,core
hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,2,Number of threads to use for refilling depleted EncryptedKeyVersion cache Queues,performance,core
hadoop.security.kms.client.encrypted.key.cache.expiry,43200000,"Cache expiry time for a Key, after which the cache Queue for this key will be dropped. Default = 12hrs",reliability,core
hadoop.security.kms.client.timeout,60,"Sets value for KMS client connection timeout, and the read timeout to KMS servers.",reliability,core
hadoop.security.kms.client.failover.sleep.base.millis,100,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the base value used in the failover calculation. The first failover will retry immediately. The 2nd failover attempt will delay at least hadoop.security.client.failover.sleep.base.millis milliseconds. And so on.",reliability,core
hadoop.security.kms.client.failover.sleep.max.millis,2000,"Expert only. The time to wait, in milliseconds, between failover attempts increases exponentially as a function of the number of attempts made so far, with a random factor of +/- 50%. This option specifies the maximum value to wait between failovers. Specifically, the time between two failover attempts will not exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis milliseconds.",reliability,core
ipc.server.max.connections,0,"The maximum number of concurrent connections a server is allowed to accept. If this limit is exceeded, incoming connections will first fill the listen queue and then may go to an OS-specific listen overflow queue. The client may fail or timeout, but the server can avoid running out of file descriptors using this feature. 0 means no limit.",reliability,core
hadoop.registry.zk.session.timeout.ms,60000,Zookeeper session timeout in milliseconds,reliability,core
hadoop.registry.zk.connection.timeout.ms,15000,Zookeeper connection timeout in milliseconds,reliability,core
hadoop.registry.zk.retry.times,5,Zookeeper connection retry count before failing,reliability,core
hadoop.registry.zk.retry.ceiling.ms,60000,"Zookeeper retry limit in milliseconds, during exponential backoff. This places a limit even if the retry times and interval limit, combined with the backoff policy, result in a long retry period",reliability,core
hadoop.registry.zk.quorum,localhost:2181,List of hostname:port pairs defining the zookeeper quorum binding for the registry,environment,core
hadoop.registry.secure,FALSE,"Key to set if the registry is secure. Turning it on changes the permissions policy from ""open access"" to restrictions on kerberos with the option of a user adding one or more auth key pairs down their own tree.",security,core
fs.client.htrace.sampler.classes,,The class names of the HTrace Samplers to use for Hadoop filesystem clients.,others,core
hadoop.htrace.span.receiver.classes,,The class names of the Span Receivers to use for Hadoop.,others,core
fs.adl.oauth2.msi.port,,"The localhost port for the MSI token service. This is the port specified when creating the Azure VM. The default, if this setting is not specified, is 50342. Used by MSI token provider.",environment,core
fs.adl.oauth2.devicecode.clientapp.id,,The app id of the AAD native app in whose context the auth request should be made. Used by DeviceCode token provider.,others,core
hadoop.caller.context.max.size,128,"The maximum bytes a caller context string can have. If the passed caller context is longer than this maximum bytes, client will truncate it before sending to server. Note that the server may have a different maximum size, and will truncate the caller context to the maximum size it allows.",performance,core
seq.io.sort.mb,100,"The total amount of buffer memory to use while sorting files, while using SequenceFile.Sorter, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.",performance,core
hadoop.zk.address,,Host:Port of the ZooKeeper server to be used.,environment,core
hadoop.zk.num-retries,1000,Number of tries to connect to ZooKeeper.,reliability,core
hadoop.zk.retry-interval-ms,1000,Retry interval in milliseconds when connecting to ZooKeeper.,reliability,core
hadoop.zk.timeout-ms,10000,"ZooKeeper session timeout in milliseconds. Session expiration is managed by the ZooKeeper cluster itself, not by the client. This value is used by the cluster to determine when the client's session expires. Expirations happens when the cluster does not hear from the client within the specified session timeout period (i.e. no heartbeat).",reliability,core
hadoop.zk.auth,,"Specify the auths to be used for the ACL's specified in hadoop.zk.acl. This takes a comma-separated list of authentication mechanisms, each of the form 'scheme:auth' (the same syntax used for the 'addAuth' command in the ZK CLI).",security,core
